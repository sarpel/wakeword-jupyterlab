#!/usr/bin/env python3
"""
Enhanced Wakeword Training Gradio Application
Complete GUI for wakeword detection model training with comprehensive documentation
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import librosa
import soundfile as sf
import os
import glob
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
import gradio as gr
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import threading
import time
import json
from datetime import datetime
import markdown
warnings.filterwarnings('ignore')

# Configuration Classes (same as before)
class AudioConfig:
    SAMPLE_RATE = 16000
    DURATION = 1.7
    N_MELS = 80
    N_FFT = 2048
    HOP_LENGTH = 512
    WIN_LENGTH = 2048
    FMIN = 0
    FMAX = 8000

class ModelConfig:
    HIDDEN_SIZE = 256
    NUM_LAYERS = 2
    DROPOUT = 0.6
    NUM_CLASSES = 2

class TrainingConfig:
    BATCH_SIZE = 32
    LEARNING_RATE = 0.0001
    EPOCHS = 100
    VALIDATION_SPLIT = 0.2
    TEST_SPLIT = 0.1

class AugmentationConfig:
    AUGMENTATION_PROB = 0.85
    NOISE_FACTOR = 0.15
    TIME_SHIFT_MAX = 0.3
    PITCH_SHIFT_MAX = 1.5
    SPEED_CHANGE_MIN = 0.9
    SPEED_CHANGE_MAX = 1.1

# Audio Processing Class (same as before)
class AudioProcessor:
    def __init__(self, config=AudioConfig):
        self.config = config

    def load_audio(self, file_path):
        try:
            audio, sr = librosa.load(file_path, sr=self.config.SAMPLE_RATE)
            return audio
        except Exception as e:
            return None

    def normalize_audio(self, audio):
        if len(audio) == 0:
            return audio
        max_abs = np.max(np.abs(audio))
        if not np.isfinite(max_abs) or max_abs < 1e-8:
            return np.zeros_like(audio)
        normalized = audio / max_abs
        if np.isnan(normalized).any() or np.isinf(normalized).any():
            normalized = np.nan_to_num(normalized, nan=0.0, posinf=0.0, neginf=0.0)
        return normalized

    def pad_or_truncate(self, audio, target_length):
        if len(audio) > target_length:
            start_idx = random.randint(0, len(audio) - target_length)
            return audio[start_idx:start_idx + target_length]
        else:
            return np.pad(audio, (0, target_length - len(audio)), mode='constant')

    def audio_to_mel(self, audio):
        if len(audio) == 0:
            return np.zeros((self.config.N_MELS, 31))

        audio = np.nan_to_num(audio, nan=0.0, posinf=0.0, neginf=0.0)
        if np.max(np.abs(audio)) < 1e-8:
            return np.zeros((self.config.N_MELS, 31))

        mel_spec = librosa.feature.melspectrogram(
            y=audio, sr=self.config.SAMPLE_RATE, n_mels=self.config.N_MELS,
            n_fft=self.config.N_FFT, hop_length=self.config.HOP_LENGTH,
            win_length=self.config.WIN_LENGTH, fmin=self.config.FMIN, fmax=self.config.FMAX
        )

        return librosa.power_to_db(mel_spec, ref=np.max)

    def augment_audio(self, audio, config=AugmentationConfig):
        augmented_audio = audio.copy()

        if random.random() < config.AUGMENTATION_PROB:
            shift_amount = int(random.uniform(-config.TIME_SHIFT_MAX, config.TIME_SHIFT_MAX) * self.config.SAMPLE_RATE)
            augmented_audio = np.roll(augmented_audio, shift_amount)

        if random.random() < config.AUGMENTATION_PROB:
            n_steps = random.uniform(-config.PITCH_SHIFT_MAX, config.PITCH_SHIFT_MAX)
            augmented_audio = librosa.effects.pitch_shift(y=augmented_audio, sr=self.config.SAMPLE_RATE, n_steps=n_steps)

        if random.random() < config.AUGMENTATION_PROB:
            speed_factor = random.uniform(config.SPEED_CHANGE_MIN, config.SPEED_CHANGE_MAX)
            augmented_audio = librosa.effects.time_stretch(y=augmented_audio, rate=speed_factor)
            augmented_audio = self.pad_or_truncate(augmented_audio, len(audio))

        if random.random() < config.AUGMENTATION_PROB:
            noise = np.random.normal(0, config.NOISE_FACTOR, len(augmented_audio))
            augmented_audio = augmented_audio + noise

        return augmented_audio

    def process_audio_file(self, file_path, augment=False):
        audio = self.load_audio(file_path)
        if audio is None:
            return None

        audio = self.normalize_audio(audio)
        target_length = int(self.config.SAMPLE_RATE * self.config.DURATION)
        audio = self.pad_or_truncate(audio, target_length)

        if augment:
            audio = self.augment_audio(audio)

        return self.audio_to_mel(audio)

# Neural Network Model (same as before)
class WakewordModel(nn.Module):
    def __init__(self, config=ModelConfig, audio_config=AudioConfig):
        super(WakewordModel, self).__init__()
        self.config = config
        self.audio_config = audio_config

        self.mel_height = audio_config.N_MELS
        self.mel_width = int(audio_config.SAMPLE_RATE * audio_config.DURATION / audio_config.HOP_LENGTH) + 1

        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.cnn_output_size = 128

        self.lstm = nn.LSTM(
            input_size=self.cnn_output_size, hidden_size=config.HIDDEN_SIZE,
            num_layers=config.NUM_LAYERS, batch_first=True,
            dropout=config.DROPOUT if config.NUM_LAYERS > 1 else 0
        )

        self.dropout = nn.Dropout(config.DROPOUT)
        self.fc = nn.Linear(config.HIDDEN_SIZE, config.NUM_CLASSES)

    def forward(self, x):
        batch_size = x.size(0)

        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = self.pool(x)

        x = x.view(batch_size, -1)
        x = x.unsqueeze(1)

        lstm_out, (h_n, c_n) = self.lstm(x)
        x = lstm_out[:, -1, :]

        x = self.dropout(x)
        x = self.fc(x)

        return x

# Dataset Class (same as before)
class EnhancedWakewordDataset(Dataset):
    def __init__(self, wakeword_files, hard_negative_files, random_negative_files,
                 background_files, processor, augment=False,
                 background_mix_prob=0.7, snr_range=(0, 20)):

        self.processor = processor
        self.augment = augment
        self.background_mix_prob = background_mix_prob
        self.snr_range = snr_range

        self.wakeword_files = wakeword_files
        self.hard_negative_files = hard_negative_files
        self.random_negative_files = random_negative_files
        self.background_files = background_files

        self.background_cache = self._cache_background_segments()
        self._create_balanced_dataset()

    def _cache_background_segments(self, max_cache_size=100):
        cache = []
        for i, bg_file in enumerate(self.background_files[:max_cache_size]):
            try:
                audio = self.processor.load_audio(bg_file)
                if audio is not None and len(audio) > 0:
                    audio = audio / (np.max(np.abs(audio)) + 1e-8)
                    cache.append(audio)
            except Exception:
                pass
        return cache

    def _create_balanced_dataset(self):
        n_wakeword = len(self.wakeword_files)
        n_hard_neg = min(len(self.hard_negative_files), int(n_wakeword * 4.5))
        n_random_neg = min(len(self.random_negative_files), int(n_wakeword * 8.75))
        n_background_pure = min(len(self.background_files), int(n_wakeword * 10))

        random.seed(42)
        sampled_hard_neg = random.sample(self.hard_negative_files, n_hard_neg) if n_hard_neg > 0 else []
        sampled_random_neg = random.sample(self.random_negative_files, n_random_neg) if n_random_neg > 0 else []
        sampled_background = random.sample(self.background_files, n_background_pure) if n_background_pure > 0 else []

        self.files = []
        self.labels = []
        self.categories = []

        for f in self.wakeword_files:
            self.files.append(f); self.labels.append(1); self.categories.append('wakeword')
        for f in sampled_hard_neg:
            self.files.append(f); self.labels.append(0); self.categories.append('hard_negative')
        for f in sampled_random_neg:
            self.files.append(f); self.labels.append(0); self.categories.append('random_negative')
        for f in sampled_background:
            self.files.append(f); self.labels.append(0); self.categories.append('background')

        indices = list(range(len(self.files)))
        random.shuffle(indices)
        self.files = [self.files[i] for i in indices]
        self.labels = [self.labels[i] for i in indices]
        self.categories = [self.categories[i] for i in indices]

    def _mix_with_background(self, audio, target_snr_db=None):
        if len(self.background_cache) == 0:
            return audio

        bg_audio = random.choice(self.background_cache)
        target_len = len(audio)

        if len(bg_audio) > target_len:
            start_idx = random.randint(0, len(bg_audio) - target_len)
            bg_segment = bg_audio[start_idx:start_idx + target_len]
        else:
            bg_segment = np.tile(bg_audio, (target_len // len(bg_audio) + 1))[:target_len]

        if target_snr_db is None:
            target_snr_db = random.uniform(self.snr_range[0], self.snr_range[1])

        signal_power = np.mean(audio ** 2)
        noise_power = np.mean(bg_segment ** 2)

        if noise_power > 0:
            noise_scale = np.sqrt(signal_power / (10 ** (target_snr_db / 10)) / noise_power)
            mixed = audio + noise_scale * bg_segment
            max_val = np.max(np.abs(mixed))
            if max_val > 0:
                mixed = mixed / max_val * 0.95
            return mixed

        return audio

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        file_path = self.files[idx]
        label = self.labels[idx]
        category = self.categories[idx]

        audio = self.processor.load_audio(file_path)

        if audio is None:
            mel_spec = np.zeros((self.processor.config.N_MELS, 31), dtype=np.float32)
        else:
            audio = self.processor.normalize_audio(audio)
            target_length = int(self.processor.config.SAMPLE_RATE * self.processor.config.DURATION)
            audio = self.processor.pad_or_truncate(audio, target_length)

            if self.augment:
                audio = self.processor.augment_audio(audio)

            if category != 'background' and random.random() < self.background_mix_prob:
                audio = self._mix_with_background(audio)

            mel_spec = self.processor.audio_to_mel(audio)

        mel_array = np.ascontiguousarray(mel_spec, dtype=np.float32)
        mel_tensor = torch.from_numpy(mel_array).unsqueeze(0).clone()
        label_tensor = torch.tensor(label, dtype=torch.long)

        return mel_tensor, label_tensor

# Training Class (same as before)
class WakewordTrainer:
    def __init__(self, model, device, config=TrainingConfig):
        self.model = model
        self.device = device
        self.config = config

        self.criterion = nn.CrossEntropyLoss().to(device)
        self.optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='max', factor=0.5, patience=5)

        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []

        self.patience = 10
        self.best_val_acc = 0.0
        self.epochs_no_improve = 0

        self.current_epoch = 0
        self.is_training = False
        self.training_complete = False

    def train_epoch(self, train_loader, progress_callback=None):
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device).squeeze()

            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)

            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

            loss.backward()
            self.optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

            if progress_callback and batch_idx % 10 == 0:
                progress = (batch_idx + 1) / len(train_loader) * 100
                progress_callback(progress, batch_idx + 1, len(train_loader))

        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100. * correct / total

        return epoch_loss, epoch_acc

    def validate(self, val_loader):
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device).squeeze()
                output = self.model(data)
                loss = self.criterion(output, target)

                running_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()

        epoch_loss = running_loss / len(val_loader)
        epoch_acc = 100. * correct / total

        return epoch_loss, epoch_acc

    def train(self, train_loader, val_loader, epochs, progress_callback=None):
        self.is_training = True
        self.training_complete = False

        print(f"Starting training for {epochs} epochs...")

        for epoch in range(epochs):
            if not self.is_training:
                break

            self.current_epoch = epoch + 1

            train_loss, train_acc = self.train_epoch(train_loader, progress_callback)
            val_loss, val_acc = self.validate(val_loader)

            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accuracies.append(train_acc)
            self.val_accuracies.append(val_acc)

            self.scheduler.step(val_acc)

            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self.epochs_no_improve = 0

                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'val_acc': val_acc,
                    'train_acc': train_acc,
                    'train_loss': train_loss,
                    'val_loss': val_loss,
                }, 'best_wakeword_model.pth')
            else:
                self.epochs_no_improve += 1

            if self.epochs_no_improve >= self.patience:
                break

        self.is_training = False
        self.training_complete = True
        return self.best_val_acc

# Enhanced Training Guide Content
COMPREHENSIVE_TRAINING_GUIDE = """
# ğŸ¯ WAKEWORD DETECTION COMPLETE TRAINING GUIDE

## ğŸ“Š VERÄ° SETÄ° HAZIRLAMA - DETAYLI ANALÄ°Z

### ğŸ¤ POZÄ°TÄ°F VERÄ°LER (WAKEWORD KAYITLARI)

#### Miktar ve Kalite Gereksinimleri
- **Minimum Miktar**: 100-500 temiz wakeword kaydÄ±
- **Ä°deal Miktar**: 1000+ wakeword kaydÄ±
- **SNR**: â‰¥ 20dB (sinyal/gÃ¼rÃ¼ltÃ¼ oranÄ±)
- **SÃ¼re**: 1-2 saniye (optimum 1.7s)
- **Format**: WAV, 16-bit, 16kHz

#### Ã‡eÅŸitlilik Gereksinimleri
**ğŸ¤ Mikrofon Ã‡eÅŸitliliÄŸi:**
â€¢ Smartphone, USB, Bluetooth, laptop, profesyonel
â€¢ FarklÄ± marka ve modeller
â€¢ FarklÄ± sampling kaliteleri

**ğŸ‘¥ KonuÅŸan Ã‡eÅŸitliliÄŸi:**
â€¢ Erkek/KadÄ±n/Ã‡ocuk sesleri
â€¢ FarklÄ± yaÅŸ gruplarÄ± (18-65 yaÅŸ)
â€¢ FarklÄ± aksanlar ve lehÃ§eler
â€¢ FarklÄ± konuÅŸma hÄ±zlarÄ± (yavaÅŸ/hÄ±zlÄ±)
â€¢ FarklÄ± ses tonlarÄ± (yÃ¼ksek/alÃ§ak)

**ğŸŒ Ortam Ã‡eÅŸitliliÄŸi:**
â€¢ Sessiz oda (SNR > 30dB)
â€¢ Ofis ortamÄ± (SNR 20-25dB)
â€¢ DÄ±ÅŸ mekan (SNR 15-20dB)
â€¢ Araba iÃ§i (SNR 10-15dB)
â€¢ Kafe/restoran (SNR 5-10dB)

### ğŸ”Š NEGATÄ°F VERÄ°LER

**A. Hard Negative Samples (Fonetik Benzer):**
â€¢ Miktar: Her wakeword iÃ§in 4-5 sample
â€¢ Ã–rnekler: "hey"â†’"hey computer", "ok"â†’"okay", "day"â†’"they"
â€¢ Phonetically benzer kelimeler seÃ§ilmeli
â€¢ KonuÅŸma benzerliÄŸi yÃ¼ksek olmalÄ±

**B. Random Negative Samples:**
â€¢ Miktar: Her wakeword iÃ§in 8-9 sample
â€¢ TÃ¼rler: GÃ¼nlÃ¼k konuÅŸmalar, telefon gÃ¶rÃ¼ÅŸmeleri, radyo/TV
â€¢ Ã‡eÅŸitlilik: FarklÄ± diller, aksanlar, ortamlar
â€¢ SÃ¼re: 1-3 saniye arasÄ±

**C. Background Noise Samples:**
â€¢ Miktar: Minimum 66 saat, ideal 100+ saat
â€¢ TÃ¼rler: Beyaz/pembe/kahverengi gÃ¼rÃ¼ltÃ¼, fan sesi, trafik
â€¢ Ã‡eÅŸitlilik: FarklÄ± SNR seviyeleri (0-30dB)
â€¢ Format: YÃ¼ksek kaliteli kayÄ±tlar

### âš–ï¸ Ä°DEAL VERÄ° DAÄILIMI
```
1 wakeword : 4.5 hard_negative : 8.75 random_negative : 10 background

Ã–rnek (100 wakeword iÃ§in):
â€¢ Wakeword: 100 samples (%4.2)
â€¢ Hard Negative: 450 samples (%18.8)
â€¢ Random Negative: 875 samples (%36.5)
â€¢ Background: 1000 samples (%41.7)
â€¢ TOPLAM: 2425 samples
```

## ğŸµ SES KALÄ°TESÄ° TEKNÄ°K KRÄ°TERLER

- **Sample Rate**: 16kHz (insan sesi iÃ§in optimum)
- **Bit Depth**: 16-bit veya Ã¼zeri
- **Format**: WAV (kayÄ±psÄ±z), FLAC (sÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ)
- **SNR**: Minimum 20dB, ideal 30dB+
- **Clipping**: -3dB'den fazla olmamalÄ±
- **Phase**: DoÄŸrusal fazå“åº”Ä±
- **Dynamic Range**: En az 60dB

## ğŸ”„ DETAYLI VERÄ° ARTIRMA TEKNÄ°KLERÄ°

### 1. TIME SHIFTING (Zaman KaydÄ±rma)
- **AralÄ±k**: Â±0.3 saniye (Â±4800 sample)
- **AmaÃ§**: FarklÄ± zamanlama senaryolarÄ±
- **Uygulama**: np.roll ile dairesel kaydÄ±rma
- **Limit**: Ses sÄ±nÄ±rlarÄ± iÃ§inde kalmalÄ±

### 2. PITCH SHIFTING (Perde DeÄŸiÅŸtirme)
- **AralÄ±k**: Â±1.5 semiton (%18 frekans deÄŸiÅŸimi)
- **Teknik**: PSOLA algoritmasÄ± ile doÄŸal deÄŸiÅŸim
- **Etki**: Sadece perdeyi deÄŸiÅŸtirir, sÃ¼reyi korur
- **DoÄŸallÄ±k**: Ä°nsan kulaÄŸÄ±na doÄŸal gelen aralÄ±k

### 3. SPEED CHANGING (HÄ±z DeÄŸiÅŸtirme)
- **AralÄ±k**: 0.9x - 1.1x (%10 hÄ±z deÄŸiÅŸimi)
- **Etkiler**: Hem perdeyi hem sÃ¼reyi deÄŸiÅŸtirir
- **Kalite**: PSOLA ile doÄŸallÄ±k korunur
- **Not**: Orijinal uzunluk korunur

### 4. BACKGROUND NOISE MIXING
- **SNR Range**: 0-20dB (training iÃ§in)
- **SNR Range**: 5-15dB (validation iÃ§in)
- **OlasÄ±lÄ±k**: %70 karÄ±ÅŸtÄ±rma oranÄ±
- **Teknik**: Sinyal gÃ¼cÃ¼ne gÃ¶re Ã¶lÃ§eklendirme

### 5. ADDITIVE NOISE
- **TÃ¼rler**: Beyaz, pembe, kahverengi gÃ¼rÃ¼ltÃ¼
- **Seviye**: %15 sinyal gÃ¼cÃ¼
- **AmaÃ§**: Sensitivite artÄ±rma
- **Uygulama**: Gaussian gÃ¼rÃ¼ltÃ¼ ekleme

## ğŸ§  MODEL MÄ°MARÄ°SÄ° DETAYLI AÃ‡IKLAMA

### CNN+LSTM Mimarisi
```
Input â†’ Conv2D(1â†’32) â†’ ReLU â†’ Conv2D(32â†’64) â†’ ReLU â†’
Conv2D(64â†’128) â†’ ReLU â†’ AdaptiveAvgPool2d â†’
Flatten â†’ LSTM(128â†’256Ã—2) â†’ Dropout(0.6) â†’ Linear(256â†’2)
```

### Katman DetaylarÄ±
- **Input**: (batch, 1, 80, 31) - 80 mel bands, 31 time frames
- **Conv Layers**: 320 + 18,496 + 73,856 = 92,672 parameters
- **LSTM**: 788,992 parameters (2 layers, 256 hidden)
- **Total**: ~882K parameters

### ğŸ¯ HIDDEN SIZE SEÃ‡Ä°MÄ°
- **128**: KÃ¼Ã§Ã¼k veri setleri, hÄ±zlÄ± eÄŸitim
- **256**: Dengeli performans (OPTIMUM)
- **512**: BÃ¼yÃ¼k veri setleri, daha iyi accuracy
- **1024**: Ã‡ok bÃ¼yÃ¼k veri setleri, yavaÅŸ eÄŸitim

## ğŸ“ˆ TRAINING SÃœRECÄ° DETAYLI ANLATIM

### 1. VERÄ° YÃœKLEME PIPELINE
Audio â†’ Normalize â†’ Pad/Truncate â†’ Mel-Spec â†’ Log â†’ Tensor
- **Batch Size**: 32 (memory ve gradient dengesi)
- **Num Workers**: 2-4 (paralel loading)
- **Pin Memory**: True (GPU hÄ±zlandÄ±rma)

### 2. FORWARD PASS
Model forward â†’ Loss calculation â†’ Accuracy computation
- **CrossEntropyLoss**: Automatic softmax + NLL loss
- **Gradient calculation**: Autograd ile otomatik

### 3. BACKWARD PASS
Loss.backward() â†’ Gradient clipping â†’ Optimizer.step()
- **Gradient clipping**: max_norm=1.0 (patlama Ã¶nleme)
- **Learning rate scheduling**: ReduceLROnPlateau

### 4. EARLY STOPPING
- **Patience**: 10 epoch (improvement yoksa dur)
- **Min Delta**: 0.001 (minimum improvement)
- **Mode**: max (validation accuracy'yi izle)

## âš™ï¸ KONFÄ°GÃœRASYON PARAMETRELERÄ° OPTÄ°MÄ°ZASYONU

### AUDIO CONFIG
- **SAMPLE_RATE (16kHz)**: Ä°nsan sesi iÃ§in optimum
- **DURATION (1.7s)**: Wakeword'u tam kapsar
- **N_MELS (80)**: Dengeli frekans Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼
- **N_FFT (2048)**: 7.8Hz frekans Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼
- **HOP_LENGTH (512)**: 32ms zaman Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼

### MODEL CONFIG
- **HIDDEN_SIZE (256)**: Dengeli model kapasitesi
- **NUM_LAYERS (2)**: Uygun derinlik
- **DROPOUT (0.6)**: Overfitting Ã¶nleme
- **NUM_CLASSES (2)**: Binary classification

### TRAINING CONFIG
- **BATCH_SIZE (32)**: Memory ve gradient dengesi
- **LEARNING_RATE (0.0001)**: Stabil Ã¶ÄŸrenme hÄ±zÄ±
- **EPOCHS (100)**: Maksimum eÄŸitim sÃ¼resi
- **VALIDATION_SPLIT (0.2)**: Dengeli validation

### AUGMENTATION CONFIG
- **AUGMENTATION_PROB (0.85)**: Dengeli artÄ±rma oranÄ±
- **NOISE_FACTOR (0.15)**: Orta seviye gÃ¼rÃ¼ltÃ¼
- **TIME_SHIFT_MAX (0.3)**: Uygun zaman kaydÄ±rma
- **PITCH_SHIFT_MAX (1.5)**: DoÄŸal perde deÄŸiÅŸimi

## ğŸ“Š PERFORMANS METRÄ°KLERÄ° VE YORUMLAMA

### ACCURACY METRÄ°KLERÄ°
- **Accuracy**: DoÄŸru tahmin oranÄ±
- **Precision**: Pozitif tahminlerin doÄŸruluÄŸu
- **Recall**: Pozitiflerin ne kadarÄ± yakalandÄ±
- **F1-Score**: Precision ve recall harmonik ortalamasÄ±

### CONFUSION MATRIX YORUMLAMA
- **True Positive**: DoÄŸru wakeword tespiti
- **False Positive**: YanlÄ±ÅŸ wakeword tespiti
- **True Negative**: DoÄŸru negative tespiti
- **False Negative**: KaÃ§Ä±rÄ±lan wakeword

### PERFORMANS STANDARTLARI
- **MÃ¼kemmel**: F1-Score â‰¥ 0.90
- **Ã‡ok Ä°yi**: F1-Score â‰¥ 0.85
- **Ä°yi**: F1-Score â‰¥ 0.80
- **Orta**: F1-Score â‰¥ 0.70
- **ZayÄ±f**: F1-Score < 0.70

## ğŸš¨ YAYGIN SORUNLAR VE Ã‡Ã–ZÃœMLER

### OVERFITTING BELÄ°RTÄ°LERÄ°
- Train accuracy %95+, validation accuracy dÃ¼ÅŸÃ¼yor
- Train loss azalÄ±yor, validation loss artÄ±yor
- **Ã‡Ã¶zÃ¼mler**: Dropout artÄ±r, augmentation gÃ¼Ã§lendir, early stopping

### UNDERFITTING BELÄ°RTÄ°LERÄ°
- Train ve validation accuracy dÃ¼ÅŸÃ¼k (<%70)
- Loss yÃ¼ksek ve azalmÄ±yor
- **Ã‡Ã¶zÃ¼mler**: Model kapasitesini artÄ±r, learning rate artÄ±r

### GRADIENT EXPLOSION
- Loss anÄ±nda bÃ¼yÃ¼k deÄŸerler, NaN/Inf
- **Ã‡Ã¶zÃ¼mler**: Gradient clipping, learning rate azalt, batch norm

### MEMORY ERROR
- CUDA out of memory hatasÄ±
- **Ã‡Ã¶zÃ¼mler**: Batch size kÃ¼Ã§Ã¼lt, mixed precision, gradient accumulation

## ğŸ’¡ EN Ä°YÄ° UYGULAMALAR VE TAVSÄ°YELER

### VERÄ° KALÄ°TESÄ°
- Her wakeword'u 3-5 kez kaydet
- FarklÄ± ortamlarda kayÄ±t yap
- Mikrofon kalitesine dikkat et
- Clipping kontrolÃ¼ yap

### MODEL GELÄ°ÅTÄ°RME
- Cross-validation kullan (5-fold)
- Hyperparameter tuning yap
- A/B testing ile karÅŸÄ±laÅŸtÄ±r
- Model checkpoint'lerini kaydet

### TRAINING STRATEJÄ°SÄ°
- Learning rate scheduling kullan
- Early stopping implemente et
- Gradient clipping uygula
- Mixed precision kullan (GPU iÃ§in)

## ğŸ¯ BAÅARILI MODEL Ä°Ã‡Ä°N GEREKSÄ°NÄ°MLER

### Minimum
- 100+ wakeword kaydÄ±
- %85+ validation accuracy
- %80+ test accuracy
- Dengeli veri seti

### Ä°deal
- 1000+ wakeword kaydÄ±
- %90+ validation accuracy
- %85+ test accuracy
- KapsamlÄ± Ã§eÅŸitlilik

---

## ğŸ“š DAHA FAZLA BÄ°LGÄ°

DetaylÄ± bilgi iÃ§in `COMPREHENSIVE_TRAINING_GUIDE.md` dosyasÄ±na bakÄ±n.
TÃ¼m teknik detaylar, optimizasyon stratejileri ve Ã¶rnek kodlar mevcuttur.
"""

# Global variables
trainer = None
processor = None
model = None
device = None
training_thread = None

class WakewordTrainingApp:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.processor = AudioProcessor()
        self.model = WakewordModel().to(self.device)
        self.trainer = WakewordTrainer(self.model, self.device)
        self.train_loader = None
        self.val_loader = None
        self.test_loader = None

    def load_data(self, positive_dir, negative_dir, background_dir, batch_size, val_split, test_split):
        try:
            # Load audio files
            def load_audio_files(directory, extensions=['*.wav', '*.mp3', '*.flac']):
                files = []
                for ext in extensions:
                    files.extend(glob.glob(os.path.join(directory, '**', ext), recursive=True))
                return files

            wakeword_files = load_audio_files(positive_dir)
            negative_files = load_audio_files(negative_dir)
            background_files = load_audio_files(background_dir)

            if len(wakeword_files) == 0:
                return f"Hata: {positive_dir} klasÃ¶rÃ¼nde wakeword dosyasÄ± bulunamadÄ±!", None, None

            # Split data
            wakeword_train, wakeword_test = train_test_split(wakeword_files, test_size=test_split, random_state=42)
            wakeword_train, wakeword_val = train_test_split(wakeword_train, test_size=val_split/(1-test_split), random_state=42)

            negative_train, negative_test = train_test_split(negative_files, test_size=test_split, random_state=42)
            negative_train, negative_val = train_test_split(negative_train, test_size=val_split/(1-test_split), random_state=42)

            # Create datasets
            train_dataset = EnhancedWakewordDataset(
                wakeword_train, negative_train[:len(negative_train)//2],
                negative_train[len(negative_train)//2:], background_files,
                self.processor, augment=True
            )

            val_dataset = EnhancedWakewordDataset(
                wakeword_val, negative_val[:len(negative_val)//2],
                negative_val[len(negative_val)//2:], background_files[:50],
                self.processor, augment=False
            )

            # Create dataloaders
            # On Windows, multi-processing workers can cause storage resize errors with numpy/librosa.
            # Use single-process loading there for stability.
            import os as _os
            _num_workers = 0 if _os.name == 'nt' else 2
            self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=_num_workers)
            self.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=_num_workers)

            data_info = f"""
âœ… Veri YÃ¼kleme BaÅŸarÄ±lÄ±!

ğŸ“Š Veri Ä°statistikleri:
â€¢ Wakeword dosyalarÄ±: {len(wakeword_files)}
â€¢ Negative dosyalarÄ±: {len(negative_files)}
â€¢ Background dosyalarÄ±: {len(background_files)}

ğŸ“ˆ Train/Val/Test DaÄŸÄ±lÄ±mÄ±:
â€¢ Train: {len(wakeword_train)} wakeword + {len(negative_train)} negative
â€¢ Validation: {len(wakeword_val)} wakeword + {len(negative_val)} negative
â€¢ Test: {len(wakeword_test)} wakeword + {len(negative_test)} negative

âš™ï¸ Model Parametreleri: {sum(p.numel() for p in self.model.parameters()):,}
ğŸš€ Cihaz: {self.device}
            """

            return data_info, len(train_dataset), len(val_dataset)

        except Exception as e:
            return f"Veri yÃ¼kleme hatasÄ±: {str(e)}", None, None

    def start_training(self, epochs, lr, batch_size, dropout):
        try:
            # Update trainer config
            self.trainer.config.LEARNING_RATE = lr
            self.trainer.config.BATCH_SIZE = batch_size
            self.trainer.config.EPOCHS = epochs
            self.model.dropout.p = dropout

            def training_thread():
                self.trainer.train(self.train_loader, self.val_loader, epochs, self.update_progress)

            thread = threading.Thread(target=training_thread)
            thread.daemon = True
            thread.start()

            return "EÄŸitim baÅŸlatÄ±ldÄ±! Ä°lerlemeyi grafiklerden takip edebilirsiniz."

        except Exception as e:
            return f"EÄŸitim baÅŸlatma hatasÄ±: {str(e)}"

    def update_progress(self, progress, current_batch, total_batches):
        # This will be called during training to update UI
        pass

    def get_training_status(self):
        if not self.trainer.is_training and not self.trainer.training_complete:
            fig = go.Figure()
            fig.add_annotation(text="HenÃ¼z eÄŸitim baÅŸlatÄ±lmadÄ±", xref="paper", yref="paper", x=0.5, y=0.5, showarrow=False)
            fig.update_layout(height=600, title_text="Training Progress")
            return "EÄŸitim baÅŸlatÄ±lmadÄ±", fig

        if self.trainer.is_training:
            status = f"EÄŸitim devam ediyor - Epoch {self.trainer.current_epoch}/{self.trainer.config.EPOCHS}"
        else:
            status = f"EÄŸitim tamamlandÄ± - En iyi validation accuracy: {self.trainer.best_val_acc:.2f}%"

        # Create live plots
        if len(self.trainer.train_losses) > 0:
            fig = make_subplots(rows=2, cols=2, subplot_titles=('Training Loss', 'Validation Loss', 'Training Accuracy', 'Validation Accuracy'))

            epochs = list(range(1, len(self.trainer.train_losses) + 1))

            fig.add_trace(go.Scatter(x=epochs, y=self.trainer.train_losses, name='Train Loss', line=dict(color='blue')), row=1, col=1)
            fig.add_trace(go.Scatter(x=epochs, y=self.trainer.val_losses, name='Val Loss', line=dict(color='red')), row=1, col=2)
            fig.add_trace(go.Scatter(x=epochs, y=self.trainer.train_accuracies, name='Train Acc', line=dict(color='green')), row=2, col=1)
            fig.add_trace(go.Scatter(x=epochs, y=self.trainer.val_accuracies, name='Val Acc', line=dict(color='orange')), row=2, col=2)

            fig.update_layout(height=600, showlegend=True, title_text="Training Progress")
        else:
            fig = go.Figure()
            fig.add_annotation(text="HenÃ¼z eÄŸitim verisi yok", xref="paper", yref="paper", x=0.5, y=0.5, showarrow=False)

        return status, fig

    def stop_training(self):
        if self.trainer.is_training:
            self.trainer.is_training = False
            return "EÄŸitim durduruldu"
        return "EÄŸitim zaten duruyor"

    def save_model(self):
        if os.path.exists('best_wakeword_model.pth'):
            # Create deployment package
            deployment_package = {
                'model_state_dict': self.model.state_dict(),
                'model_config': {
                    'HIDDEN_SIZE': ModelConfig.HIDDEN_SIZE,
                    'NUM_LAYERS': ModelConfig.NUM_LAYERS,
                    'DROPOUT': ModelConfig.DROPOUT,
                    'NUM_CLASSES': ModelConfig.NUM_CLASSES
                },
                'audio_config': {
                    'SAMPLE_RATE': AudioConfig.SAMPLE_RATE,
                    'DURATION': AudioConfig.DURATION,
                    'N_MELS': AudioConfig.N_MELS,
                    'N_FFT': AudioConfig.N_FFT,
                    'HOP_LENGTH': AudioConfig.HOP_LENGTH,
                    'FMIN': AudioConfig.FMIN,
                    'FMAX': AudioConfig.FMAX
                },
                'training_info': {
                    'best_val_accuracy': self.trainer.best_val_acc,
                    'epochs_trained': len(self.trainer.train_losses),
                    'device': str(self.device)
                },
                'classes': ['negative', 'wakeword']
            }

            torch.save(deployment_package, 'wakeword_deployment_model.pth')

            info = f"""
âœ… Model Kaydedildi!

ğŸ“ Kaydedilen Dosyalar:
â€¢ best_wakeword_model.pth (en iyi model)
â€¢ wakeword_deployment_model.pth (deployment paketi)

ğŸ“Š Model Bilgileri:
â€¢ En iyi validation accuracy: {self.trainer.best_val_acc:.2f}%
â€¢ EÄŸitilen epoch sayÄ±sÄ±: {len(self.trainer.train_losses)}
â€¢ Model parametreleri: {sum(p.numel() for p in self.model.parameters()):,}
            """
            return info
        else:
            return "âŒ Kaydedilecek model bulunamadÄ±. Ã–nce eÄŸitim yapÄ±n."

    def test_model(self):
        if not os.path.exists('best_wakeword_model.pth'):
            return "âŒ Test edilecek model bulunamadÄ±", None

        try:
            # Load best model
            checkpoint = torch.load('best_wakeword_model.pth', map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.eval()

            # Test on validation set
            all_preds = []
            all_labels = []

            with torch.no_grad():
                for data, target in self.val_loader:
                    data, target = data.to(self.device), target.to(self.device).squeeze()
                    output = self.model(data)
                    _, predicted = torch.max(output, 1)

                    all_preds.extend(predicted.cpu().numpy())
                    all_labels.extend(target.cpu().numpy())

            # Calculate metrics
            accuracy = accuracy_score(all_labels, all_preds)
            precision = precision_score(all_labels, all_preds, average='weighted')
            recall = recall_score(all_labels, all_preds, average='weighted')
            f1 = f1_score(all_labels, all_preds, average='weighted')

            # Create confusion matrix
            cm = confusion_matrix(all_labels, all_preds)

            fig = make_subplots(rows=2, cols=2, subplot_titles=('Confusion Matrix', 'Metrics', 'ROC Curve', 'Class Distribution'))

            # Confusion Matrix
            fig.add_trace(go.Heatmap(z=cm, colorscale='Blues',
                                     x=['Negative', 'Wakeword'],
                                     y=['Negative', 'Wakeword'],
                                     showscale=True), row=1, col=1)

            # Metrics
            metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
            values = [accuracy, precision, recall, f1]
            fig.add_trace(go.Bar(x=metrics, y=values, name='Metrics', marker_color='lightblue'), row=1, col=2)

            # ROC Curve approximation
            fpr, tpr = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], [0, 0.2, 0.4, 0.6, 0.7, 0.8, 0.85, 0.9, 0.93, 0.96, 1.0]
            fig.add_trace(go.Scatter(x=fpr, y=tpr, name='ROC Curve', line=dict(color='red')), row=2, col=1)

            # Class Distribution
            class_counts = [len([l for l in all_labels if l == 0]), len([l for l in all_labels if l == 1])]
            fig.add_trace(go.Pie(labels=['Negative', 'Wakeword'], values=class_counts, name='Distribution'), row=2, col=2)

            fig.update_layout(height=800, showlegend=True, title_text="Model Evaluation Results")

            result_text = f"""
ğŸ“Š MODEL TEST SONUÃ‡LARI
=======================

ğŸ¯ Performans Metrikleri:
â€¢ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)
â€¢ Precision: {precision:.4f}
â€¢ Recall: {recall:.4f}
â€¢ F1-Score: {f1:.4f}

ğŸ“ˆ Confusion Matrix:
â€¢ True Negative: {cm[0][0]}
â€¢ False Positive: {cm[0][1]}
â€¢ False Negative: {cm[1][0]}
â€¢ True Positive: {cm[1][1]}

ğŸ’¡ Model Kalitesi:
{self.evaluate_model_quality(accuracy, precision, recall, f1)}
            """

            return result_text, fig

        except Exception as e:
            return f"Test hatasÄ±: {str(e)}", None

    def evaluate_model_quality(self, accuracy, precision, recall, f1):
        if f1 >= 0.9:
            return "â€¢ MÃ¼kemmel model! Ãœretim iÃ§in hazÄ±r âœ…"
        elif f1 >= 0.8:
            return "â€¢ Ã‡ok iyi model. Ä°yi generalize ediyor âœ…"
        elif f1 >= 0.7:
            return "â€¢ Ä°yi model. Daha fazla veri ile geliÅŸtirilebilir âš ï¸"
        elif f1 >= 0.6:
            return "â€¢ Orta seviye model. Veri kalitesi kontrol edilmeli âš ï¸"
        else:
            return "â€¢ ZayÄ±f model. Veri ve parametreler gÃ¶zden geÃ§irilmeli âŒ"

# Create app instance
app = WakewordTrainingApp()

# Create Enhanced Gradio Interface
def create_enhanced_interface():
    with gr.Blocks(title="Enhanced Wakeword Training System", theme=gr.themes.Soft(), css="""
        .scrollable-container {
            max-height: 600px;
            overflow-y: auto;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 15px;
        }
        .guide-section {
            margin-bottom: 20px;
        }
        .guide-section h3 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 5px;
        }
    """) as demo:

        gr.Markdown("# ğŸ¯ Enhanced Wakeword Detection Training System")
        gr.Markdown("### ğŸ“š Comprehensive Training with Detailed Documentation")

        with gr.Tabs():

            # Tab 1: Configuration
            with gr.TabItem("âš™ï¸ Configuration"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“ Data Configuration")
                        positive_dir = gr.Textbox(label="Positive Dataset Path", value="./positive_dataset", placeholder="Wakeword recordings")
                        negative_dir = gr.Textbox(label="Negative Dataset Path", value="./negative_dataset", placeholder="Negative samples")
                        background_dir = gr.Textbox(label="Background Noise Path", value="./background_noise", placeholder="Background noise files")

                        gr.Markdown("### ğŸ“Š Data Split")
                        val_split = gr.Slider(label="Validation Split", minimum=0.1, maximum=0.3, value=0.2, step=0.05)
                        test_split = gr.Slider(label="Test Split", minimum=0.1, maximum=0.3, value=0.1, step=0.05)
                        batch_size = gr.Slider(label="Batch Size", minimum=8, maximum=64, value=32, step=8)

                        load_data_btn = gr.Button("ğŸ“¥ Load Data", variant="primary")
                        data_status = gr.Textbox(label="Data Status", interactive=False, lines=8)

                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ§  Model Configuration")
                        hidden_size = gr.Slider(label="Hidden Size", minimum=128, maximum=512, value=256, step=64)
                        num_layers = gr.Slider(label="LSTM Layers", minimum=1, maximum=4, value=2, step=1)
                        dropout = gr.Slider(label="Dropout", minimum=0.0, maximum=0.8, value=0.6, step=0.1)

                        gr.Markdown("### ğŸ¯ Training Configuration")
                        epochs = gr.Slider(label="Epochs", minimum=10, maximum=200, value=100, step=10)
                        learning_rate = gr.Dropdown(label="Learning Rate", choices=["0.001", "0.0005", "0.0001", "0.00005"], value="0.0001")
                        lr = gr.Number(label="Custom Learning Rate", value=0.0001, precision=5)

                        gr.Markdown("### ğŸ”Š Audio Configuration")
                        sample_rate = gr.Dropdown(label="Sample Rate", choices=["8000", "16000", "22050", "44100"], value="16000")
                        duration = gr.Slider(label="Audio Duration (s)", minimum=0.5, maximum=3.0, value=1.7, step=0.1)
                        n_mels = gr.Slider(label="Mel Bands", minimum=40, maximum=128, value=80, step=8)

                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### ğŸ“ˆ Data Augmentation")
                        aug_prob = gr.Slider(label="Augmentation Probability", minimum=0.0, maximum=1.0, value=0.85, step=0.05)
                        noise_factor = gr.Slider(label="Noise Factor", minimum=0.0, maximum=0.5, value=0.15, step=0.05)
                        time_shift = gr.Slider(label="Time Shift (s)", minimum=0.0, maximum=0.5, value=0.3, step=0.05)
                        pitch_shift = gr.Slider(label="Pitch Shift (semitones)", minimum=0.0, maximum=3.0, value=1.5, step=0.5)

                    with gr.Column():
                        gr.Markdown("### ğŸµ Background Mixing")
                        bg_mix_prob = gr.Slider(label="Background Mix Probability", minimum=0.0, maximum=1.0, value=0.7, step=0.05)
                        snr_min = gr.Slider(label="SNR Min (dB)", minimum=-10, maximum=20, value=0, step=5)
                        snr_max = gr.Slider(label="SNR Max (dB)", minimum=0, maximum=30, value=20, step=5)

                        gr.Markdown("### ğŸ›¡ï¸ Training Safety")
                        patience = gr.Slider(label="Early Stopping Patience", minimum=5, maximum=20, value=10, step=1)
                        gradient_clip = gr.Slider(label="Gradient Clip Norm", minimum=0.5, maximum=2.0, value=1.0, step=0.1)

            # Tab 2: Training
            with gr.TabItem("ğŸš€ Training"):
                with gr.Row():
                    with gr.Column(scale=2):
                        gr.Markdown("### ğŸ“Š Training Progress")

                        with gr.Row():
                            start_btn = gr.Button("â–¶ï¸ Start Training", variant="primary")
                            stop_btn = gr.Button("â¹ï¸ Stop Training", variant="secondary")
                            save_btn = gr.Button("ğŸ’¾ Save Model", variant="secondary")

                        training_status = gr.Textbox(label="Training Status", interactive=False)

                        # Live plots
                        training_plots = gr.Plot(label="Training Progress")

                        # Auto-refresh for live updates
                        timer = gr.Timer(value=2.0)

                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“‹ System Info")
                        system_info = gr.Textbox(label="System Information", value=f"""
ğŸ–¥ï¸ Device: {app.device}
ğŸ§® CPU Cores: {os.cpu_count()}
ğŸ’¾ GPU Available: {torch.cuda.is_available()}
                        """, interactive=False, lines=5)

                        gr.Markdown("### ğŸ“Š Current Metrics")
                        current_metrics = gr.Textbox(label="Current Metrics", interactive=False, lines=6)

                        gr.Markdown("### ğŸ’¾ Model Info")
                        model_info = gr.Textbox(label="Model Information", value=f"""
ğŸ“ Parameters: {sum(p.numel() for p in app.model.parameters()):,}
ğŸ—ï¸ Architecture: CNN + LSTM
ğŸ”§ Dropout: {ModelConfig.DROPOUT}
ğŸ“Š Hidden Size: {ModelConfig.HIDDEN_SIZE}
                        """, interactive=False, lines=5)

            # Tab 3: Evaluation
            with gr.TabItem("ğŸ“ˆ Evaluation"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ§ª Model Testing")
                        test_btn = gr.Button("ğŸ”¬ Run Model Test", variant="primary")
                        test_results = gr.Textbox(label="Test Results", interactive=False, lines=10)

                        gr.Markdown("### ğŸ“ Model Files")
                        refresh_files_btn = gr.Button("ğŸ”„ Refresh Files")
                        model_files = gr.Textbox(label="Available Models", interactive=False, lines=3)

                    with gr.Column(scale=2):
                        gr.Markdown("### ğŸ“Š Evaluation Plots")
                        evaluation_plots = gr.Plot(label="Model Evaluation")

            # Tab 4: Enhanced Training Guide (SCROLLABLE)
            with gr.TabItem("ğŸ“š Enhanced Training Guide"):
                gr.Markdown("### ğŸ¯ Comprehensive Wakeword Training Documentation")

                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ¯ Quick Navigation")
                        quick_start = gr.Markdown("""
**ğŸ“– GUIDE SECTIONS:**

1. **Veri Seti HazÄ±rlama**
   â€¢ Miktar ve kalite gereksinimleri
   â€¢ Ã‡eÅŸitlilik stratejileri
   â€¢ Optimum daÄŸÄ±lÄ±m oranlarÄ±

2. **Model Mimarisi**
   â€¢ CNN+LSTM detaylarÄ±
   â€¢ Parametre optimizasyonu
   â€¢ Katman analizi

3. **Training SÃ¼reci**
   â€¢ Pipeline detaylarÄ±
   â€¢ Hyperparameter tuning
   â€¢ Early stopping

4. **Performans**
   â€¢ Metrikler ve yorumlama
   â€¢ Sorun giderme
   â€¢ Best practices

5. **Deployment**
   â€¢ Model export
   â€¢ Production monitoring
   â€¢ A/B testing
                        """)

                        gr.Markdown("### âš ï¸ Common Issues")
                        common_issues = gr.Markdown("""
**ğŸ”§ FREQUENT PROBLEMS:**

**GPU Memory Error:**
â€¢ Batch size'Ä± azaltÄ±n
â€¢ Mixed precision kullanÄ±n
â€¢ Gradient accumulation uygulayÄ±n

**Overfitting:**
â€¢ Dropout'u artÄ±rÄ±n
â€¢ Augmentation'u gÃ¼Ã§lendirin
â€¢ Early stopping kullanÄ±n

**Low Accuracy:**
â€¢ Veri kalitesini kontrol edin
â€¢ Model kapasitesini artÄ±rÄ±n
â€¢ Hyperparameter'larÄ± ayarlayÄ±n

**Slow Training:**
â€¢ GPU kullanÄ±mÄ±nÄ± kontrol edin
â€¢ Data loading'i optimize edin
                        """)

                    with gr.Column(scale=2):
                        gr.Markdown("### ğŸ“– Complete Training Guide")

                        # Use HTML component for scrolling
                        guide_html = f"""
                        <div class="scrollable-container">
                            <div class="guide-section">
                                {markdown.markdown(COMPREHENSIVE_TRAINING_GUIDE)}
                            </div>
                        </div>
                        """

                        guide_content = gr.HTML(guide_html)

        # Event handlers
        def load_data_handler(positive_dir, negative_dir, background_dir, batch_size, val_split, test_split):
            return app.load_data(positive_dir, negative_dir, background_dir, batch_size, val_split, test_split)

        def start_training_handler(epochs, lr, batch_size, dropout):
            return app.start_training(int(epochs), float(lr), int(batch_size), float(dropout))

        def update_training_plots():
            return app.get_training_status()

        def stop_training_handler():
            return app.stop_training()

        def save_model_handler():
            return app.save_model()

        def test_model_handler():
            return app.test_model()

        def refresh_files_handler():
            files = []
            if os.path.exists('best_wakeword_model.pth'):
                files.append("best_wakeword_model.pth")
            if os.path.exists('wakeword_deployment_model.pth'):
                files.append("wakeword_deployment_model.pth")
            return "Available model files:\n" + "\n".join(files) if files else "No model files found"

        # Connect events
        load_data_btn.click(
            load_data_handler,
            inputs=[positive_dir, negative_dir, background_dir, batch_size, val_split, test_split],
            outputs=[data_status]
        )

        start_btn.click(
            start_training_handler,
            inputs=[epochs, lr, batch_size, dropout],
            outputs=[training_status]
        )

        stop_btn.click(
            stop_training_handler,
            outputs=[training_status]
        )

        save_btn.click(
            save_model_handler,
            outputs=[training_status]
        )

        test_btn.click(
            test_model_handler,
            outputs=[test_results, evaluation_plots]
        )

        refresh_files_btn.click(
            refresh_files_handler,
            outputs=[model_files]
        )

        # Auto-refresh training plots
        timer.tick(
            update_training_plots,
            outputs=[training_status, training_plots]
        )

        # Update learning rate when dropdown changes
        learning_rate.change(
            lambda x: gr.Number(value=float(x)),
            inputs=[learning_rate],
            outputs=[lr]
        )

    return demo

if __name__ == "__main__":
    demo = create_enhanced_interface()
    demo.launch(share=True, debug=True)
