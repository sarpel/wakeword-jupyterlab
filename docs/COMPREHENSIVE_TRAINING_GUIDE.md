# ğŸ¯ WAKEWORD DETECTION COMPLETE TRAINING GUIDE
## EFSANEVI DETAYLI KAPSAMLI BELGELEME

---

## ğŸ“‹ Ä°Ã‡Ä°NDEKÄ°LER
1. [Sistem Genel BakÄ±ÅŸ](#sistem-genel-bakÄ±ÅŸ)
2. [Veri Seti HazÄ±rlama](#veri-seti-hazÄ±rlama)
3. [Model Mimarisi](#model-mimarisi)
4. [EÄŸitim SÃ¼reci](#eÄŸitim-sÃ¼reci)
5. [KonfigÃ¼rasyon Parametreleri](#konfigÃ¼rasyon-parametreleri)
6. [Veri ArtÄ±rma Teknikleri](#veri-artÄ±rma-teknikleri)
7. [Performans Optimizasyonu](#performans-optimizasyonu)
8. [Sorun Giderme](#sorun-giderme)
9. [En Ä°yi Uygulamalar](#en-iyi-uygulamalar)

---

## ğŸ—ï¸ SÄ°STEM GENEL BAKIÅ

### Mimari YapÄ±sÄ±
```
Audio Input â†’ Feature Extraction â†’ CNN â†’ LSTM â†’ Classification â†’ Output
    â†“              â†“               â†“     â†“          â†“           â†“
  .wav/.mp3    Mel-Spectrogram   2D Conv  Sequence   Softmax   Wake/No-Wake
  16kHz        80x31 matrix     128 ch   256 hidden   2 classes   Probability
```

### Temel BileÅŸenler
1. **AudioProcessor**: Ses dosyasÄ± yÃ¼kleme ve Ã¶n iÅŸleme
2. **EnhancedWakewordDataset**: Veri seti yÃ¶netimi ve artÄ±rma
3. **WakewordModel**: CNN+LSTM sinir aÄŸÄ± mimarisi
4. **WakewordTrainer**: EÄŸitim sÃ¼reci yÃ¶netimi
5. **Gradio ArayÃ¼zÃ¼**: GÃ¶rsel arayÃ¼z ve canlÄ± izleme

---

## ğŸ“Š VERÄ° SETÄ° HAZIRLAMA

### 1.1. Pozitif Veriler (Wakeword KayÄ±tlarÄ±)

#### Miktar ve Kalite Gereksinimleri
- **Minimum Miktar**: 100-500 temiz wakeword kaydÄ±
- **Ä°deal Miktar**: 1000+ wakeword kaydÄ±
- **Kalite Kriterleri**:
  - SNR (Sinyal/GÃ¼rÃ¼ltÃ¼ OranÄ±): â‰¥ 20dB
  - Clipping olmamalÄ± (ses zirve noktalarÄ± -3dB'den fazla olmamalÄ±)
  - Arka plan gÃ¼rÃ¼ltÃ¼sÃ¼ minimum dÃ¼zeyde
  - Net ve anlaÅŸabilir telaffuz

#### Ã‡eÅŸitlilik Gereksinimleri
```
ğŸ¤ FarklÄ± Mikrofon Ã‡eÅŸitliliÄŸi:
â€¢ Smartphone mikrofonlarÄ±
â€¢ USB mikrofonlar
â€¢ Bluetooth mikrofonlar
â€¢ Laptop dahili mikrofonlar
â€¢ Profesyonel kayÄ±t cihazlarÄ±

ğŸ‘¥ FarklÄ± KonuÅŸanlar:
â€¢ Erkek/KadÄ±n/Ã‡ocuk sesleri
â€¢ FarklÄ± yaÅŸ gruplarÄ±
â€¢ FarklÄ± aksanlar
â€¢ FarklÄ± konuÅŸma hÄ±zlarÄ±
â€¢ FarklÄ± ses tonlarÄ± (yÃ¼ksek/alÃ§ak)

ğŸŒ FarklÄ± Ortamlar:
â€¢ Sessiz oda (SNR > 30dB)
â€¢ Ofis ortamÄ± (SNR 20-25dB)
â€¢ DÄ±ÅŸ mekan (SNR 15-20dB)
â€¢ Araba iÃ§i (SNR 10-15dB)
â€¢ Kafe/restoran (SNR 5-10dB)

ğŸ“ Teknik Ã‡eÅŸitlilik:
â€¢ FarklÄ± sample rate'ler (16kHz, 44.1kHz)
â€¢ FarklÄ± bit derinlikleri (16-bit, 24-bit)
â€¢ FarklÄ± formatlar (WAV, FLAC, MP3)
â€¢ FarklÄ± sÃ¼reler (0.5s - 3.0s)
```

#### KayÄ±t Ä°puÃ§larÄ±
1. **Mesafe**: Mikrofona 15-30cm mesafeden konuÅŸun
2. **Hacim**: Normal konuÅŸma ses seviyesinde
3. **Arka Plan**: MÃ¼mkÃ¼n olduÄŸunca sessiz ortam
4. **Tekrar**: Her wakeword'u 3-5 kez kaydedin
5. **DoÄŸallÄ±k**: GÃ¼nlÃ¼k konuÅŸma tarzÄ±nda, yapay olmayan

### 1.2. Negatif Veriler

#### A. Hard Negative Samples (Fonetik Benzer Kelimeler)
```
ğŸ¯ Phonetically Benzer Kelimeler:
â€¢ "hey" â†’ "hey computer", "hey google", "hey siri"
â€¢ "ok" â†’ "okay", "ok google", "ok computer"
â€¢ "day" â†’ "hey", "they", "say"
â€¢ "night" â†’ "light", "right", "bright"
â€¢ "computer" â†’ "commuter", "compute", "commute"

ğŸ“Š Miktar OranlarÄ±:
â€¢ Her wakeword iÃ§in 4-5 hard negative
â€¢ Toplamda wakeword sayÄ±sÄ±nÄ±n 4.5 katÄ±
â€¢ Ã–rnek: 100 wakeword â†’ 450 hard negative
```

#### B. Random Negative Samples (Genel KonuÅŸma Sesleri)
```
ğŸ—£ï¸ KonuÅŸma Ã‡eÅŸitliliÄŸi:
â€¢ GÃ¼nlÃ¼k konuÅŸmalar
â€¢ Telefon gÃ¶rÃ¼ÅŸmeleri
â€¢ Radyo/TV yayÄ±nlarÄ±
â€¢ Podcast'ler
â€¢ ToplantÄ± kayÄ±tlarÄ±
â€¢ Sokak sesleri
â€¢ AlÄ±ÅŸveriÅŸ merkezi sesleri

ğŸ“Š Miktar OranlarÄ±:
â€¢ Her wakeword iÃ§in 8-9 random negative
â€¢ Toplamda wakeword sayÄ±sÄ±nÄ±n 8.75 katÄ±
â€¢ Ã–rnek: 100 wakeword â†’ 875 random negative
```

#### C. Background Noise Samples (Arka Plan GÃ¼rÃ¼ltÃ¼leri)
```
ğŸ”Š GÃ¼rÃ¼ltÃ¼ Ã‡eÅŸitliliÄŸi:
â€¢ Beyaz gÃ¼rÃ¼ltÃ¼ (white noise)
â€¢ Pembe gÃ¼rÃ¼ltÃ¼ (pink noise)
â€¢ Kahverengi gÃ¼rÃ¼ltÃ¼ (brown noise)
â€¢ Fan/klimalar sesi
â€¢ Trafik gÃ¼rÃ¼ltÃ¼sÃ¼
â€¢ Ä°nsan kalabalÄ±ÄŸÄ± sesi
â€¢ MÃ¼zik (farklÄ± tÃ¼rler)
â€¢ YaÄŸmur/rÃ¼zgar sesi
â€¢ Elektronik cihaz sesleri

ğŸ“Š Miktar OranlarÄ±:
â€¢ Minimum 66 saat arka plan gÃ¼rÃ¼ltÃ¼sÃ¼
â€¢ Her wakeword iÃ§in 10 background sample
â€¢ Toplamda wakeword sayÄ±sÄ±nÄ±n 10 katÄ±
â€¢ Ã–rnek: 100 wakeword â†’ 1000 background sample
```

### 1.3. Veri Dengeleme ve OranlarÄ±

#### Ä°deal Veri DaÄŸÄ±lÄ±mÄ±
```
ğŸ“ˆ Optimum Veri Seti OranlarÄ±:
1 wakeword : 4.5 hard_negative : 8.75 random_negative : 10 background

ğŸ“Š Ã–rnek DaÄŸÄ±lÄ±m (100 wakeword iÃ§in):
â€¢ Wakeword: 100 samples (%4.2)
â€¢ Hard Negative: 450 samples (%18.8)
â€¢ Random Negative: 875 samples (%36.5)
â€¢ Background: 1000 samples (%41.7)
â€¢ TOPLAM: 2425 samples (%100)

âš–ï¸ Dengesiz Verinin Etkileri:
â€¢ Az wakeword â†’ Model wakeword'u Ã¶ÄŸrenemez
â€¢ Ã‡ok wakeword â†’ Model her ÅŸeyi wakeword sanar
â€¢ Az negative â†’ False positive artar
â€¢ Ã‡ok negative â†’ False negative artar
```

#### Train/Validation/Test Split
```
ğŸ¯ Optimum Split OranlarÄ±:
â€¢ Training: %70 (%80 validation/test hariÃ§)
â€¢ Validation: %20 (%25 training hariÃ§)
â€¢ Test: %10 (%12.5 training+validation hariÃ§)

ğŸ“Š 2425 sample iÃ§in:
â€¢ Training: 1698 samples
â€¢ Validation: 485 samples
â€¢ Test: 242 samples

âš ï¸ Ã–nemli Notlar:
â€¢ Her kategoriden orantÄ±lÄ± daÄŸÄ±lÄ±m olmalÄ±
â€¢ AynÄ± kiÅŸi/kayÄ±t farklÄ± setlere daÄŸÄ±lmamalÄ±
â€¢ Random seed ile tutarlÄ±lÄ±k saÄŸlanmalÄ±
```

---

## ğŸ§  MODEL MÄ°MARÄ°SÄ°

### 2.1. CNN+LSTM Mimarisi DetaylÄ± Analiz

#### Katman YapÄ±sÄ±
```python
# Input Shape: (batch, 1, 80, 31) - 80 mel bands, 31 time frames
Input â†’ [Conv2D(1â†’32)] â†’ [ReLU] â†’ [Conv2D(32â†’64)] â†’ [ReLU] â†’
[Conv2D(64â†’128)] â†’ [ReLU] â†’ [AdaptiveAvgPool2d(1,1)] â†’
[Flatten] â†’ [LSTM(128â†’256Ã—2)] â†’ [Dropout(0.6)] â†’ [Linear(256â†’2)]
```

#### CNN KatmanlarÄ± DetaylÄ±
```
ğŸ”² Convolutional Layer 1:
â€¢ Input: (1, 80, 31) - 1 channel, 80 mel bands, 31 time frames
â€¢ Filters: 32 filters of size 3Ã—3
â€¢ Padding: 1 pixel (same padding)
â€¢ Output: (32, 80, 31)
â€¢ Activation: ReLU
â€¢ Parameters: (3Ã—3Ã—1 + 1) Ã— 32 = 320 parameters
â€¢ Purpose: Temel frekans Ã¶zelliklerini Ã§Ä±karma

ğŸ”² Convolutional Layer 2:
â€¢ Input: (32, 80, 31)
â€¢ Filters: 64 filters of size 3Ã—3
â€¢ Padding: 1 pixel
â€¢ Output: (64, 80, 31)
â€¢ Activation: ReLU
â€¢ Parameters: (3Ã—3Ã—32 + 1) Ã— 64 = 18,496 parameters
â€¢ Purpose: Daha karmaÅŸÄ±k akustik desenleri tanÄ±ma

ğŸ”² Convolutional Layer 3:
â€¢ Input: (64, 80, 31)
â€¢ Filters: 128 filters of size 3Ã—3
â€¢ Padding: 1 pixel
â€¢ Output: (128, 80, 31)
â€¢ Activation: ReLU
â€¢ Parameters: (3Ã—3Ã—64 + 1) Ã— 128 = 73,856 parameters
â€¢ Purpose: YÃ¼ksek seviye akustik Ã¶zellikleri Ã§Ä±karma

ğŸ”² Adaptive Average Pooling:
â€¢ Input: (128, 80, 31)
â€¢ Output: (128, 1, 1)
â€¢ Purpose: Sabit boyutlu vektÃ¶r Ã§Ä±karma
â€¢ Avantaj: FarklÄ± ses uzunluklarÄ± iÃ§in Ã§alÄ±ÅŸÄ±r
```

#### LSTM KatmanlarÄ± DetaylÄ±
```
ğŸ”„ LSTM Layer 1:
â€¢ Input: (batch, 1, 128) - sequence length=1, features=128
â€¢ Hidden Size: 256
â€¢ Num Layers: 2
â€¢ Dropout: 0.6 (between layers)
â€¢ Batch First: True
â€¢ Output: (batch, 1, 256)
â€¢ Parameters: 4 Ã— (128Ã—256 + 256Ã—256 + 256) Ã— 2 = 788,992 parameters
â€¢ Purpose: Zaman serisi iÃ§indeki desenleri Ã¶ÄŸrenme

ğŸ¯ LSTM Ã‡alÄ±ÅŸma Prensibi:
â€¢ Forget Gate: Hangi bilgileri unutacaÄŸÄ±na karar verir
â€¢ Input Gate: Yeni bilgileri hafÄ±zaya alÄ±r
â€¢ Output Gate: Ã‡Ä±ktÄ±yÄ± Ã¼retir
â€¢ Cell State: Uzun sÃ¼reli hafÄ±za
â€¢ Hidden State: KÄ±sa sÃ¼reli hafÄ±za

ğŸ’¡ LSTM AvantajlarÄ±:
â€¢ Uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenebilir
â€¢ Ses dizilerindeki temporal desenleri yakalar
â€¢ DeÄŸiÅŸken uzunluktaki girdileri iÅŸleyebilir
```

#### Final Katmanlar
```
ğŸ¯ Dropout Layer:
â€¢ Dropout Rate: 0.6 (%60 nÃ¶ron rastgele devre dÄ±ÅŸÄ±)
â€¢ Purpose: Overfitting'i Ã¶nleme
â€¢ Training sÄ±rasÄ±nda aktif, inference'da pasif

ğŸ¯ Linear Layer:
â€¢ Input: 256 features (LSTM output)
â€¢ Output: 2 classes (wakeword, negative)
â€¢ Activation: Softmax (implicit in CrossEntropyLoss)
â€¢ Parameters: (256 + 1) Ã— 2 = 514 parameters
â€¢ Purpose: SÄ±nÄ±flandÄ±rma kararÄ±

ğŸ“Š Toplam Parametre SayÄ±sÄ±:
â€¢ CNN KatmanlarÄ±: 320 + 18,496 + 73,856 = 92,672
â€¢ LSTM KatmanlarÄ±: 788,992
â€¢ Final Katman: 514
â€¢ TOPLAM: 882,178 parametre (~882K)
```

### 2.2. Model Parametre Optimizasyonu

#### Hidden Size SeÃ§imi
```
ğŸ”¢ Hidden Size SeÃ§enekleri:
â€¢ 128: KÃ¼Ã§Ã¼k veri setleri iÃ§in, hÄ±zlÄ± eÄŸitim
â€¢ 256: Dengeli performans, optimum deÄŸer
â€¢ 512: BÃ¼yÃ¼k veri setleri iÃ§in, daha iyi accuracy
â€¢ 1024: Ã‡ok bÃ¼yÃ¼k veri setleri, yavaÅŸ eÄŸitim

âš–ï¸ Trade-off Analizi:
â€¢ KÃ¼Ã§Ã¼k Hidden Size:
  - Avantaj: HÄ±zlÄ± eÄŸitim, az memory kullanÄ±mÄ±
  - Dezavantaj: DÃ¼ÅŸÃ¼k model kapasitesi, underfitting riski

â€¢ BÃ¼yÃ¼k Hidden Size:
  - Avantaj: YÃ¼ksek model kapasitesi, better accuracy
  - Dezavantaj: YavaÅŸ eÄŸitim, yÃ¼ksek memory kullanÄ±mÄ±, overfitting riski
```

#### Dropout OranÄ±
```
ğŸ¯ Dropout SeÃ§enekleri:
â€¢ 0.2-0.3: BÃ¼yÃ¼k veri setleri iÃ§in
â€¢ 0.4-0.6: Orta bÃ¼yÃ¼klÃ¼kte veri setleri iÃ§in (optimum)
â€¢ 0.7-0.8: KÃ¼Ã§Ã¼k veri setleri iÃ§in

âš ï¸ Dropout Yan Etkileri:
â€¢ Ã‡ok dÃ¼ÅŸÃ¼k dropout â†’ Overfitting
â€¢ Ã‡ok yÃ¼ksek dropout â†’ Underfitting
â€¢ Training: Rastgele nÃ¶ronlarÄ± devre dÄ±ÅŸÄ± bÄ±rakÄ±r
â€¢ Inference: TÃ¼m nÃ¶ronlar aktif, Ã§Ä±kÄ±ÅŸ Ã¶lÃ§eklenir
```

---

## ğŸ¯ EÄÄ°TÄ°M SÃœRECÄ°

### 3.1. Training Pipeline DetaylÄ± AnlatÄ±m

#### 1. Veri YÃ¼kleme ve Ã–n Ä°ÅŸleme
```python
# AdÄ±m 1: Ses DosyalarÄ±nÄ± YÃ¼kle
audio_files = glob.glob("positive_dataset/*.wav")
audio_data = [librosa.load(f, sr=16000)[0] for f in audio_files]

# AdÄ±m 2: Normalizasyon
normalized_audio = [audio / np.max(np.abs(audio)) for audio in audio_data]

# AdÄ±m 3: Boyut Standardizasyonu
target_length = int(16000 * 1.7)  # 1.7 saniye
padded_audio = [pad_or_truncate(audio, target_length) for audio in normalized_audio]

# AdÄ±m 4: Mel-Spectrogram DÃ¶nÃ¼ÅŸÃ¼mÃ¼
mel_specs = [librosa.feature.melspectrogram(
    y=audio, sr=16000, n_mels=80, n_fft=2048,
    hop_length=512, fmin=0, fmax=8000
) for audio in padded_audio]

# AdÄ±m 5: Log Scale DÃ¶nÃ¼ÅŸÃ¼mÃ¼
log_mel_specs = [librosa.power_to_db(mel, ref=np.max) for mel in mel_specs]

# AdÄ±m 6: Tensor DÃ¶nÃ¼ÅŸÃ¼mÃ¼
tensors = [torch.FloatTensor(mel).unsqueeze(0) for mel in log_mel_specs]
```

#### 2. Batch OluÅŸturma ve Shuffling
```python
# AdÄ±m 1: Dataset OluÅŸturma
dataset = EnhancedWakewordDataset(
    wakeword_files=wakeword_files,
    hard_negative_files=hard_neg_files,
    random_negative_files=random_neg_files,
    background_files=background_files,
    processor=processor,
    augment=True
)

# AdÄ±m 2: DataLoader OluÅŸturma
dataloader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=2,
    pin_memory=True
)

# AdÄ±m 3: Batch Ã–rneÄŸi
for batch_idx, (data, target) in enumerate(dataloader):
    # data.shape: (32, 1, 80, 31)
    # target.shape: (32,)

    # GPU'ya taÅŸÄ±
    data = data.to(device)
    target = target.to(device)
```

#### 3. Forward Pass ve Loss Hesaplama
```python
# AdÄ±m 1: Model Forward Pass
with torch.set_grad_enabled(True):  # Training modu
    output = model(data)  # (32, 2)

    # AdÄ±m 2: Loss Hesaplama
    loss = criterion(output, target)

    # AdÄ±m 3: Accuracy Hesaplama
    _, predicted = torch.max(output.data, 1)
    correct = (predicted == target).sum().item()
    accuracy = 100 * correct / target.size(0)
```

#### 4. Backward Pass ve Optimizasyon
```python
# AdÄ±m 1: Gradient SÄ±fÄ±rlama
optimizer.zero_grad()

# AdÄ±m 2: Backward Pass
loss.backward()

# AdÄ±m 3: Gradient Clipping (Patlama Ã¶nleme)
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# AdÄ±m 4: Parametre GÃ¼ncelleme
optimizer.step()

# AdÄ±m 5: Learning Rate Scheduling
scheduler.step(val_accuracy)
```

### 3.2. Learning Rate Scheduling

#### ReduceLROnPlateau MekanizmasÄ±
```python
# KonfigÃ¼rasyon
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='max',           # validation accuracy'yi izle
    factor=0.5,          # learning rate'i yarÄ±ya indir
    patience=5,          # 5 epoch boyunca improvement yoksa
    verbose=True,
    min_lr=1e-7          # minimum learning rate
)

# Ã‡alÄ±ÅŸma Prensibi:
# 1. Her epoch sonunda validation accuracy'yi kontrol et
# 2. EÄŸer accuracy improvement gÃ¶stermiyorsa:
#    - patience sayacÄ±nÄ± artÄ±r
#    - patience >= 5 ise learning rate'i factor ile Ã§arp
# 3. Improvement gÃ¶sterirse sayacÄ± sÄ±fÄ±rla
# 4. Learning rate min_lr'nin altÄ±na inemez
```

#### Learning Rate SeÃ§imi
```
ğŸ¯ FarklÄ± Learning Rate'lerin Etkileri:
â€¢ 0.001: Ã‡ok hÄ±zlÄ±, unstable eÄŸitim, gradient explosion riski
â€¢ 0.0005: HÄ±zlÄ± Ã¶ÄŸrenme, riskli
â€¢ 0.0001: Dengeli, stabil eÄŸitim (OPTIMUM)
â€¢ 0.00005: YavaÅŸ ama stabil
â€¢ 0.00001: Ã‡ok yavaÅŸ, uzun eÄŸitim sÃ¼resi

âš¡ Learning Rate Adaptasyonu:
â€¢ BaÅŸlangÄ±Ã§: 0.0001
â€¢ Patlama durumu: 0.5 Ã— current_lr
â€¢ Minimum: 1e-7
â€¢ Maksimum: 0.001
```

### 3.3. Early Stopping MekanizmasÄ±

#### Early Stopping AlgoritmasÄ±
```python
class EarlyStopping:
    def __init__(self, patience=10, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, val_accuracy):
        score = val_accuracy

        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.counter = 0
```

#### Early Stopping Parametreleri
```
â±ï¸ Patience SeÃ§imi:
â€¢ 5-10 epoch: HÄ±zlÄ± development iÃ§in
â€¢ 10-15 epoch: Normal eÄŸitim iÃ§in (OPTIMUM)
â€¢ 15-20 epoch: KarmaÅŸÄ±k problemler iÃ§in

ğŸ“Š Min Delta SeÃ§imi:
â€¢ 0.001: SÄ±kÄ± durdurma (hassas)
â€¢ 0.005: Normal durdurma
â€¢ 0.01: Esnek durdurma

ğŸ’¡ Early Stopping AvantajlarÄ±:
â€¢ Overfitting'i Ã¶nler
â€¢ EÄŸitim sÃ¼resini kÄ±saltÄ±r
â€¢ En iyi modeli kaydeder
â€¢ Computational kaynaklarÄ± korur
```

---

## âš™ï¸ KONFÄ°GÃœRASYON PARAMETRELERÄ°

### 4.1. AudioConfig DetaylÄ± AÃ§Ä±klama

```python
class AudioConfig:
    SAMPLE_RATE = 16000      # Ã–rnekleme frekansÄ± (Hz)
    DURATION = 1.7          # Ses sÃ¼resi (saniye)
    N_MELS = 80             # Mel bant sayÄ±sÄ±
    N_FFT = 2048            # FFT boyutu
    HOP_LENGTH = 512        # AdÄ±m boyutu
    WIN_LENGTH = 2048       # Pencere boyutu
    FMIN = 0                # Minimum frekans (Hz)
    FMAX = 8000             # Maksimum frekans (Hz)
```

#### Parametre DetaylÄ± Analizi
```
ğŸµ SAMPLE_RATE (16kHz):
â€¢ Neden 16kHz? Ä°nsan sesi iÃ§in yeterli frekans aralÄ±ÄŸÄ±
â€¢ Ä°nsan konuÅŸmasÄ±: 85Hz - 255Hz (fundamental)
â€¢ Harmonikler: 8kHz'e kadar Ã¶nemli
â€¢ Nyquist teoremi: 16kHz â†’ 8kHz maksimum frekans
â€¢ Daha yÃ¼ksek sample rate: Daha fazla veri, daha fazla iÅŸlem
â€¢ Daha dÃ¼ÅŸÃ¼k sample rate: Bilgi kaybÄ±, aliasing

â±ï¸ DURATION (1.7 saniye):
â€¢ Neden 1.7s? Wakeword'lar genellikle 0.5-2s arasÄ±
â€¢ Ã‡ok kÄ±sa: Bilgi eksikliÄŸi
â€¢ Ã‡ok uzun: Gereksiz bilgi, computational maliyet
â€¢ 1.7s: Dengeli deÄŸer, wakeword'u tam kapsar

ğŸ¼ N_MELS (80 Mel BantlarÄ±):
â€¢ Mel skalasÄ±: Ä°nsan iÅŸitmesi ile uyumlu
â€¢ 40 bant: Minimum kabul edilebilir
â€¢ 80 bant: Dengeli Ã§Ã¶zÃ¼nÃ¼rlÃ¼k (OPTIMUM)
â€¢ 128 bant: YÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼k, daha fazla computation
â€¢ Mel bantlarÄ±: DÃ¼ÅŸÃ¼k frekanslarda yoÄŸun, yÃ¼ksek frekanslarda seyrek

ğŸ“Š N_FFT (2048 Nokta FFT):
â€¢ Frekans Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼ belirler
â€¢ 2048: 16kHz / 2048 = 7.8Hz frekans Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼
â€¢ Daha bÃ¼yÃ¼k FFT: Daha iyi frekans Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼, daha az zaman Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼
â€¢ Daha kÃ¼Ã§Ã¼k FFT: Daha iyi zaman Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼, daha az frekans Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼

ğŸ‘£ HOP_LENGTH (512 Ã–rnek AdÄ±mÄ±):
â€¢ Zaman Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼nÃ¼ belirler
â€¢ 512 / 16000 = 32ms zaman adÄ±mÄ±
â€¢ 1.7s / 0.032s = 53 zaman adÄ±mÄ±
â€¢ Daha kÃ¼Ã§Ã¼k hop: Daha fazla zaman adÄ±mÄ±, daha fazla computation
â€¢ Daha bÃ¼yÃ¼k hop: Daha az zaman adÄ±mÄ±, bilgi kaybÄ±

ğŸªŸ WIN_LENGTH (2048 Pencere Boyutu):
â€¢ Frekans Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼nÃ¼ etkiler
â€¢ Genellikle N_FFT ile aynÄ± olur
â€¢ Hamming/Hanning window kullanÄ±lÄ±r
â€¢ Window fonksiyonu: Spektral sÄ±zÄ±ntÄ±yÄ± Ã¶nler
```

### 4.2. ModelConfig DetaylÄ± AÃ§Ä±klama

```python
class ModelConfig:
    HIDDEN_SIZE = 256      # LSTM gizli katman boyutu
    NUM_LAYERS = 2         # LSTM katman sayÄ±sÄ±
    DROPOUT = 0.6          # Dropout oranÄ±
    NUM_CLASSES = 2        # SÄ±nÄ±f sayÄ±sÄ±
```

#### Parametre Optimizasyonu
```
ğŸ§  HIDDEN_SIZE (256):
â€¢ LSTM kapasitesini belirler
â€¢ 128: KÃ¼Ã§Ã¼k model (~400K parametre)
â€¢ 256: Dengeli model (~882K parametre) - OPTIMUM
â€¢ 512: BÃ¼yÃ¼k model (~1.7M parametre)
â€¢ 1024: Ã‡ok bÃ¼yÃ¼k model (~3.4M parametre)

ğŸ“š NUM_LAYERS (2):
â€¢ LSTM katman derinliÄŸi
â€¢ 1 katman: HÄ±zlÄ±, simple iliÅŸkiler
â€¢ 2 katman: Dengeli, karmaÅŸÄ±k iliÅŸkiler - OPTIMUM
â€¢ 3-4 katman: Derin, Ã§ok karmaÅŸÄ±k iliÅŸkiler
â€¢ Fazla katman: Vanishing gradient, eÄŸitim zorluÄŸu

ğŸ­ DROPOUT (0.6):
â€¢ Regularization oranÄ±
â€¢ 0.2-0.3: BÃ¼yÃ¼k veri setleri
â€¢ 0.4-0.6: Orta veri setleri - OPTIMUM
â€¢ 0.7-0.8: KÃ¼Ã§Ã¼k veri setleri
â€¢ Training: Rastgele nÃ¶ronlarÄ± devre dÄ±ÅŸÄ± bÄ±rakÄ±r
â€¢ Inference: TÃ¼m nÃ¶ronlar aktif, Ã§Ä±kÄ±ÅŸ Ã¶lÃ§eklenir
```

### 4.3. TrainingConfig DetaylÄ± AÃ§Ä±klama

```python
class TrainingConfig:
    BATCH_SIZE = 32        # Batch boyutu
    LEARNING_RATE = 0.0001 # Ã–ÄŸrenme oranÄ±
    EPOCHS = 100           # Epoch sayÄ±sÄ±
    VALIDATION_SPLIT = 0.2 # Validation oranÄ±
    TEST_SPLIT = 0.1       # Test oranÄ±
```

#### Training Parametre Analizi
```
ğŸ“¦ BATCH_SIZE (32):
â€¢ Memory ve gradient dengesi
â€¢ 8-16: KÃ¼Ã§Ã¼k batch, daha iyi generalization
â€¢ 32-64: Dengeli batch - OPTIMUM
â€¢ 128-256: BÃ¼yÃ¼k batch, daha hÄ±zlÄ± eÄŸitim
â€¢ KÃ¼Ã§Ã¼k batch: Daha fazla noise, daha iyi generalization
â€¢ BÃ¼yÃ¼k batch: Daha stabil gradient, daha hÄ±zlÄ± eÄŸitim

ğŸ¯ LEARNING_RATE (0.0001):
â€¢ Ã–ÄŸrenme hÄ±zÄ±
â€¢ 0.001: Ã‡ok hÄ±zlÄ±, unstable
â€¢ 0.0001: Dengeli - OPTIMUM
â€¢ 0.00001: Ã‡ok yavaÅŸ
â€¢ Adaptif: ReduceLROnPlateau ile optimizasyon

â° EPOCHS (100):
â€¢ Maksimum epoch sayÄ±sÄ±
â€¢ Early stopping ile otomatik durma
â€¢ GerÃ§ek epoch sayÄ±sÄ± genellikle 20-50 arasÄ±
â€¢ 100: Yeterli bÃ¼yÃ¼k sayÄ±, early stopping ile kontrol

âœ‚ï¸ VALIDATION_SPLIT (0.2):
â€¢ Validation iÃ§in ayrÄ±lan oran
â€¢ 0.1-0.15: KÃ¼Ã§Ã¼k validation seti
â€¢ 0.2-0.25: Dengeli - OPTIMUM
â€¢ 0.3: BÃ¼yÃ¼k validation seti
â€¢ Fazla validation: EÄŸitim verisi azalÄ±r
â€¢ Az validation: Overfitting riski
```

### 4.4. AugmentationConfig DetaylÄ± AÃ§Ä±klama

```python
class AugmentationConfig:
    AUGMENTATION_PROB = 0.85    # ArtÄ±rma olasÄ±lÄ±ÄŸÄ±
    NOISE_FACTOR = 0.15         # GÃ¼rÃ¼ltÃ¼ faktÃ¶rÃ¼
    TIME_SHIFT_MAX = 0.3        # Zaman kaydÄ±rma maksimumu
    PITCH_SHIFT_MAX = 1.5       # Perde deÄŸiÅŸtirme maksimumu
    SPEED_CHANGE_MIN = 0.9      # HÄ±z deÄŸiÅŸtirme minimumu
    SPEED_CHANGE_MAX = 1.1      # HÄ±z deÄŸiÅŸtirme maksimumu
```

#### Augmentation Parametre Optimizasyonu
```
ğŸ² AUGMENTATION_PROB (0.85):
â€¢ Her sample iÃ§in artÄ±rma uygulanma olasÄ±lÄ±ÄŸÄ±
â€¢ 0.5: Orta seviye artÄ±rma
â€¢ 0.7-0.9: YoÄŸun artÄ±rma - OPTIMUM
â€¢ 1.0: Her sample artÄ±rÄ±lÄ±r
â€¢ Ã‡ok yÃ¼ksek: Orijinal veri kaybolabilir
â€¢ Ã‡ok dÃ¼ÅŸÃ¼k: Yetersiz Ã§eÅŸitlilik

ğŸ”Š NOISE_FACTOR (0.15):
â€¢ GÃ¼rÃ¼ltÃ¼ seviyesi
â€¢ 0.05: Hafif gÃ¼rÃ¼ltÃ¼
â€¢ 0.1-0.2: Orta seviye gÃ¼rÃ¼ltÃ¼ - OPTIMUM
â€¢ 0.3+: YoÄŸun gÃ¼rÃ¼ltÃ¼
â€¢ SNR ile iliÅŸkili: -20dB ila -6dB arasÄ±

â±ï¸ TIME_SHIFT_MAX (0.3s):
â€¢ Zaman kaydÄ±rma miktarÄ±
â€¢ Â±0.1s: Hafif kaydÄ±rma
â€¢ Â±0.2-0.4s: Dengeli kaydÄ±rma - OPTIMUM
â€¢ Â±0.5s+: YoÄŸun kaydÄ±rma
â€¢ WAKETIME iÃ§in: Â±0.3s uygun

ğŸµ PITCH_SHIFT_MAX (Â±1.5 semiton):
â€¢ Perde deÄŸiÅŸtirme miktarÄ±
â€¢ Â±0.5: Hafif perde deÄŸiÅŸimi
â€¢ Â±1.0-2.0: Dengeli - OPTIMUM
â€¢ Â±3.0+: YoÄŸun perde deÄŸiÅŸimi
â€¢ 1 semiton = %12 frekans deÄŸiÅŸimi

âš¡ SPEED_CHANGE (0.9x - 1.1x):
â€¢ HÄ±z deÄŸiÅŸtirme oranÄ±
â€¢ 0.95-1.05: Hafif hÄ±z deÄŸiÅŸimi
â€¢ 0.9-1.1: Dengeli - OPTIMUM
â€¢ 0.8-1.2: YoÄŸun hÄ±z deÄŸiÅŸimi
â€¢ Hem perdeyi hem sÃ¼reyi deÄŸiÅŸtirir
```

---

## ğŸ”„ VERÄ° ARTIRMA TEKNÄ°KLERÄ°

### 5.1. Zaman TabanlÄ± ArtÄ±rmalar

#### Time Shifting (Zaman KaydÄ±rma)
```python
def time_shift(audio, max_shift_seconds=0.3, sample_rate=16000):
    """
    Sesi zaman iÃ§inde kaydÄ±rÄ±r
    """
    max_shift_samples = int(max_shift_seconds * sample_rate)
    shift_amount = random.randint(-max_shift_samples, max_shift_samples)

    # np.roll ile dairesel kaydÄ±rma
    shifted_audio = np.roll(audio, shift_amount)

    return shifted_audio
```

**DetaylÄ± AÃ§Ä±klama:**
```
â° Time Shifting MekanizmasÄ±:
â€¢ AmaÃ§: Wakeword'un farklÄ± zamanlarda olmasÄ± senaryosu
â€¢ Ã‡alÄ±ÅŸma: np.roll ile dairesel kaydÄ±rma
â€¢ AralÄ±k: Â±0.3 saniye (Â±4800 sample)
â€¢ Etki: Modelin zaman baÄŸÄ±msÄ±z Ã¶ÄŸrenmesi
â€¢ Limit: Ses sÄ±nÄ±rÄ±nÄ± aÅŸmamalÄ±

ğŸ¯ Uygulama DetaylarÄ±:
â€¢ Pozitif kaydÄ±rma: Wakeword'u ileri alma
â€¢ Negatif kaydÄ±rma: Wakeword'u geri alma
â€¢ SÄ±nÄ±rlar: Ses uzunluÄŸu iÃ§inde kalmalÄ±
â€¢ Random: Her seferinde farklÄ± miktar
```

#### Speed Changing (HÄ±z DeÄŸiÅŸtirme)
```python
def speed_change(audio, speed_factor):
    """
    Ses hÄ±zÄ±nÄ± deÄŸiÅŸtirir
    """
    # librosa time_stretch kullanÄ±mÄ±
    stretched = librosa.effects.time_stretch(audio, rate=speed_factor)

    # Orijinal uzunluÄŸa getirme
    if len(stretched) < len(audio):
        stretched = np.pad(stretched, (0, len(audio) - len(stretched)))
    else:
        stretched = stretched[:len(audio)]

    return stretched
```

**DetaylÄ± AÃ§Ä±klama:**
```
âš¡ Speed Changing Etkileri:
â€¢ AmaÃ§: FarklÄ± konuÅŸma hÄ±zlarÄ± senaryosu
â€¢ Ã‡alÄ±ÅŸma: librosa time_stretch ile PSOLA algoritmasÄ±
â€¢ AralÄ±k: 0.9x - 1.1x (%10 hÄ±z deÄŸiÅŸimi)
â€¢ Etkiler: Hem perdeyi hem sÃ¼reyi deÄŸiÅŸtirir
â€¢ Kalite: PSOLA ile doÄŸal ses koruma

ğŸµ Akustik Etkiler:
â€¢ 0.9x: Daha yavaÅŸ, daha bass perde
â€¢ 1.1x: Daha hÄ±zlÄ±, daha tiz perde
â€¢ DoÄŸallÄ±k: PSOLA sayesinde doÄŸal kalÄ±r
â€¢ Uyum: Time stretching ile uyumlu
```

### 5.2. Frekans TabanlÄ± ArtÄ±rmalar

#### Pitch Shifting (Perde DeÄŸiÅŸtirme)
```python
def pitch_shift(audio, n_steps, sample_rate=16000):
    """
    Ses perdesini deÄŸiÅŸtirir
    """
    # librosa pitch_shift kullanÄ±mÄ±
    shifted = librosa.effects.pitch_shift(
        y=audio,
        sr=sample_rate,
        n_steps=n_steps
    )

    return shifted
```

**DetaylÄ± AÃ§Ä±klama:**
```
ğŸ¼ Pitch Shifting DetaylarÄ±:
â€¢ AmaÃ§: FarklÄ± ses tonlarÄ± senaryosu
â€¢ Ã‡alÄ±ÅŸma: librosa pitch_shift ile PSOLA
â€¢ Birim: Semiton (yarÄ±m perde)
â€¢ AralÄ±k: Â±1.5 semiton
â€¢ Etki: Sadece perdeyi deÄŸiÅŸtirir, sÃ¼reyi korur

ğŸ¤ Semiton KavramÄ±:
â€¢ 1 semiton = %12 frekans deÄŸiÅŸimi
â€¢ 12 semiton = 1 oktav
â€¢ Ä°nsan kulaÄŸÄ±: 1-2 semiton farkÄ± algÄ±lar
â€¢ DoÄŸallÄ±k: Â±3 semitone'a kadar doÄŸal

ğŸ‘¥ FarklÄ± Sesler iÃ§in:
â€¢ Erkek sesleri: DÃ¼ÅŸÃ¼k perde deÄŸiÅŸimi
â€¢ KadÄ±n sesleri: YÃ¼ksek perde deÄŸiÅŸimi
â€¢ Ã‡ocuk sesleri: Orta perde deÄŸiÅŸimi
â€¢ Uyum: DoÄŸal konuÅŸma aralÄ±ÄŸÄ±nda
```

### 5.3. GÃ¼rÃ¼ltÃ¼ TabanlÄ± ArtÄ±rmalar

#### Background Noise Mixing (Arka Plan GÃ¼rÃ¼ltÃ¼sÃ¼ KarÄ±ÅŸtÄ±rma)
```python
def mix_with_background(audio, background_audio, target_snr_db):
    """
    Sesi arka plan gÃ¼rÃ¼ltÃ¼sÃ¼ ile karÄ±ÅŸtÄ±rÄ±r
    """
    # SNR hesaplama
    signal_power = np.mean(audio ** 2)
    noise_power = np.mean(background_audio ** 2)

    # Hedef SNR iÃ§in Ã¶lÃ§eklendirme
    if noise_power > 0:
        target_noise_power = signal_power / (10 ** (target_snr_db / 10))
        scale_factor = np.sqrt(target_noise_power / noise_power)

        # GÃ¼rÃ¼ltÃ¼yÃ¼ Ã¶lÃ§eklendir ve karÄ±ÅŸtÄ±r
        scaled_noise = background_audio * scale_factor
        mixed = audio + scaled_noise

        # Normalize etme
        max_val = np.max(np.abs(mixed))
        if max_val > 0:
            mixed = mixed / max_val * 0.95

        return mixed

    return audio
```

**DetaylÄ± AÃ§Ä±klama:**
```
ğŸ”Š SNR (Signal-to-Noise Ratio) KavramÄ±:
â€¢ TanÄ±m: Sinyal gÃ¼cÃ¼nÃ¼n gÃ¼rÃ¼ltÃ¼ gÃ¼cÃ¼ne oranÄ±
â€¢ Birim: dB (desibel)
â€¢ FormÃ¼l: SNR_dB = 10 Ã— log10(P_signal / P_noise)
â€¢ YÃ¼ksek SNR: Temiz sinyal
â€¢ DÃ¼ÅŸÃ¼k SNR: GÃ¼rÃ¼ltÃ¼lÃ¼ sinyal

ğŸ“Š FarklÄ± SNR Seviyeleri:
â€¢ 20dB+: Ã‡ok temiz (stÃ¼dyo kalitesi)
â€¢ 10-20dB: Temiz (ofis ortamÄ±)
â€¢ 5-10dB: Orta (dÄ±ÅŸ mekan)
â€¢ 0-5dB: GÃ¼rÃ¼ltÃ¼lÃ¼ (kafe, trafik)
â€¢ 0dB-: Ã‡ok gÃ¼rÃ¼ltÃ¼lÃ¼ (fabrika, konser)

ğŸ¯ SNR SeÃ§imi:
â€¢ Training: 0-20dB arasÄ± geniÅŸ range
â€¢ Validation: 5-15dB arasÄ± dar range
â€¢ Real-world: Genellikle 5-15dB arasÄ±
```

#### Additive Noise (Toplamsal GÃ¼rÃ¼ltÃ¼)
```python
def add_noise(audio, noise_factor=0.15):
    """
    Sese rastgele gÃ¼rÃ¼ltÃ¼ ekler
    """
    # Beyaz gÃ¼rÃ¼ltÃ¼ oluÅŸturma
    noise = np.random.normal(0, noise_factor, len(audio))

    # GÃ¼rÃ¼ltÃ¼yÃ¼ ekleme
    noisy_audio = audio + noise

    return noisy_audio
```

**DetaylÄ± AÃ§Ä±klama:**
```
ğŸ² GÃ¼rÃ¼ltÃ¼ TÃ¼rleri:
â€¢ Beyaz gÃ¼rÃ¼ltÃ¼: TÃ¼m frekanslarda eÅŸit gÃ¼Ã§
â€¢ Pembe gÃ¼rÃ¼ltÃ¼: DÃ¼ÅŸÃ¼k frekanslarda daha gÃ¼Ã§lÃ¼
â€¢ Kahverengi gÃ¼rÃ¼ltÃ¼: Daha da dÃ¼ÅŸÃ¼k frekanslarda gÃ¼Ã§lÃ¼
â€¢ Mavi gÃ¼rÃ¼ltÃ¼: YÃ¼ksek frekanslarda daha gÃ¼Ã§lÃ¼

ğŸ”¢ Noise Factor SeÃ§imi:
â€¢ 0.05: Hafif gÃ¼rÃ¼ltÃ¼
â€¢ 0.1-0.2: Orta seviye gÃ¼rÃ¼ltÃ¼ - OPTIMUM
â€¢ 0.3+: YoÄŸun gÃ¼rÃ¼ltÃ¼
â€¢ Sinyal gÃ¼cÃ¼ne gÃ¶re normalize edilmeli
```

### 5.4. BileÅŸik ArtÄ±rma Stratejileri

#### ArtÄ±rma Pipeline'Ä±
```python
def comprehensive_augmentation(audio, config):
    """
    KapsamlÄ± artÄ±rma pipeline'Ä±
    """
    augmented = audio.copy()

    # Zaman tabanlÄ± artÄ±rmalar
    if random.random() < config.AUGMENTATION_PROB:
        augmented = time_shift(augmented, config.TIME_SHIFT_MAX)

    if random.random() < config.AUGMENTATION_PROB:
        speed_factor = random.uniform(config.SPEED_CHANGE_MIN, config.SPEED_CHANGE_MAX)
        augmented = speed_change(augmented, speed_factor)

    # Frekans tabanlÄ± artÄ±rmalar
    if random.random() < config.AUGMENTATION_PROB:
        n_steps = random.uniform(-config.PITCH_SHIFT_MAX, config.PITCH_SHIFT_MAX)
        augmented = pitch_shift(augmented, n_steps)

    # GÃ¼rÃ¼ltÃ¼ tabanlÄ± artÄ±rmalar
    if random.random() < config.AUGMENTATION_PROB:
        augmented = add_noise(augmented, config.NOISE_FACTOR)

    # Background mixing
    if random.random() < config.BACKGROUND_MIX_PROB:
        bg_audio = random.choice(background_cache)
        target_snr = random.uniform(config.SNR_MIN, config.SNR_MAX)
        augmented = mix_with_background(augmented, bg_audio, target_snr)

    return augmented
```

**DetaylÄ± AÃ§Ä±klama:**
```
ğŸ¯ ArtÄ±rma Stratejisi:
â€¢ AÅŸamalÄ±: FarklÄ± artÄ±rma tÃ¼rleri sÄ±rayla
â€¢ OlasÄ±lÄ±ksal: Her artÄ±rma iÃ§in ayrÄ± olasÄ±lÄ±k
â€¢ BaÄŸÄ±msÄ±z: Birbirini etkilemez
â€¢ Normalize: Son aÅŸamada normalize etme

ğŸ“Š ArtÄ±rma DaÄŸÄ±lÄ±mÄ±:
â€¢ %85: En az bir artÄ±rma uygulanÄ±r
â€¢ %50: Birden fazla artÄ±rma uygulanÄ±r
â€¢ %15: HiÃ§ artÄ±rma uygulanmaz (orijinal korunur)

âš–ï¸ Denge NoktalarÄ±:
â€¢ Ã‡ok fazla artÄ±rma: Orijinali bozar
â€¢ Ã‡ok az artÄ±rma: Ã‡eÅŸitlilik yetersiz
â€¢ %85: Dengeli oran - OPTIMUM
```

---

## ğŸ“ˆ PERFORMANS OPTÄ°MÄ°ZASYONU

### 6.1. GPU Optimizasyonu

#### GPU Memory YÃ¶netimi
```python
# GPU memory monitoring
def monitor_gpu_memory():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9  # GB
        reserved = torch.cuda.memory_reserved() / 1e9   # GB
        total = torch.cuda.get_device_properties(0).total_memory / 1e9

        print(f"GPU Memory: {allocated:.1f}GB / {total:.1f}GB ({allocated/total*100:.1f}%)")
        print(f"Reserved: {reserved:.1f}GB")

        return allocated, reserved, total
```

**GPU Optimizasyon Stratejileri:**
```
ğŸ’¾ Memory Optimizasyonu:
â€¢ Pin Memory: CPU-GPU transfer hÄ±zlandÄ±rma
â€¢ Gradient Accumulation: BÃ¼yÃ¼k batch'ler iÃ§in
â€¢ Mixed Precision: HafÄ±za ve hÄ±z artÄ±ÅŸÄ±
â€¢ Gradient Checkpointing: Memory tasarrufu

âš¡ HÄ±z Optimizasyonu:
â€¢ CUDA Kernels: Optimize edilmiÅŸ GPU fonksiyonlarÄ±
â€¢ Asynchronous Transfer: Paralel data transferi
â€¢ Tensor Cores: Modern GPU'lar iÃ§in
â€¢ Memory Layout: Optimize edilmiÅŸ veri dÃ¼zeni

ğŸ”§ Memory AyarlarÄ±:
â€¢ Batch Size: Memory sÄ±nÄ±rlarÄ±na gÃ¶re
â€¢ Num Workers: Paralel data loading
â€¢ Pin Memory: True (GPU iÃ§in)
â€¢ Non-blocking: True (asynchronous)
```

#### Mixed Precision Training
```python
# Mixed precision iÃ§in scaler
scaler = torch.cuda.amp.GradScaler()

def train_epoch_mixed_precision(model, dataloader, optimizer, criterion):
    model.train()

    for batch_idx, (data, target) in enumerate(dataloader):
        data, target = data.to(device), target.to(device)

        # Mixed precision forward pass
        with torch.cuda.amp.autocast():
            output = model(data)
            loss = criterion(output, target)

        # Backward pass with scaling
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()
```

**Mixed Precision AvantajlarÄ±:**
```
ğŸš€ Mixed Precision FaydalarÄ±:
â€¢ Memory: %50 memory tasarrufu
â€¢ HÄ±z: %2-3x hÄ±z artÄ±ÅŸÄ±
â€¢ Kalite: Minimum accuracy kaybÄ±
â€¢ Uyum: Modern GPU'lar ile Ã§alÄ±ÅŸÄ±r

âš ï¸ Dikkat Edilmesi Gerekenler:
â€¢ Gradient scaling gerekli
â€¢ Numerical stability sorunlarÄ±
â€¢ TÃ¼m iÅŸlemler desteklemeyebilir
â€¢ Model architecture etkisi
```

### 6.2. Data Loading Optimizasyonu

#### Paralel Data Loading
```python
# Optimize edilmiÅŸ DataLoader
dataloader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,        # Paralel worker sayÄ±sÄ±
    pin_memory=True,     # GPU iÃ§in pin memory
    persistent_workers=True,  # Worker'larÄ± canlÄ± tut
    prefetch_factor=2,   # Prefetch faktÃ¶rÃ¼
    non_blocking=True    # Non-blocking transfer
)
```

**Optimizasyon Parametreleri:**
```
ğŸ‘¥ Num Workers SeÃ§imi:
â€¢ 0: Ana process'te loading (yavaÅŸ)
â€¢ 2-4: Optimum deÄŸer
â€¢ 8+: Ã‡ok fazla worker (overhead)
â€¢ CPU core sayÄ±sÄ±na gÃ¶re ayarla

ğŸ“Œ Pin Memory:
â€¢ True: GPU memory'de Ã¶nceden alan ayÄ±rÄ±r
â€¢ False: Normal memory allocation
â€¢ Speed: %10-20 hÄ±z artÄ±ÅŸÄ± saÄŸlar
â€¢ Memory: Biraz daha fazla memory kullanÄ±r

â¡ Persistent Workers:
â€¢ True: Worker'larÄ± epoch'lar arasÄ±nda canlÄ± tutar
â€¢ False: Her epoch'da yeniden baÅŸlatÄ±r
â€¢ Speed: Epoch baÅŸÄ±na hÄ±zlandÄ±rma
â€¢ Memory: SÃ¼rekli memory kullanÄ±mÄ±
```

### 6.3. Model Optimizasyonu

#### Model Pruning
```python
def prune_model(model, pruning_amount=0.2):
    """
    Model pruning ile optimizasyon
    """
    parameters_to_prune = []

    for name, module in model.named_modules():
        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):
            parameters_to_prune.append((module, 'weight'))

    # Global pruning
    prune.global_unstructured(
        parameters_to_prune,
        pruning_method=prune.L1Unstructured,
        amount=pruning_amount
    )

    # Pruning'i kalÄ±cÄ± hale getirme
    for module, param_name in parameters_to_prune:
        prune.remove(module, param_name)

    return model
```

**Pruning Stratejileri:**
```
âœ‚ï¸ Pruning TÃ¼rleri:
â€¢ L1 Unstructured: En kÃ¼Ã§Ã¼k aÄŸÄ±rlÄ±klarÄ± kaldÄ±rÄ±r
â€¢ Structured: TÃ¼m filtreleri/katmanlarÄ± kaldÄ±rÄ±r
â€¢ Global: Model genelinde pruning
â€¢ Local: Katman bazÄ±nda pruning

ğŸ“Š Pruning MiktarÄ±:
â€¢ %10-20: Hafif pruning
â€¢ %30-50: Orta seviye pruning
â€¢ %60+: AÄŸÄ±r pruning
â€¢ Accuracy loss ile dengeli olmalÄ±

âš–ï¸ Pruning Trade-off'larÄ±:
â€¢ Avantaj: Daha kÃ¼Ã§Ã¼k model, daha hÄ±zlÄ± inference
â€¢ Dezavantaj: Potansiyel accuracy kaybÄ±
â€¢ Uygulama: Deployment iÃ§in ideal
```

---

## ğŸš¨ SORUN GÄ°DERME

### 7.1. YaygÄ±n EÄŸitim SorunlarÄ±

#### 7.1.1. Overfitting

**Belirtiler:**
```
ğŸ“ˆ Overfitting GÃ¶stergeleri:
â€¢ Train accuracy sÃ¼rekli artÄ±yor (%95+)
â€¢ Validation accuracy dÃ¼ÅŸÃ¼yor veya sabit
â€¢ Train loss sÃ¼rekli azalÄ±yor
â€¢ Validation loss artÄ±yor
â€¢ Confusion matrix'de imbalance
```

**Ã‡Ã¶zÃ¼mler:**
```python
# 1. Dropout artÄ±rma
model.dropout.p = 0.7  # 0.6'dan 0.7'a

# 2. Data augmentation gÃ¼Ã§lendirme
config.AUGMENTATION_PROB = 0.9  # 0.85'ten 0.9'a
config.NOISE_FACTOR = 0.2       # 0.15'ten 0.2'a

# 3. Early stopping uygulama
early_stopping = EarlyStopping(patience=8, min_delta=0.001)

# 4. Learning rate azaltma
optimizer = optim.Adam(model.parameters(), lr=0.00005)  # 0.0001'den 0.00005'e

# 5. Regularization ekleme
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)
```

**Prevention (Ã–nleme) Stratejileri:**
```
ğŸ›¡ï¸ Overfitting Ã–nleme:
â€¢ Cross-validation kullanma
â€¢ Daha fazla veri toplama
â€¢ Augmentation Ã§eÅŸitliliÄŸi artÄ±rma
â€¢ Model karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± azaltma
â€¢ Regularization teknikleri
â€¢ Early stopping implementasyonu
```

#### 7.1.2. Underfitting

**Belirtiler:**
```
ğŸ“‰ Underfitting GÃ¶stergeleri:
â€¢ Train accuracy dÃ¼ÅŸÃ¼k (<%70)
â€¢ Validation accuracy de dÃ¼ÅŸÃ¼k
â€¢ Her iki loss da yÃ¼ksek
â€¢ Model Ã¶ÄŸrenemiyor
â€¢ Confusion matrix'de random daÄŸÄ±lÄ±m
```

**Ã‡Ã¶zÃ¼mler:**
```python
# 1. Model kapasitesini artÄ±rma
model_config.HIDDEN_SIZE = 512    # 256'dan 512'e
model_config.NUM_LAYERS = 3       # 2'den 3'e

# 2. Learning rate artÄ±rma
optimizer = optim.Adam(model.parameters(), lr=0.001)  # 0.0001'den 0.001'e

# 3. Dropout azaltma
model.dropout.p = 0.3  # 0.6'dan 0.3'a

# 4. Epoch sayÄ±sÄ±nÄ± artÄ±rma
training_config.EPOCHS = 200  # 100'den 200'e

# 5. Augmentation azaltma
config.AUGMENTATION_PROB = 0.5  # 0.85'ten 0.5'e
```

#### 7.1.3. Gradient Explosion

**Belirtiler:**
```
ğŸ’¥ Gradient Explosion Belirtileri:
â€¢ Loss aniden Ã§ok bÃ¼yÃ¼k deÄŸerler alÄ±yor
â€¢ Training NaN/Inf deÄŸerleri Ã¼retiyor
â€¢ Model aÄŸÄ±rlÄ±klarÄ± Ã§ok bÃ¼yÃ¼yor
â€¢ Accuracy dÃ¼ÅŸtÃ¼ (model Ã§Ã¶ktÃ¼)
â€¢ GPU memory hatalarÄ±
```

**Ã‡Ã¶zÃ¼mler:**
```python
# 1. Gradient clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 2. Learning rate azaltma
optimizer = optim.Adam(model.parameters(), lr=0.00001)

# 3. Batch normalization ekleme
model.conv1 = nn.Conv2d(1, 32, 3, padding=1)
model.bn1 = nn.BatchNorm2d(32)

# 4. Weight initialization
def init_weights(m):
    if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
    elif isinstance(m, nn.Linear):
        nn.init.xavier_normal_(m.weight)

model.apply(init_weights)
```

#### 7.1.4. Vanishing Gradient

**Belirtiler:**
```
ğŸŒ€ Vanishing Gradient Belirtileri:
â€¢ Early katmanlar Ã¶ÄŸrenmiyor
â€¢ Loss Ã§ok yavaÅŸ azalÄ±yor
â€¢ Accuracy sabit kalÄ±yor
â€¢ Gradientler Ã§ok kÃ¼Ã§Ã¼k
â€¢ Deep network'lerde sÄ±k gÃ¶rÃ¼lÃ¼r
```

**Ã‡Ã¶zÃ¼mler:**
```python
# 1. Batch normalization ekleme
model = nn.Sequential(
    nn.Conv2d(1, 32, 3, padding=1),
    nn.BatchNorm2d(32),
    nn.ReLU(),
    nn.Conv2d(32, 64, 3, padding=1),
    nn.BatchNorm2d(64),
    nn.ReLU()
)

# 2. Residual connections
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out += residual
        out = F.relu(out)
        return out

# 3. LSTM layer sayÄ±sÄ±nÄ± azaltma
model_config.NUM_LAYERS = 1  # 2'den 1'e
```

### 7.2. Memory ve Performance SorunlarÄ±

#### 7.2.1. GPU Memory Error

**Belirtiler:**
```
ğŸ’¾ GPU Memory Error Belirtileri:
â€¢ CUDA out of memory hatasÄ±
â€¢ Training baÅŸarÄ±sÄ±z oluyor
â€¢ Batch size kÃ¼Ã§Ã¼ltmek sorunu Ã§Ã¶zÃ¼yor
â€¢ GPU memory usage %90+
```

**Ã‡Ã¶zÃ¼mler:**
```python
# 1. Batch size kÃ¼Ã§Ã¼ltme
training_config.BATCH_SIZE = 16  # 32'den 16'ya

# 2. Gradient accumulation
accumulation_steps = 2
optimizer.zero_grad()
for i, (data, target) in enumerate(dataloader):
    data, target = data.to(device), target.to(device)
    output = model(data)
    loss = criterion(output, target)
    loss = loss / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 3. Mixed precision training
scaler = torch.cuda.amp.GradScaler()
with torch.cuda.amp.autocast():
    output = model(data)
    loss = criterion(output, target)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
optimizer.zero_grad()

# 4. Data loading optimization
dataloader = DataLoader(
    dataset,
    batch_size=16,
    num_workers=2,
    pin_memory=True,
    persistent_workers=True
)
```

#### 7.2.2. YavaÅŸ Training

**Belirtiler:**
```
ğŸŒ YavaÅŸ Training Belirtileri:
â€¢ Epoch baÅŸÄ±na Ã§ok zaman alÄ±yor
â€¢ GPU kullanÄ±mÄ± dÃ¼ÅŸÃ¼k
â€¢ CPU bottleneck var
â€¢ Data loading yavaÅŸ
```

**Ã‡Ã¶zÃ¼mler:**
```python
# 1. Paralel data loading
dataloader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=4,  # Worker sayÄ±sÄ±nÄ± artÄ±r
    pin_memory=True,
    persistent_workers=True,
    prefetch_factor=2
)

# 2. GPU memory optimization
torch.backends.cudnn.benchmark = True  # CNN iÃ§in
torch.backends.cuda.matmul.allow_tf32 = True  # TensorFloat-32

# 3. Model optimization ile baÅŸlatma
model = model.to(device).train()  # Training modunda

# 4. Async data loading
for batch_idx, (data, target) in enumerate(dataloader):
    data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
```

### 7.3. Veri Kalitesi SorunlarÄ±

#### 7.3.1. DÃ¼ÅŸÃ¼k Accuracy

**Belirtiler:**
```
ğŸ“Š DÃ¼ÅŸÃ¼k Accuracy Belirtileri:
â€¢ Test accuracy < %70
â€¢ Validation accuracy < %75
â€¢ Model rastgele tahmin yapÄ±yor
â€¢ Confusion matrix dengesiz
```

**Ã‡Ã¶zÃ¼mler:**
```python
# 1. Veri kalitesini kontrol etme
def check_data_quality():
    # Ses dosyalarÄ±nÄ± kontrol et
    for file in wakeword_files[:10]:
        audio, sr = librosa.load(file, sr=16000)
        if len(audio) == 0:
            print(f"BoÅŸ dosya: {file}")
        if np.max(np.abs(audio)) > 1.0:
            print(f"Clipping: {file}")
        if len(audio) < sr * 0.5:
            print(f"Ã‡ok kÄ±sa: {file}")

# 2. Veri setini dengeleme
def balance_dataset():
    # Her kategoriden eÅŸit sayÄ±da sample
    min_samples = min(len(wakeword_files), len(negative_files))
    wakeword_balanced = random.sample(wakeword_files, min_samples)
    negative_balanced = random.sample(negative_files, min_samples)
    return wakeword_balanced + negative_balanced

# 3. Augmentation gÃ¼Ã§lendirme
config.AUGMENTATION_PROB = 0.95
config.NOISE_FACTOR = 0.25
config.TIME_SHIFT_MAX = 0.5
```

#### 7.3.2. Imbalanced Dataset

**Belirtiler:**
```
âš–ï¸ Imbalanced Dataset Belirtileri:
â€¢ Bir sÄ±nÄ±f diÄŸerinden Ã§ok daha fazla
â€¢ Model Ã§oÄŸunluk sÄ±nÄ±fÄ±nÄ± tahmin ediyor
â€¢ Precision/Recall dengesiz
â€¢ F1-score dÃ¼ÅŸÃ¼k
```

**Ã‡Ã¶zÃ¼mler:**
```python
# 1. Class weights
class_counts = [len(negative_files), len(wakeword_files)]
class_weights = torch.tensor([
    len(wakeword_files) / len(negative_files),
    len(negative_files) / len(wakeword_files)
], dtype=torch.float32).to(device)

criterion = nn.CrossEntropyLoss(weight=class_weights)

# 2. Oversampling
from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)

# 3. Undersampling
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(X, y)

# 4. SMOTE
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
```

---

## ğŸ† EN Ä°YÄ° UYGULAMALAR

### 8.1. Veri Toplama ve HazÄ±rlama

#### 8.1.1. Kaliteli Veri Toplama Ä°puÃ§larÄ±
```
ğŸ¤ KayÄ±t OrtamÄ±:
â€¢ Sessiz bir oda (SNR > 30dB)
â€¢ YankÄ±sÄ±z ortam (halÄ±, perde vb.)
â€¢ Sabit mesafe (15-30cm)
â€¢ Sabit ses seviyesi
â€¢ Arka plan gÃ¼rÃ¼ltÃ¼sÃ¼ minimum

ğŸ‘¥ KonuÅŸan Ã‡eÅŸitliliÄŸi:
â€¢ FarklÄ± yaÅŸ gruplarÄ±
â€¢ FarklÄ± cinsiyetler
â€¢ FarklÄ± aksanlar
â€¢ FarklÄ± konuÅŸma stilleri
â€¢ FarklÄ± ses tonlarÄ±

ğŸ“ Mikrofon Ã‡eÅŸitliliÄŸi:
â€¢ Smartphone mikrofonlarÄ±
â€¢ USB mikrofonlar
â€¢ Bluetooth kulaklÄ±klar
â€¢ Laptop dahili mikrofonlar
â€¢ Profesyonel kayÄ±t ekipmanlarÄ±

ğŸ”Š Teknik Kalite:
â€¢ Sample rate: 16kHz veya Ã¼zeri
â€¢ Bit depth: 16-bit veya Ã¼zeri
â€¢ Format: WAV (kayÄ±psÄ±z)
â€¢ SÃ¼re: 1-2 saniye
â€¢ Clipping olmamalÄ±
```

#### 8.1.2. Veri Ã–n Ä°ÅŸleme Best Practices
```python
def optimal_preprocessing_pipeline(audio_path):
    """
    Optimum veri Ã¶n iÅŸleme pipeline'Ä±
    """
    # 1. YÃ¼kleme
    audio, sr = librosa.load(audio_path, sr=16000)

    # 2. Kalite kontrolÃ¼
    if len(audio) == 0:
        return None
    if np.max(np.abs(audio)) > 1.0:
        audio = audio / np.max(np.abs(audio)) * 0.95

    # 3. GÃ¼rÃ¼ltÃ¼ azaltma (isteÄŸe baÄŸlÄ±)
    if noisy_environment:
        audio = reduce_noise(audio, sr)

    # 4. Normalizasyon
    audio = audio / np.max(np.abs(audio))

    # 5. Boyut standardizasyonu
    target_length = int(sr * 1.7)
    if len(audio) > target_length:
        start_idx = random.randint(0, len(audio) - target_length)
        audio = audio[start_idx:start_idx + target_length]
    else:
        audio = np.pad(audio, (0, target_length - len(audio)))

    # 6. Feature extraction
    mel_spec = librosa.feature.melspectrogram(
        y=audio, sr=sr, n_mels=80, n_fft=2048,
        hop_length=512, fmin=0, fmax=8000
    )

    # 7. Log scale dÃ¶nÃ¼ÅŸÃ¼mÃ¼
    log_mel = librosa.power_to_db(mel_spec, ref=np.max)

    return log_mel
```

### 8.2. Model EÄŸitimi Best Practices

#### 8.2.1. Hyperparameter Tuning
```python
def hyperparameter_tuning():
    """
    Sistematik hyperparameter tuning
    """
    param_grid = {
        'learning_rate': [0.001, 0.0005, 0.0001, 0.00005],
        'batch_size': [16, 32, 64],
        'hidden_size': [128, 256, 512],
        'dropout': [0.3, 0.5, 0.7],
        'augmentation_prob': [0.5, 0.7, 0.9]
    }

    best_params = None
    best_accuracy = 0

    for lr in param_grid['learning_rate']:
        for batch_size in param_grid['batch_size']:
            for hidden_size in param_grid['hidden_size']:
                for dropout in param_grid['dropout']:
                    for aug_prob in param_grid['augmentation_prob']:
                        # Model oluÅŸtur ve eÄŸit
                        model = create_model(hidden_size, dropout)
                        accuracy = train_and_evaluate(model, lr, batch_size, aug_prob)

                        if accuracy > best_accuracy:
                            best_accuracy = accuracy
                            best_params = {
                                'learning_rate': lr,
                                'batch_size': batch_size,
                                'hidden_size': hidden_size,
                                'dropout': dropout,
                                'augmentation_prob': aug_prob
                            }

    return best_params, best_accuracy
```

#### 8.2.2. Cross-Validation Implementasyonu
```python
from sklearn.model_selection import StratifiedKFold

def cross_validation_training():
    """
    K-fold cross-validation
    """
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    fold_accuracies = []

    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):
        print(f"Fold {fold + 1}/{kfold.n_splits}")

        # Veriyi bÃ¶l
        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        # Dataset ve DataLoader oluÅŸtur
        train_dataset = WakewordDataset(X_train, y_train)
        val_dataset = WakewordDataset(X_val, y_val)

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

        # Model eÄŸitimi
        model = create_model()
        trainer = WakewordTrainer(model, device)
        accuracy = trainer.train(train_loader, val_loader, epochs=50)

        fold_accuracies.append(accuracy)
        print(f"Fold {fold + 1} Accuracy: {accuracy:.2f}%")

    mean_accuracy = np.mean(fold_accuracies)
    std_accuracy = np.std(fold_accuracies)

    print(f"Mean Accuracy: {mean_accuracy:.2f}% Â± {std_accuracy:.2f}%")

    return mean_accuracy, std_accuracy
```

### 8.3. Deployment Best Practices

#### 8.3.1. Model Export ve Optimizasyon
```python
def optimize_model_for_deployment(model):
    """
    Deployment iÃ§in model optimizasyonu
    """
    # 1. Evaluation moduna al
    model.eval()

    # 2. Gradient calculation'i kapat
    for param in model.parameters():
        param.requires_grad = False

    # 3. TorchScript'e dÃ¶nÃ¼ÅŸtÃ¼r
    scripted_model = torch.jit.script(model)

    # 4. ONNX'e dÃ¶nÃ¼ÅŸtÃ¼r (isteÄŸe baÄŸlÄ±)
    dummy_input = torch.randn(1, 1, 80, 31)
    torch.onnx.export(model, dummy_input, "wakeword_model.onnx")

    # 5. Model bilgilerini kaydet
    model_info = {
        'input_shape': (1, 1, 80, 31),
        'output_shape': (1, 2),
        'sample_rate': 16000,
        'duration': 1.7,
        'n_mels': 80,
        'threshold': 0.8
    }

    with open('model_info.json', 'w') as f:
        json.dump(model_info, f, indent=2)

    return scripted_model
```

#### 8.3.2. Production Monitoring
```python
def monitor_model_performance():
    """
    Production'da model performansÄ±nÄ± izleme
    """
    metrics = {
        'accuracy': [],
        'latency': [],
        'memory_usage': [],
        'false_positives': [],
        'false_negatives': []
    }

    def log_prediction(prediction, ground_truth, latency):
        metrics['accuracy'].append(prediction == ground_truth)
        metrics['latency'].append(latency)

        if prediction == 1 and ground_truth == 0:
            metrics['false_positives'].append(1)
        if prediction == 0 and ground_truth == 1:
            metrics['false_negatives'].append(1)

        # Memory monitoring
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1e9
            metrics['memory_usage'].append(memory_used)

    def generate_report():
        report = {
            'average_accuracy': np.mean(metrics['accuracy']),
            'average_latency': np.mean(metrics['latency']),
            'average_memory': np.mean(metrics['memory_usage']),
            'false_positive_rate': np.mean(metrics['false_positives']),
            'false_negative_rate': np.mean(metrics['false_negatives'])
        }
        return report

    return log_prediction, generate_report
```

---

## ğŸ¯ SONUÃ‡ VE Ã–NERÄ°LER

### 9.1. BaÅŸarÄ±lÄ± Bir Wakeword Modeli Ä°Ã§in Gereksinimler

#### Minimum Gereksinimler:
```
ğŸ“Š Veri:
â€¢ 100+ temiz wakeword kaydÄ±
â€¢ 450+ phonetically benzer negative
â€¢ 875+ random negative ses
â€¢ 66+ saat arka plan gÃ¼rÃ¼ltÃ¼sÃ¼

âš™ï¸ Model:
â€¢ CNN+LSTM mimarisi
â€¢ 882K+ parametre
â€¢ %85+ validation accuracy
â€¢ %80+ test accuracy

ğŸ”§ KonfigÃ¼rasyon:
â€¢ Learning rate: 0.0001
â€¢ Batch size: 32
â€¢ Dropout: 0.6
â€¢ Augmentation: %85
```

#### Ä°deal Gereksinimler:
```
ğŸ“Š Veri:
â€¢ 1000+ temiz wakeword kaydÄ±
â€¢ 4500+ phonetically benzer negative
â€¢ 8750+ random negative ses
â€¢ 100+ saat arka plan gÃ¼rÃ¼ltÃ¼sÃ¼

âš™ï¸ Model:
â€¢ CNN+LSTM mimarisi
â€¢ 882K+ parametre
â€¢ %90+ validation accuracy
â€¢ %85+ test accuracy

ğŸ”§ KonfigÃ¼rasyon:
â€¢ Learning rate: 0.0001 (with scheduling)
â€¢ Batch size: 32 (with gradient accumulation)
â€¢ Dropout: 0.6
â€¢ Augmentation: %85
â€¢ Cross-validation: 5-fold
```

### 9.2. Ã–nerilen Workflow

#### Phase 1: Veri HazÄ±rlama (1-2 Hafta)
```
1. Wakeword kayÄ±tlarÄ±nÄ± topla (100+ sample)
2. Negative sample'leri topla (phonetically benzer)
3. Arka plan gÃ¼rÃ¼ltÃ¼lerini topla (66+ saat)
4. Veri kalitesini kontrol et
5. Veri setini dene
```

#### Phase 2: Model GeliÅŸtirme (2-3 Hafta)
```
1. Model mimarisini kur
2. Hyperparameter tuning yap
3. Cross-validation ile test et
4. Overfitting kontrolÃ¼
5. Model optimizasyonu
```

#### Phase 3: Training ve Evaluation (1-2 Hafta)
```
1. Full training yap
2. Model performansÄ±nÄ± deÄŸerlendir
3. Test seti ile validate et
4. Deployment iÃ§in optimize et
5. Monitoring kurulumu yap
```

### 9.3. SÃ¼rekli Ä°yileÅŸtirme Stratejileri

#### A/B Testing:
```
ğŸ§ª Test Edilecek Ã–zellikler:
â€¢ FarklÄ± augmentation teknikleri
â€¢ FarklÄ± model mimarileri
â€¢ FarklÄ± hyperparameter'lar
â€¢ FarklÄ± veri setleri
```

#### Monitoring ve Alerting:
```
ğŸ“ˆ Ä°zlenecek Metrikler:
â€¢ Accuracy drift
â€¢ Latency deÄŸiÅŸimleri
â€¢ Memory usage
â€¢ False positive/negative oranlarÄ±
â€¢ KullanÄ±cÄ± feedback'i
```

---

## ğŸ“š EK BÄ°LGÄ°LER

### A.1. Teknik Terimler SÃ¶zlÃ¼ÄŸÃ¼

#### Ses Ä°ÅŸleme Terimleri:
```
ğŸµ Sample Rate: Sinyalin saniyedeki Ã¶rnek sayÄ±sÄ±
ğŸ¼ Mel Scale: Ä°nsan iÅŸitmesine gÃ¶re Ã¶lÃ§eklenmiÅŸ frekans skalasÄ±
ğŸ“Š Spectrogram: Zaman-frekans domeninde ses gÃ¶sterimi
ğŸ”Š SNR (Signal-to-Noise Ratio): Sinyal/gÃ¼rÃ¼ltÃ¼ oranÄ±
ğŸ¤ FFT (Fast Fourier Transform): Frekans domenine dÃ¶nÃ¼ÅŸÃ¼m
```

#### Derin Ã–ÄŸrenme Terimleri:
```
ğŸ§  CNN (Convolutional Neural Network): EvriÅŸimli sinir aÄŸÄ±
ğŸ”„ LSTM (Long Short-Term Memory): Uzun kÄ±sa sÃ¼reli hafÄ±za
ğŸ­ Dropout: Regularization tekniÄŸi
ğŸ“ˆ Backpropagation: Geri yayÄ±lÄ±m algoritmasÄ±
âš¡ Gradient Descent: Gradient iniÅŸ optimizasyonu
```

### A.2. YararlÄ± KÃ¼tÃ¼phaneler ve AraÃ§lar

#### Ses Ä°ÅŸleme:
```
ğŸµ librosa: Ses analizi ve feature extraction
ğŸ”Š soundfile: Ses dosyasÄ± okuma/yazma
ğŸ¼ pydub: Ses manipÃ¼lasyonu
ğŸ“Š matplotlib: Ses gÃ¶rselleÅŸtirme
```

#### Derin Ã–ÄŸrenme:
```
ğŸ§  PyTorch: Derin Ã¶ÄŸrenme framework'Ã¼
âš¡ CUDA: GPU hesaplama
ğŸ“ˆ scikit-learn: Machine learning araÃ§larÄ±
ğŸ¯ TensorFlow: Alternatif framework
```

#### Development:
```
ğŸ Python: Ana programlama dili
ğŸ”§ NumPy: SayÄ±sal hesaplama
ğŸ“Š Pandas: Veri iÅŸleme
ğŸ¨ Gradio: Web arayÃ¼zÃ¼
```

### A.3. Referanslar ve Kaynaklar

#### Akademik Makaleler:
```
1. "Wake Word Detection using Deep Neural Networks" - Google Research
2. "End-to-End Wake Word Detection" - Amazon Alexa
3. "Small-Footprint Keyword Spotting" - Apple Siri
4. "Convolutional Neural Networks for Audio Classification" - Stanford
5. "LSTM Networks for Speech Recognition" - University of Toronto
```

#### Open Source Projeler:
```
1. Porcupine - Picovoice (Open source wake word engine)
2. Snowboy - KITT.AI (Wake word detection toolkit)
3. Mycroft - Open source voice assistant
4. Coqui TTS - Text-to-speech synthesis
5. Mozilla DeepSpeech - Speech-to-text engine
```

#### Online Kaynaklar:
```
1. PyTorch Audio Documentation
2. LibROSA Documentation
3. TensorFlow Audio Tutorials
4. Kaggle Audio Competition Kernels
5. GitHub Speech Recognition Projects
```

---

## ğŸ¯ SON

Bu kapsamlÄ± rehber, wakeword detection iÃ§in gerekli tÃ¼m teknik detaylarÄ± iÃ§erir. BaÅŸarÄ±lÄ± bir model iÃ§in:

1. **Kaliteli veri toplama** - Temiz, Ã§eÅŸitli, dengeli veri setleri
2. **DoÄŸru model seÃ§imi** - CNN+LSTM mimarisi
3. **Optimum konfigÃ¼rasyon** - Hyperparameter tuning
4. **Sistem yaklaÅŸÄ±mÄ±** - Cross-validation, monitoring, A/B testing

UnutmayÄ±n, wakeword detection bir sanat bilimidir. Veri kalitesi en Ã¶nemli faktÃ¶rdÃ¼r. Ä°yi eÄŸlenceler! ğŸ‰

---

*Bu rehber sÃ¼rekli gÃ¼ncellenmektedir. Yeni Ã¶zellikler ve optimizasyonlar iÃ§in dÃ¼zenli kontrol edin.*