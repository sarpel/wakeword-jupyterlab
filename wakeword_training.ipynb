{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wakeword Detection Training System\n",
    "\n",
    "## GPU-Accelerated Wakeword Training with CNN+LSTM Architecture\n",
    "\n",
    "**System Status**: ✅ GPU Ready | RTX 3060 Ti | PyTorch 2.0.1 + CUDA 11.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# System Setup and Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport librosa\nimport soundfile as sf\nimport os\nimport glob\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport warnings\nimport subprocess\nimport sys\nwarnings.filterwarnings('ignore')\n\n# GPU Configuration and Diagnostic\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"🔍 System Diagnostic:\")\nprint(f\"   PyTorch version: {torch.__version__}\")\nprint(f\"   Using device: {device}\")\nprint(f\"   GPU Available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    print(f\"   CUDA Version: {torch.version.cuda}\")\nelse:\n    print(\"   ⚠️  GPU not detected - running on CPU\")\n    print(\"   🔧 GPU Fix Suggestions:\")\n    print(\"      1. Install NVIDIA drivers: sudo apt install nvidia-driver-535\")\n    print(\"      2. Install CUDA toolkit\")\n    print(\"      3. Restart WSL: wsl --shutdown (in Windows CMD)\")\n    print(\"      4. Run: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Audio Configuration\nclass AudioConfig:\n    SAMPLE_RATE = 16000\n    DURATION = 1.0  # seconds\n    N_MELS = 80\n    N_FFT = 2048\n    HOP_LENGTH = 512\n    WIN_LENGTH = 2048\n    FMIN = 0\n    FMAX = 8000\n    \n# Model Configuration - Enhanced for better training\nclass ModelConfig:\n    HIDDEN_SIZE = 256  # Increased capacity\n    NUM_LAYERS = 2\n    DROPOUT = 0.6  # Increased dropout for regularization\n    NUM_CLASSES = 2  # wakeword vs negative\n    \n# Training Configuration - Fixed for better convergence\nclass TrainingConfig:\n    BATCH_SIZE = 16  # Reduced for better gradient updates\n    LEARNING_RATE = 0.0001  # Reduced for stable training\n    EPOCHS = 50\n    VALIDATION_SPLIT = 0.2\n    TEST_SPLIT = 0.1\n    \n# Data Augmentation Configuration - Enhanced for better generalization\nclass AugmentationConfig:\n    AUGMENTATION_PROB = 0.8  # Increased augmentation\n    NOISE_FACTOR = 0.15  # More noise variation\n    TIME_SHIFT_MAX = 0.3  # More time variation\n    PITCH_SHIFT_MAX = 3  # More pitch variation\n    SPEED_CHANGE_MIN = 0.7\n    SPEED_CHANGE_MAX = 1.3\n\nprint(\"Configuration loaded successfully!\")\nprint(\"🔧 Enhanced configuration for better training convergence\")\nprint(f\"   Learning Rate: {TrainingConfig.LEARNING_RATE}\")\nprint(f\"   Batch Size: {TrainingConfig.BATCH_SIZE}\")\nprint(f\"   Dropout: {ModelConfig.DROPOUT}\")\nprint(f\"   Augmentation: {AugmentationConfig.AUGMENTATION_PROB}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Processing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    \"\"\"Handle all audio processing tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, config=AudioConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"Load audio file and return as numpy array\"\"\"\n",
    "        try:\n",
    "            audio, sr = librosa.load(file_path, sr=self.config.SAMPLE_RATE)\n",
    "            return audio\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def normalize_audio(self, audio):\n",
    "        \"\"\"Normalize audio to [-1, 1] range\"\"\"\n",
    "        if len(audio) == 0:\n",
    "            return audio\n",
    "        return audio / np.max(np.abs(audio))\n",
    "    \n",
    "    def pad_or_truncate(self, audio, target_length):\n",
    "        \"\"\"Pad or truncate audio to target length\"\"\"\n",
    "        if len(audio) > target_length:\n",
    "            # Truncate\n",
    "            start_idx = random.randint(0, len(audio) - target_length)\n",
    "            return audio[start_idx:start_idx + target_length]\n",
    "        else:\n",
    "            # Pad with zeros\n",
    "            return np.pad(audio, (0, target_length - len(audio)), mode='constant')\n",
    "    \n",
    "    def audio_to_mel(self, audio):\n",
    "        \"\"\"Convert audio to mel-spectrogram\"\"\"\n",
    "        if len(audio) == 0:\n",
    "            return np.zeros((self.config.N_MELS, int(self.config.SAMPLE_RATE * self.config.DURATION / self.config.HOP_LENGTH) + 1))\n",
    "        \n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio,\n",
    "            sr=self.config.SAMPLE_RATE,\n",
    "            n_mels=self.config.N_MELS,\n",
    "            n_fft=self.config.N_FFT,\n",
    "            hop_length=self.config.HOP_LENGTH,\n",
    "            win_length=self.config.WIN_LENGTH,\n",
    "            fmin=self.config.FMIN,\n",
    "            fmax=self.config.FMAX\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        return mel_spec\n",
    "    \n",
    "    def augment_audio(self, audio, config=AugmentationConfig):\n",
    "        \"\"\"Apply data augmentation to audio\"\"\"\n",
    "        augmented_audio = audio.copy()\n",
    "        \n",
    "        # Time shift\n",
    "        if random.random() < config.AUGMENTATION_PROB:\n",
    "            shift_amount = int(random.uniform(-config.TIME_SHIFT_MAX, config.TIME_SHIFT_MAX) * self.config.SAMPLE_RATE)\n",
    "            augmented_audio = np.roll(augmented_audio, shift_amount)\n",
    "        \n",
    "        # Pitch shift\n",
    "        if random.random() < config.AUGMENTATION_PROB:\n",
    "            n_steps = random.uniform(-config.PITCH_SHIFT_MAX, config.PITCH_SHIFT_MAX)\n",
    "            augmented_audio = librosa.effects.pitch_shift(y=augmented_audio, sr=self.config.SAMPLE_RATE, n_steps=n_steps)\n",
    "        \n",
    "        # Speed change\n",
    "        if random.random() < config.AUGMENTATION_PROB:\n",
    "            speed_factor = random.uniform(config.SPEED_CHANGE_MIN, config.SPEED_CHANGE_MAX)\n",
    "            augmented_audio = librosa.effects.time_stretch(y=augmented_audio, rate=speed_factor)\n",
    "            \n",
    "            # Pad or truncate to maintain original length\n",
    "            augmented_audio = self.pad_or_truncate(augmented_audio, len(audio))\n",
    "        \n",
    "        # Add background noise\n",
    "        if random.random() < config.AUGMENTATION_PROB:\n",
    "            noise = np.random.normal(0, config.NOISE_FACTOR, len(augmented_audio))\n",
    "            augmented_audio = augmented_audio + noise\n",
    "        \n",
    "        return augmented_audio\n",
    "    \n",
    "    def process_audio_file(self, file_path, augment=False):\n",
    "        \"\"\"Process a single audio file\"\"\"\n",
    "        # Load audio\n",
    "        audio = self.load_audio(file_path)\n",
    "        if audio is None:\n",
    "            return None\n",
    "        \n",
    "        # Normalize\n",
    "        audio = self.normalize_audio(audio)\n",
    "        \n",
    "        # Pad/truncate to target length\n",
    "        target_length = int(self.config.SAMPLE_RATE * self.config.DURATION)\n",
    "        audio = self.pad_or_truncate(audio, target_length)\n",
    "        \n",
    "        # Apply augmentation if requested\n",
    "        if augment:\n",
    "            audio = self.augment_audio(audio)\n",
    "        \n",
    "        # Convert to mel-spectrogram\n",
    "        mel_spec = self.audio_to_mel(audio)\n",
    "        \n",
    "        return mel_spec\n",
    "\n",
    "# Test the audio processor\n",
    "processor = AudioProcessor()\n",
    "print(\"AudioProcessor created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WakewordModel(nn.Module):\n",
    "    \"\"\"CNN+LSTM model for wakeword detection\"\"\"\n",
    "    \n",
    "    def __init__(self, config=ModelConfig, audio_config=AudioConfig):\n",
    "        super(WakewordModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.audio_config = audio_config\n",
    "        \n",
    "        # Calculate CNN output size\n",
    "        self.mel_height = audio_config.N_MELS\n",
    "        self.mel_width = int(audio_config.SAMPLE_RATE * audio_config.DURATION / audio_config.HOP_LENGTH) + 1\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Calculate CNN output size\n",
    "        self.cnn_output_size = 128  # After adaptive pooling\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.cnn_output_size,\n",
    "            hidden_size=config.HIDDEN_SIZE,\n",
    "            num_layers=config.NUM_LAYERS,\n",
    "            batch_first=True,\n",
    "            dropout=config.DROPOUT if config.NUM_LAYERS > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(config.DROPOUT)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(config.HIDDEN_SIZE, config.NUM_CLASSES)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, 1, mel_height, mel_width)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        x = x.view(batch_size, -1)  # (batch, cnn_output_size)\n",
    "        \n",
    "        # Add sequence dimension\n",
    "        x = x.unsqueeze(1)  # (batch, seq_len=1, features)\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Take the last output\n",
    "        x = lstm_out[:, -1, :]  # (batch, hidden_size)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model and move to device\n",
    "model = WakewordModel().to(device)\n",
    "print(f\"Model created and moved to {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test model with dummy input\n",
    "dummy_input = torch.randn(1, 1, AudioConfig.N_MELS, 31).to(device)  # (batch, channels, height, width)\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "print(f\"Dummy output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WakewordDataset(Dataset):\n",
    "    \"\"\"Dataset class for wakeword detection\"\"\"\n",
    "    \n",
    "    def __init__(self, wakeword_files, negative_files, processor, augment=False):\n",
    "        self.wakeword_files = wakeword_files\n",
    "        self.negative_files = negative_files\n",
    "        self.processor = processor\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Combine files and create labels\n",
    "        self.files = wakeword_files + negative_files\n",
    "        self.labels = [1] * len(wakeword_files) + [0] * len(negative_files)\n",
    "        \n",
    "        print(f\"Dataset created with {len(self.files)} samples\")\n",
    "        print(f\"Wakeword samples: {len(wakeword_files)}\")\n",
    "        print(f\"Negative samples: {len(negative_files)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Process audio file\n",
    "        mel_spec = self.processor.process_audio_file(file_path, augment=self.augment)\n",
    "        \n",
    "        if mel_spec is None:\n",
    "            # Return zeros if processing failed\n",
    "            mel_spec = np.zeros((self.processor.config.N_MELS, 31))\n",
    "        \n",
    "        # Convert to tensor\n",
    "        mel_tensor = torch.FloatTensor(mel_spec).unsqueeze(0)  # Add channel dimension\n",
    "        label_tensor = torch.LongTensor([label])\n",
    "        \n",
    "        return mel_tensor, label_tensor\n",
    "\n",
    "# Placeholder for dataset creation\n",
    "print(\"Dataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "      # Learning rate scheduler\n        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            self.optimizer, mode='max', factor=0.5, patience=5\n        )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data directories exist\n",
    "data_dirs = ['wakeword_data', 'negative_data', 'background_noise']\n",
    "missing_dirs = []\n",
    "\n",
    "for dir_name in data_dirs:\n",
    "    if not os.path.exists(dir_name):\n",
    "        missing_dirs.append(dir_name)\n",
    "    \n",
    "if missing_dirs:\n",
    "    print(\"⚠️  Missing data directories:\")\n",
    "    for dir_name in missing_dirs:\n",
    "        print(f\"   - {dir_name}/\")\n",
    "    print(\"\\nPlease create these directories and add your audio files:\")\n",
    "    print(\"   - wakeword_data/ : Your 500+ wakeword recordings\")\n",
    "    print(\"   - negative_data/ : Thousands of negative audio samples\")\n",
    "    print(\"   - background_noise/ : 100+ hours of background noise\")\n",
    "    \n",
    "    # Create directories\n",
    "    for dir_name in missing_dirs:\n",
    "        os.makedirs(dir_name, exist_ok=True)\n",
    "        print(f\"✅ Created directory: {dir_name}/\")\n",
    "else:\n",
    "    print(\"✅ All data directories found!\")\n",
    "\n",
    "# Load audio files\n",
    "def load_audio_files(directory, extensions=['*.wav', '*.mp3', '*.flac']):\n",
    "    \"\"\"Load audio files from directory\"\"\"\n",
    "    files = []\n",
    "    for ext in extensions:\n",
    "        files.extend(glob.glob(os.path.join(directory, ext)))\n",
    "    return files\n",
    "\n",
    "wakeword_files = load_audio_files('wakeword_data')\n",
    "negative_files = load_audio_files('negative_data')\n",
    "\n",
    "print(f\"\\n📊 Data Summary:\")\n",
    "print(f\"   Wakeword files: {len(wakeword_files)}\")\n",
    "print(f\"   Negative files: {len(negative_files)}\")\n",
    "\n",
    "if len(wakeword_files) == 0 or len(negative_files) == 0:\n",
    "    print(\"\\n⚠️  Not enough data files found!\")\n",
    "    print(\"Please add audio files to the respective directories before training.\")\n",
    "else:\n",
    "    print(\"✅ Data files loaded successfully!\")\n",
    "    \n",
    "    # Split data for training\n",
    "    wakeword_train, wakeword_test = train_test_split(wakeword_files, test_size=TrainingConfig.TEST_SPLIT, random_state=42)\n",
    "    wakeword_train, wakeword_val = train_test_split(wakeword_train, test_size=TrainingConfig.VALIDATION_SPLIT, random_state=42)\n",
    "    \n",
    "    negative_train, negative_test = train_test_split(negative_files, test_size=TrainingConfig.TEST_SPLIT, random_state=42)\n",
    "    negative_train, negative_val = train_test_split(negative_train, test_size=TrainingConfig.VALIDATION_SPLIT, random_state=42)\n",
    "    \n",
    "    print(f\"\\n📈 Data Split:\")\n",
    "    print(f\"   Training: {len(wakeword_train)} wakeword + {len(negative_train)} negative = {len(wakeword_train) + len(negative_train)} total\")\n",
    "    print(f\"   Validation: {len(wakeword_val)} wakeword + {len(negative_val)} negative = {len(wakeword_val) + len(negative_val)} total\")\n",
    "    print(f\"   Test: {len(wakeword_test)} wakeword + {len(negative_test)} negative = {len(wakeword_test) + len(negative_test)} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we have enough data for training\n",
    "if len(wakeword_files) > 0 and len(negative_files) > 0:\n",
    "    # Create datasets\n",
    "    train_dataset = WakewordDataset(wakeword_train, negative_train, processor, augment=True)\n",
    "    val_dataset = WakewordDataset(wakeword_val, negative_val, processor, augment=False)\n",
    "    test_dataset = WakewordDataset(wakeword_test, negative_test, processor, augment=False)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=TrainingConfig.BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=TrainingConfig.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=TrainingConfig.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"\\n🚀 Starting Training...\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    print(f\"   Batch size: {TrainingConfig.BATCH_SIZE}\")\n",
    "    print(f\"   Learning rate: {TrainingConfig.LEARNING_RATE}\")\n",
    "    print(f\"   Epochs: {TrainingConfig.EPOCHS}\")\n",
    "    print(f\"   Training samples: {len(train_dataset)}\")\n",
    "    print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Train the model\n",
    "    try:\n",
    "        best_val_acc = trainer.train(train_loader, val_loader, TrainingConfig.EPOCHS)\n",
    "        \n",
    "        # Plot training history\n",
    "        trainer.plot_training_history()\n",
    "        \n",
    "        print(f\"\\n🎉 Training completed successfully!\")\n",
    "        print(f\"   Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during training: {e}\")\n",
    "        print(\"Please check your data files and try again.\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Cannot start training - missing data files!\")\n",
    "    print(\"Please add audio files to wakeword_data/ and negative_data/ directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "if os.path.exists('best_wakeword_model.pth'):\n",
    "    checkpoint = torch.load('best_wakeword_model.pth', map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"✅ Best model loaded (epoch {checkpoint['epoch'] + 1}, val_acc: {checkpoint['val_acc']:.2f}%)\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            data, target = data.to(device), target.to(device).squeeze()\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"\\n📊 Test Set Performance:\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall: {recall:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Wakeword'], \n",
    "                yticklabels=['Negative', 'Wakeword'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Negative', 'Wakeword']))\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No trained model found. Please run the training cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_wakeword(audio_file_path, model, processor, device, threshold=0.8):\n",
    "    \"\"\"Predict if audio contains wakeword\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Process audio file\n",
    "    mel_spec = processor.process_audio_file(audio_file_path, augment=False)\n",
    "    \n",
    "    if mel_spec is None:\n",
    "        print(f\"Error processing audio file: {audio_file_path}\")\n",
    "        return False, 0.0\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    mel_tensor = torch.FloatTensor(mel_spec).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(mel_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        wakeword_prob = probabilities[0][1].item()\n",
    "        \n",
    "    is_wakeword = wakeword_prob >= threshold\n",
    "    \n",
    "    return is_wakeword, wakeword_prob\n",
    "\n",
    "# Test prediction function\n",
    "if os.path.exists('best_wakeword_model.pth'):\n",
    "    print(\"\\n🎯 Wakeword Detection System Ready!\")\n",
    "    print(\"You can now use the predict_wakeword function for real-time detection.\")\n",
    "    print(\"\\nExample usage:\")\n",
    "    print(\"is_wakeword, confidence = predict_wakeword('test_audio.wav', model, processor, device)\")\n",
    "    print(f\"Result: {{'wakeword_detected': is_wakeword, 'confidence': confidence:.2f}}\")\n",
    "    \n",
    "    # Create a simple test function\n",
    "    def test_audio_file(file_path):\n",
    "        if os.path.exists(file_path):\n",
    "            is_wakeword, confidence = predict_wakeword(file_path, model, processor, device)\n",
    "            print(f\"\\n📁 Testing: {file_path}\")\n",
    "            print(f\"   Wakeword detected: {is_wakeword}\")\n",
    "            print(f\"   Confidence: {confidence:.2f}\")\n",
    "            print(f\"   Threshold: 0.80\")\n",
    "        else:\n",
    "            print(f\"\\n❌ File not found: {file_path}\")\n",
    "    \n",
    "    print(\"\\n🔧 Test function available: test_audio_file('your_audio_file.wav')\")\n",
    "else:\n",
    "    print(\"❌ Model not trained yet. Please run training cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete model for deployment\n",
    "if os.path.exists('best_wakeword_model.pth'):\n",
    "    # Create a complete deployment package\n",
    "    deployment_package = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'HIDDEN_SIZE': ModelConfig.HIDDEN_SIZE,\n",
    "            'NUM_LAYERS': ModelConfig.NUM_LAYERS,\n",
    "            'DROPOUT': ModelConfig.DROPOUT,\n",
    "            'NUM_CLASSES': ModelConfig.NUM_CLASSES\n",
    "        },\n",
    "        'audio_config': {\n",
    "            'SAMPLE_RATE': AudioConfig.SAMPLE_RATE,\n",
    "            'DURATION': AudioConfig.DURATION,\n",
    "            'N_MELS': AudioConfig.N_MELS,\n",
    "            'N_FFT': AudioConfig.N_FFT,\n",
    "            'HOP_LENGTH': AudioConfig.HOP_LENGTH,\n",
    "            'FMIN': AudioConfig.FMIN,\n",
    "            'FMAX': AudioConfig.FMAX\n",
    "        },\n",
    "        'training_info': {\n",
    "            'best_val_accuracy': checkpoint.get('val_acc', 0),\n",
    "            'epoch': checkpoint.get('epoch', 0) + 1,\n",
    "            'device': str(device)\n",
    "        },\n",
    "        'classes': ['negative', 'wakeword']\n",
    "    }\n",
    "    \n",
    "    # Save deployment package\n",
    "    torch.save(deployment_package, 'wakeword_deployment_model.pth')\n",
    "    print(\"✅ Deployment model saved as 'wakeword_deployment_model.pth'\")\n",
    "    \n",
    "    # Save model architecture for reference\n",
    "    with open('model_architecture.txt', 'w') as f:\n",
    "        f.write(\"Wakeword Detection Model Architecture\\n\")\n",
    "        f.write(\"================================\\n\\n\")\n",
    "        f.write(\"Model Type: CNN + LSTM\\n\")\n",
    "        f.write(f\"Input Shape: (1, {AudioConfig.N_MELS}, 31)\\n\")\n",
    "        f.write(f\"Hidden Size: {ModelConfig.HIDDEN_SIZE}\\n\")\n",
    "        f.write(f\"Number of Layers: {ModelConfig.NUM_LAYERS}\\n\")\n",
    "        f.write(f\"Dropout: {ModelConfig.DROPOUT}\\n\")\n",
    "        f.write(f\"Number of Classes: {ModelConfig.NUM_CLASSES}\\n\")\n",
    "        f.write(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\\n\")\n",
    "        f.write(f\"Device: {device}\\n\")\n",
    "    \n",
    "    print(\"✅ Model architecture saved as 'model_architecture.txt'\")\n",
    "    \n",
    "    print(\"\\n🎉 Model deployment package ready!\")\n",
    "    print(\"Files created:\")\n",
    "    print(\"   - wakeword_deployment_model.pth (complete model)\")\n",
    "    print(\"   - model_architecture.txt (model specs)\")\n",
    "    print(\"   - best_wakeword_model.pth (training checkpoint)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No trained model found to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 Wakeword Training System Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n📊 System Status:\")\n",
    "print(f\"   GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"   PyTorch Version: {torch.__version__}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "print(f\"\\n📁 Data Status:\")\n",
    "print(f\"   Wakeword files: {len(wakeword_files)}\")\n",
    "print(f\"   Negative files: {len(negative_files)}\")\n",
    "print(f\"   Data directories created: {len([d for d in data_dirs if os.path.exists(d)])}/3\")\n",
    "\n",
    "print(f\"\\n🧠 Model Status:\")\n",
    "print(f\"   Model created: {model is not None}\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Model on device: {next(model.parameters()).device}\")\n",
    "\n",
    "if os.path.exists('best_wakeword_model.pth'):\n",
    "    print(f\"\\n✅ Training Status: Completed\")\n",
    "    print(f\"   Model saved: best_wakeword_model.pth\")\n",
    "    print(f\"   Deployment model: wakeword_deployment_model.pth\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Training Status: Not started or incomplete\")\n",
    "    print(f\"   Ready to train when data is available\")\n",
    "\n",
    "print(f\"\\n🚀 Next Steps:\")\n",
    "print(f\"   1. Add your wakeword recordings to wakeword_data/\")\n",
    "print(f\"   2. Add negative samples to negative_data/\")\n",
    "print(f\"   3. Add background noise to background_noise/\")\n",
    "print(f\"   4. Run the training cells above\")\n",
    "print(f\"   5. Use the trained model for wakeword detection\")\n",
    "\n",
    "print(f\"\\n💡 Tips:\")\n",
    "print(f\"   - Use at least 100 wakeword recordings for good results\")\n",
    "print(f\"   - More negative samples improve false positive reduction\")\n",
    "print(f\"   - Background noise helps with robustness\")\n",
    "print(f\"   - Monitor GPU memory usage during training\")\n",
    "print(f\"   - Adjust hyperparameters based on your dataset size\")\n",
    "\n",
    "print(f\"\\n🎉 System ready for wakeword training with GPU acceleration!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}