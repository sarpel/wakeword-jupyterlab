{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wakeword Generation and Training System\n",
    "\n",
    "This notebook provides a complete pipeline for:\n",
    "1. Setting up the environment\n",
    "2. Audio data preprocessing and augmentation\n",
    "3. Wakeword model training\n",
    "4. Model evaluation and testing\n",
    "\n",
    "**Environment:**\n",
    "- Ubuntu 24.04.3 LTS (WSL2)\n",
    "- RTX 3060 Ti 8GB GPU\n",
    "- PyTorch 2.8.0 (CPU version for now)\n",
    "- Librosa 0.11.0 for audio processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if we have the required libraries\n",
    "try:\n",
    "    import torch\n",
    "    import torchaudio\n",
    "    import librosa\n",
    "    import soundfile as sf\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    print(\"✅ All required libraries imported successfully\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"Librosa version: {librosa.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Missing library: {e}\")\n",
    "    print(\"Please install missing packages with: pip install [package_name]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Audio parameters\n",
    "    SAMPLE_RATE = 16000\n",
    "    DURATION = 1.0  # 1 second audio clips\n",
    "    N_MELS = 64\n",
    "    N_FFT = 2048\n",
    "    HOP_LENGTH = 512\n",
    "    \n",
    "    # Data paths (modify these to match your setup)\n",
    "    WAKEWORD_DIR = \"./wakeword_data\"\n",
    "    NEGATIVE_DIR = \"./negative_data\" \n",
    "    BACKGROUND_DIR = \"./background_noise\"\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # Model parameters\n",
    "    HIDDEN_SIZE = 128\n",
    "    NUM_LAYERS = 2\n",
    "    DROPOUT = 0.3\n",
    "    \n",
    "    # Augmentation parameters\n",
    "    NOISE_FACTOR = 0.1\n",
    "    TIME_SHIFT = 0.1\n",
    "    PITCH_SHIFT = 2\n",
    "    SPEED_CHANGE = 0.2\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for dir_path in [config.WAKEWORD_DIR, config.NEGATIVE_DIR, config.BACKGROUND_DIR]:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"📁 Created/verified directory: {dir_path}\")\n",
    "\n",
    "print(\"\\n🎯 Configuration loaded successfully!\")\n",
    "print(f\"Sample rate: {config.SAMPLE_RATE} Hz\")\n",
    "print(f\"Duration: {config.DURATION} seconds\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Audio Data Processing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    \"\"\"Utility class for audio processing operations\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        \n",
    "    def load_audio(self, file_path, sr=None):\n",
    "        \"\"\"Load audio file and resample if needed\"\"\"\n",
    "        try:\n",
    "            audio, sr = librosa.load(file_path, sr=sr or self.config.SAMPLE_RATE)\n",
    "            return audio, sr\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def normalize_audio(self, audio):\n",
    "        \"\"\"Normalize audio to [-1, 1] range\"\"\"\n",
    "        if audio is None or len(audio) == 0:\n",
    "            return None\n",
    "        return audio / np.max(np.abs(audio))\n",
    "    \n",
    "    def pad_or_truncate(self, audio, target_length):\n",
    "        \"\"\"Pad or truncate audio to target length\"\"\"\n",
    "        if audio is None:\n",
    "            return np.zeros(target_length)\n",
    "            \n",
    "        current_length = len(audio)\n",
    "        \n",
    "        if current_length > target_length:\n",
    "            # Truncate from the beginning\n",
    "            return audio[:target_length]\n",
    "        elif current_length < target_length:\n",
    "            # Pad with zeros\n",
    "            padding = target_length - current_length\n",
    "            return np.pad(audio, (0, padding), mode='constant')\n",
    "        else:\n",
    "            return audio\n",
    "    \n",
    "    def extract_melspectrogram(self, audio):\n",
    "        \"\"\"Extract mel-spectrogram from audio\"\"\"\n",
    "        if audio is None or len(audio) == 0:\n",
    "            return None\n",
    "            \n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio,\n",
    "            sr=self.config.SAMPLE_RATE,\n",
    "            n_mels=self.config.N_MELS,\n",
    "            n_fft=self.config.N_FFT,\n",
    "            hop_length=self.config.HOP_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        return mel_spec_db\n",
    "    \n",
    "    def add_noise(self, audio, noise_factor=0.1):\n",
    "        \"\"\"Add random noise to audio\"\"\"\n",
    "        if audio is None:\n",
    "            return None\n",
    "            \n",
    "        noise = np.random.normal(0, noise_factor, len(audio))\n",
    "        return audio + noise\n",
    "    \n",
    "    def time_shift(self, audio, shift_factor=0.1):\n",
    "        \"\"\"Shift audio in time\"\"\"\n",
    "        if audio is None:\n",
    "            return None\n",
    "            \n",
    "        shift_amount = int(len(audio) * shift_factor)\n",
    "        if shift_amount > 0:\n",
    "            return np.roll(audio, shift_amount)\n",
    "        else:\n",
    "            return audio\n",
    "    \n",
    "    def pitch_shift(self, audio, n_steps=2):\n",
    "        \"\"\"Shift pitch of audio\"\"\"\n",
    "        if audio is None:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            return librosa.effects.pitch_shift(\n",
    "                y=audio, \n",
    "                sr=self.config.SAMPLE_RATE, \n",
    "                n_steps=n_steps\n",
    "            )\n",
    "        except:\n",
    "            return audio\n",
    "    \n",
    "    def change_speed(self, audio, speed_factor=1.2):\n",
    "        \"\"\"Change speed of audio\"\"\"\n",
    "        if audio is None:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            return librosa.effects.time_stretch(y=audio, rate=speed_factor)\n",
    "        except:\n",
    "            return audio\n",
    "\n",
    "# Initialize audio processor\n",
    "audio_processor = AudioProcessor(config)\n",
    "print(\"🎵 Audio processor initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Data loading and augmentation class\"\"\"\n",
    "    \n",
    "    def __init__(self, config, audio_processor):\n",
    "        self.config = config\n",
    "        self.audio_processor = audio_processor\n",
    "        \n",
    "    def load_wakeword_data(self):\n",
    "        \"\"\"Load wakeword audio files\"\"\"\n",
    "        wakeword_files = []\n",
    "        \n",
    "        # Walk through wakeword directory\n",
    "        wakeword_path = Path(self.config.WAKEWORD_DIR)\n",
    "        if wakeword_path.exists():\n",
    "            for ext in ['*.wav', '*.mp3', '*.flac']:\n",
    "                wakeword_files.extend(list(wakeword_path.rglob(ext)))\n",
    "        \n",
    "        print(f\"Found {len(wakeword_files)} wakeword files\")\n",
    "        return wakeword_files\n",
    "    \n",
    "    def load_negative_data(self):\n",
    "        \"\"\"Load negative audio files\"\"\"\n",
    "        negative_files = []\n",
    "        \n",
    "        negative_path = Path(self.config.NEGATIVE_DIR)\n",
    "        if negative_path.exists():\n",
    "            for ext in ['*.wav', '*.mp3', '*.flac']:\n",
    "                negative_files.extend(list(negative_path.rglob(ext)))\n",
    "        \n",
    "        print(f\"Found {len(negative_files)} negative files\")\n",
    "        return negative_files\n",
    "    \n",
    "    def load_background_noise(self):\n",
    "        \"\"\"Load background noise files\"\"\"\n",
    "        noise_files = []\n",
    "        \n",
    "        noise_path = Path(self.config.BACKGROUND_DIR)\n",
    "        if noise_path.exists():\n",
    "            for ext in ['*.wav', '*.mp3', '*.flac']:\n",
    "                noise_files.extend(list(noise_path.rglob(ext)))\n",
    "        \n",
    "        print(f\"Found {len(noise_files)} background noise files\")\n",
    "        return noise_files\n",
    "    \n",
    "    def augment_audio(self, audio, background_noises=None):\n",
    "        \"\"\"Apply various augmentations to audio\"\"\"\n",
    "        if audio is None:\n",
    "            return None\n",
    "            \n",
    "        augmented = [audio]  # Original audio\n",
    "        \n",
    "        # Add noise\n",
    "        augmented.append(self.audio_processor.add_noise(audio, self.config.NOISE_FACTOR))\n",
    "        \n",
    "        # Time shift\n",
    "        augmented.append(self.audio_processor.time_shift(audio, self.config.TIME_SHIFT))\n",
    "        \n",
    "        # Pitch shift\n",
    "        augmented.append(self.audio_processor.pitch_shift(audio, self.config.PITCH_SHIFT))\n",
    "        \n",
    "        # Speed change\n",
    "        augmented.append(self.audio_processor.change_speed(audio, 1.0 + self.config.SPEED_CHANGE))\n",
    "        augmented.append(self.audio_processor.change_speed(audio, 1.0 - self.config.SPEED_CHANGE))\n",
    "        \n",
    "        # Add background noise if available\n",
    "        if background_noises and len(background_noises) > 0:\n",
    "            bg_noise = np.random.choice(background_noises)\n",
    "            bg_noise = self.audio_processor.pad_or_truncate(bg_noise, len(audio))\n",
    "            bg_noise = bg_noise * 0.3  # Reduce volume\n",
    "            augmented.append(audio + bg_noise)\n",
    "        \n",
    "        return augmented\n",
    "    \n",
    "    def create_dataset(self, max_samples=None):\n",
    "        \"\"\"Create complete dataset with augmentations\"\"\"\n",
    "        print(\"🔄 Loading data...\")\n",
    "        \n",
    "        # Load all data\n",
    "        wakeword_files = self.load_wakeword_data()\n",
    "        negative_files = self.load_negative_data()\n",
    "        background_files = self.load_background_noise()\n",
    "        \n",
    "        # Load background noises\n",
    "        background_noises = []\n",
    "        for noise_file in background_files[:10]:  # Limit to first 10 files\n",
    "            audio, _ = self.audio_processor.load_audio(noise_file)\n",
    "            if audio is not None:\n",
    "                background_noises.append(audio)\n",
    "        \n",
    "        print(f\"Loaded {len(background_noises)} background noise samples\")\n",
    "        \n",
    "        # Process wakeword data\n",
    "        X_wakeword = []\n",
    "        y_wakeword = []\n",
    "        \n",
    "        for file_path in tqdm(wakeword_files[:max_samples], desc=\"Processing wakeword data\"):\n",
    "            audio, _ = self.audio_processor.load_audio(file_path)\n",
    "            if audio is not None:\n",
    "                # Normalize and process\n",
    "                audio = self.audio_processor.normalize_audio(audio)\n",
    "                audio = self.audio_processor.pad_or_truncate(audio, int(self.config.SAMPLE_RATE * self.config.DURATION))\n",
    "                \n",
    "                # Extract mel-spectrogram\n",
    "                mel_spec = self.audio_processor.extract_melspectrogram(audio)\n",
    "                if mel_spec is not None:\n",
    "                    X_wakeword.append(mel_spec)\n",
    "                    y_wakeword.append(1)  # Wakeword label\n",
    "                    \n",
    "                    # Add augmentations\n",
    "                    augmented_audios = self.augment_audio(audio, background_noises)\n",
    "                    for aug_audio in augmented_audios[1:]:  # Skip original\n",
    "                        aug_mel_spec = self.audio_processor.extract_melspectrogram(aug_audio)\n",
    "                        if aug_mel_spec is not None:\n",
    "                            X_wakeword.append(aug_mel_spec)\n",
    "                            y_wakeword.append(1)\n",
    "        \n",
    "        # Process negative data\n",
    "        X_negative = []\n",
    "        y_negative = []\n",
    "        \n",
    "        for file_path in tqdm(negative_files[:max_samples], desc=\"Processing negative data\"):\n",
    "            audio, _ = self.audio_processor.load_audio(file_path)\n",
    "            if audio is not None:\n",
    "                # Take random 1-second segments\n",
    "                if len(audio) > self.config.SAMPLE_RATE:\n",
    "                    start_idx = np.random.randint(0, len(audio) - self.config.SAMPLE_RATE)\n",
    "                    audio = audio[start_idx:start_idx + self.config.SAMPLE_RATE]\n",
    "                \n",
    "                audio = self.audio_processor.normalize_audio(audio)\n",
    "                audio = self.audio_processor.pad_or_truncate(audio, int(self.config.SAMPLE_RATE * self.config.DURATION))\n",
    "                \n",
    "                # Extract mel-spectrogram\n",
    "                mel_spec = self.audio_processor.extract_melspectrogram(audio)\n",
    "                if mel_spec is not None:\n",
    "                    X_negative.append(mel_spec)\n",
    "                    y_negative.append(0)  # Negative label\n",
    "        \n",
    "        # Combine datasets\n",
    "        X = X_wakeword + X_negative\n",
    "        y = y_wakeword + y_negative\n",
    "        \n",
    "        print(f\"📊 Dataset created:\")\n",
    "        print(f\"  Total samples: {len(X)}\")\n",
    "        print(f\"  Wakeword samples: {len(X_wakeword)}\")\n",
    "        print(f\"  Negative samples: {len(X_negative)}\")\n",
    "        print(f\"  Shape of each sample: {X[0].shape if X else 'None'}\")\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(config, audio_processor)\n",
    "print(\"📂 Data loader initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Wakeword Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WakewordModel(nn.Module):\n",
    "    \"\"\"CNN + LSTM model for wakeword detection\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(WakewordModel, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Calculate input dimensions\n",
    "        self.time_steps = int(config.SAMPLE_RATE * config.DURATION / config.HOP_LENGTH) + 1\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(config.DROPOUT)\n",
    "        \n",
    "        # Calculate CNN output size\n",
    "        cnn_output_size = self._get_cnn_output_size()\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_output_size,\n",
    "            hidden_size=config.HIDDEN_SIZE,\n",
    "            num_layers=config.NUM_LAYERS,\n",
    "            batch_first=True,\n",
    "            dropout=config.DROPOUT if config.NUM_LAYERS > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(config.HIDDEN_SIZE, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def _get_cnn_output_size(self):\n",
    "        \"\"\"Calculate CNN output size\"\"\"\n",
    "        # Input: (batch, 1, n_mels, time_steps)\n",
    "        x = torch.zeros(1, 1, config.N_MELS, self.time_steps)\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        return x.view(x.size(0), -1).size(1) // self.time_steps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add channel dimension if missing\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # CNN layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        x = x.view(batch_size, width, -1)  # (batch, time_steps, features)\n",
    "        \n",
    "        # LSTM layers\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Take the last time step\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.fc(lstm_out)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "# Create model instance\n",
    "model = WakewordModel(config)\n",
    "print(f\"🧠 Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class WakewordDataset(Dataset):\n",
    "    \"\"\"Custom dataset for wakeword training\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Training class for wakeword model\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        self.criterion = nn.BCELoss().to(self.device) # Moved criterion to device\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "        \n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predicted = (output > 0.5).float()\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def validate(self, val_loader):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                predicted = (output > 0.5).float()\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        accuracy = 100. * correct / total\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        print(f\"🚀 Starting training on {self.device}\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "            print('-' * 50)\n",
    "            \n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate(val_loader)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                }, f'wakeword_checkpoint_epoch_{epoch+1}.pth')\n",
    "                print(f'💾 Checkpoint saved for epoch {epoch+1}')\n",
    "        \n",
    "        print('\\n✅ Training completed!')\n",
    "        \n",
    "        # Save final model\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'config': self.config.__dict__,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "        }, 'wakeword_model_final.pth')\n",
    "        print('💾 Final model saved!')\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(self.train_losses, label='Train Loss')\n",
    "        ax1.plot(self.val_losses, label='Val Loss')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax2.plot(self.train_accuracies, label='Train Accuracy')\n",
    "        ax2.plot(self.val_accuracies, label='Val Accuracy')\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"🏋️ Training pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Training Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data():\n",
    "    \"\"\"Create sample data for demonstration\"\"\"\n",
    "    print(\"📝 Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Create sample wakeword data (simulated)\n",
    "    X_sample = []\n",
    "    y_sample = []\n",
    "    \n",
    "    # Generate synthetic mel-spectrograms\n",
    "    for i in range(100):  # 50 wakeword + 50 negative samples\n",
    "        # Random mel-spectrogram (simulated)\n",
    "        mel_spec = np.random.rand(config.N_MELS, 32) * 10  # 32 time steps\n",
    "        \n",
    "        # Add some pattern for wakeword samples\n",
    "        if i < 50:\n",
    "            # Wakeword pattern\n",
    "            mel_spec[10:20, 10:20] += 5  # Add energy in specific region\n",
    "            y_sample.append(1)\n",
    "        else:\n",
    "            # Negative pattern\n",
    "            y_sample.append(0)\n",
    "        \n",
    "        X_sample.append(mel_spec)\n",
    "    \n",
    "    return np.array(X_sample), np.array(y_sample)\n",
    "\n",
    "def main_training_workflow():\n",
    "    \"\"\"Complete training workflow\"\"\"\n",
    "    print(\"🎯 Starting complete wakeword training workflow...\")\n",
    "    \n",
    "    # Step 1: Load or create data\n",
    "    try:\n",
    "        # Try to load real data\n",
    "        X, y = data_loader.create_dataset(max_samples=100)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load real data: {e}\")\n",
    "        print(\"Using synthetic sample data for demonstration...\")\n",
    "        X, y = create_sample_data()\n",
    "    \n",
    "    if len(X) == 0:\n",
    "        print(\"❌ No data available. Please add audio files to the directories.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"📊 Data split:\")\n",
    "    print(f\"  Training samples: {len(X_train)}\")\n",
    "    print(f\"  Validation samples: {len(X_val)}\")\n",
    "    print(f\"  Training wakeword ratio: {sum(y_train)/len(y_train):.2f}\")\n",
    "    print(f\"  Validation wakeword ratio: {sum(y_val)/len(y_val):.2f}\")\n",
    "    \n",
    "    # Step 3: Create datasets and loaders\n",
    "    train_dataset = WakewordDataset(X_train, y_train)\n",
    "    val_dataset = WakewordDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Step 4: Initialize trainer\n",
    "    trainer = Trainer(model, config)\n",
    "    \n",
    "    # Step 5: Train model\n",
    "    try:\n",
    "        trainer.train(train_loader, val_loader, epochs=config.EPOCHS)\n",
    "        \n",
    "        # Step 6: Plot results\n",
    "        trainer.plot_training_history()\n",
    "        \n",
    "        print(\"\\n🎉 Training completed successfully!\")\n",
    "        print(f\"Final training accuracy: {trainer.train_accuracies[-1]:.2f}%\")\n",
    "        print(f\"Final validation accuracy: {trainer.val_accuracies[-1]:.2f}%\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "# Run the complete workflow\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = main_training_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Testing and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(trainer, test_audio_path=None):\n",
    "    \"\"\"Test the trained model\"\"\"\n",
    "    print(\"🧪 Testing model...\")\n",
    "    \n",
    "    if test_audio_path is None:\n",
    "        # Create synthetic test data\n",
    "        test_audio = np.random.rand(config.SAMPLE_RATE) * 0.1\n",
    "        test_mel = audio_processor.extract_melspectrogram(test_audio)\n",
    "    else:\n",
    "        # Load test audio\n",
    "        test_audio, _ = audio_processor.load_audio(test_audio_path)\n",
    "        if test_audio is not None:\n",
    "            test_audio = audio_processor.normalize_audio(test_audio)\n",
    "            test_audio = audio_processor.pad_or_truncate(test_audio, int(config.SAMPLE_RATE * config.DURATION))\n",
    "            test_mel = audio_processor.extract_melspectrogram(test_audio)\n",
    "        else:\n",
    "            print(\"❌ Could not load test audio\")\n",
    "            return\n",
    "    \n",
    "    if test_mel is None:\n",
    "        print(\"❌ Could not extract features from test audio\")\n",
    "        return\n",
    "    \n",
    "    # Prepare input\n",
    "    input_tensor = torch.FloatTensor(test_mel).unsqueeze(0).to(trainer.device)\n",
    "    \n",
    "    # Inference\n",
    "    trainer.model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = trainer.model(input_tensor)\n",
    "        probability = output.item()\n",
    "        prediction = 1 if probability > 0.5 else 0\n",
    "    \n",
    "    print(f\"\\n📊 Test Results:\")\n",
    "    print(f\"  Probability: {probability:.4f}\")\n",
    "    print(f\"  Prediction: {'Wakeword' if prediction == 1 else 'Negative'}\")\n",
    "    print(f\"  Confidence: {probability * 100:.2f}%\")\n",
    "    \n",
    "    return probability, prediction\n",
    "\n",
    "def save_model_for_deployment(trainer, model_path=\"wakeword_deploy.pt\"):\n",
    "    \"\"\"Save model for deployment\"\"\"\n",
    "    print(f\"💾 Saving model for deployment to {model_path}...\")\n",
    "    \n",
    "    # Create deployment-ready model\n",
    "    deploy_dict = {\n",
    "        'model_state_dict': trainer.model.state_dict(),\n",
    "        'config': config.__dict__,\n",
    "        'model_class': 'WakewordModel',\n",
    "        'input_shape': (1, config.N_MELS, 32),  # Example input shape\n",
    "        'sample_rate': config.SAMPLE_RATE,\n",
    "        'duration': config.DURATION,\n",
    "    }\n",
    "    \n",
    "    torch.save(deploy_dict, model_path)\n",
    "    print(f\"✅ Model saved successfully!\")\n",
    "    print(f\"Model size: {os.path.getsize(model_path) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Example usage\n",
    "if hasattr(trainer, 'model'):\n",
    "    # Test the model\n",
    "    test_model(trainer)\n",
    "    \n",
    "    # Save for deployment\n",
    "    save_model_for_deployment(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Instructions for Your Data\n",
    "\n",
    "To use this notebook with your actual data:\n",
    "\n",
    "1. **Organize your audio files:**\n",
    "   ```
",
    "   ./wakeword_data/      # Your 500 wakeword recordings\n",
    "   ./negative_data/     # Your thousands of negative samples\n",
    "   ./background_noise/  # Your 100 hours of background noise\n",
    "   ```\n",
    "\n",
    "2. **Supported audio formats:** WAV, MP3, FLAC\n",
    "\n",
    "3. **Recommended directory structure:**\n",
    "   ```\n",
    "   wakeword_data/\n",
    "   ├── person1_wakeword1.wav\n",
    "   ├── person1_wakeword2.wav\n",
    "   ├── person2_wakeword1.wav\n",
    "   └── ...\n",
    "   \n",
    "   negative_data/\n",
    "   ├── speech_sample1.wav\n",
    "   ├── speech_sample2.wav\n",
    "   └── ...\n",
    "   \n",
    "   background_noise/\n",
    "   ├── home_noise1.wav\n",
    "   ├── home_noise2.wav\n",
    "   └── ...\n",
    "   ```\n",
    "\n",
    "4. **To run JupyterLab:**\n",
    "   ```bash\n",
    "   source wakeword_env/bin/activate\n",
    "   jupyter lab --ip=0.0.0.0 --port=8888 --no-browser\n",
    "   ```\n",
    "   \n",
    "   Then access from Windows at: `http://localhost:8888`\n",
    "\n",
    "5. **To enable GPU support (once CUDA is properly installed):**\n",
    "   ```bash\n",
    "   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}