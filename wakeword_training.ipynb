{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wakeword Detection Training System\n",
    "\n",
    "## GPU-Accelerated Wakeword Training with CNN+LSTM Architecture\n",
    "\n",
    "**System Status**: ✅ GPU Ready | RTX 3060 Ti | PyTorch 2.0.1 + CUDA 11.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Setup and Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import subprocess\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU Configuration and Diagnostic\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🔍 System Diagnostic:\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   Using device: {device}\")\n",
    "print(f\"   GPU Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"   ⚠️  GPU not detected - running on CPU\")\n",
    "    print(\"   🔧 GPU Fix Suggestions:\")\n",
    "    print(\"      1. Install NVIDIA drivers: sudo apt install nvidia-driver-535\")\n",
    "    print(\"      2. Install CUDA toolkit\")\n",
    "    print(\"      3. Restart WSL: wsl --shutdown (in Windows CMD)\")\n",
    "    print(\"      4. Run: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Configuration\n",
    "class AudioConfig:\n",
    "    SAMPLE_RATE = 16000\n",
    "    DURATION = 1.7  # seconds\n",
    "    N_MELS = 80\n",
    "    N_FFT = 2048\n",
    "    HOP_LENGTH = 512\n",
    "    WIN_LENGTH = 2048\n",
    "    FMIN = 0\n",
    "    FMAX = 8000\n",
    "    \n",
    "# Model Configuration - Enhanced for better training\n",
    "class ModelConfig:\n",
    "    HIDDEN_SIZE = 256  # Increased capacity\n",
    "    NUM_LAYERS = 2\n",
    "    DROPOUT = 0.6  # Increased dropout for regularization\n",
    "    NUM_CLASSES = 2  # wakeword vs negative\n",
    "    \n",
    "# Training Configuration - Fixed for better convergence\n",
    "class TrainingConfig:\n",
    "    BATCH_SIZE = 32  # Reduced for better gradient updates\n",
    "    LEARNING_RATE = 0.0001  # Reduced for stable training\n",
    "    EPOCHS = 100\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    TEST_SPLIT = 0.1\n",
    "    \n",
    "# Data Augmentation Configuration - Enhanced for better generalization\n",
    "class AugmentationConfig:\n",
    "    AUGMENTATION_PROB = 0.85  # Increased augmentation\n",
    "    NOISE_FACTOR = 0.15  # More noise variation\n",
    "    TIME_SHIFT_MAX = 0.3  # More time variation\n",
    "    PITCH_SHIFT_MAX = 1.5  # More pitch variation\n",
    "    SPEED_CHANGE_MIN = 0.9\n",
    "    SPEED_CHANGE_MAX = 1.1\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(\"🔧 Enhanced configuration for better training convergence\")\n",
    "print(f\"   Learning Rate: {TrainingConfig.LEARNING_RATE}\")\n",
    "print(f\"   Batch Size: {TrainingConfig.BATCH_SIZE}\")\n",
    "print(f\"   Dropout: {ModelConfig.DROPOUT}\")\n",
    "print(f\"   Augmentation: {AugmentationConfig.AUGMENTATION_PROB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Processing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    \"\"\"Handle all audio processing tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, config=AudioConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"Load audio file and return as numpy array\"\"\"\n",
    "        try:\n",
    "            audio, sr = librosa.load(file_path, sr=self.config.SAMPLE_RATE)\n",
    "            return audio\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def normalize_audio(self, audio):\n",
    "        \"\"\"Normalize audio to [-1, 1] range\"\"\"\n",
    "        if len(audio) == 0:\n",
    "            return audio\n",
    "        return audio / np.max(np.abs(audio))\n",
    "    \n",
    "    def pad_or_truncate(self, audio, target_length):\n",
    "        \"\"\"Pad or truncate audio to target length\"\"\"\n",
    "        if len(audio) > target_length:\n",
    "            # Truncate\n",
    "            start_idx = random.randint(0, len(audio) - target_length)\n",
    "            return audio[start_idx:start_idx + target_length]\n",
    "        else:\n",
    "            # Pad with zeros\n",
    "            return np.pad(audio, (0, target_length - len(audio)), mode='constant')\n",
    "    \n",
    "    def audio_to_mel(self, audio):\n",
    "        \"\"\"Convert audio to mel-spectrogram\"\"\"\n",
    "        if len(audio) == 0:\n",
    "            return np.zeros((self.config.N_MELS, int(self.config.SAMPLE_RATE * self.config.DURATION / self.config.HOP_LENGTH) + 1))\n",
    "        \n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio,\n",
    "            sr=self.config.SAMPLE_RATE,\n",
    "            n_mels=self.config.N_MELS,\n",
    "            n_fft=self.config.N_FFT,\n",
    "            hop_length=self.config.HOP_LENGTH,\n",
    "            win_length=self.config.WIN_LENGTH,\n",
    "            fmin=self.config.FMIN,\n",
    "            fmax=self.config.FMAX\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        return mel_spec\n",
    "    \n",
    "    def augment_audio(self, audio, config=AugmentationConfig):\n",
    "        \"\"\"Apply data augmentation to audio\"\"\"\n",
    "        augmented_audio = audio.copy()\n",
    "        \n",
    "        # Time shift\n",
    "        if random.random() < config.AUGMENTATION_PROB:\n",
    "            shift_amount = int(random.uniform(-config.TIME_SHIFT_MAX, config.TIME_SHIFT_MAX) * self.config.SAMPLE_RATE)\n",
    "            augmented_audio = np.roll(augmented_audio, shift_amount)\n",
    "        \n",
    "        # Pitch shift\n",
    "        if random.random() < config.AUGMENTATION_PROB:\n",
    "            n_steps = random.uniform(-config.PITCH_SHIFT_MAX, config.PITCH_SHIFT_MAX)\n",
    "            augmented_audio = librosa.effects.pitch_shift(y=augmented_audio, sr=self.config.SAMPLE_RATE, n_steps=n_steps)\n",
    "        \n",
    "        # Speed change\n",
    "        if random.random() < config.AUGMENTATION_PROB:\n",
    "            speed_factor = random.uniform(config.SPEED_CHANGE_MIN, config.SPEED_CHANGE_MAX)\n",
    "            augmented_audio = librosa.effects.time_stretch(y=augmented_audio, rate=speed_factor)\n",
    "            \n",
    "            # Pad or truncate to maintain original length\n",
    "            augmented_audio = self.pad_or_truncate(augmented_audio, len(audio))\n",
    "        \n",
    "        # Add background noise\n",
    "        if random.random() < config.AUGMENTATION_PROB:\n",
    "            noise = np.random.normal(0, config.NOISE_FACTOR, len(augmented_audio))\n",
    "            augmented_audio = augmented_audio + noise\n",
    "        \n",
    "        return augmented_audio\n",
    "    \n",
    "    def process_audio_file(self, file_path, augment=False):\n",
    "        \"\"\"Process a single audio file\"\"\"\n",
    "        # Load audio\n",
    "        audio = self.load_audio(file_path)\n",
    "        if audio is None:\n",
    "            return None\n",
    "        \n",
    "        # Normalize\n",
    "        audio = self.normalize_audio(audio)\n",
    "        \n",
    "        # Pad/truncate to target length\n",
    "        target_length = int(self.config.SAMPLE_RATE * self.config.DURATION)\n",
    "        audio = self.pad_or_truncate(audio, target_length)\n",
    "        \n",
    "        # Apply augmentation if requested\n",
    "        if augment:\n",
    "            audio = self.augment_audio(audio)\n",
    "        \n",
    "        # Convert to mel-spectrogram\n",
    "        mel_spec = self.audio_to_mel(audio)\n",
    "        \n",
    "        return mel_spec\n",
    "\n",
    "# Test the audio processor\n",
    "processor = AudioProcessor()\n",
    "print(\"AudioProcessor created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WakewordModel(nn.Module):\n",
    "    \"\"\"CNN+LSTM model for wakeword detection\"\"\"\n",
    "    \n",
    "    def __init__(self, config=ModelConfig, audio_config=AudioConfig):\n",
    "        super(WakewordModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.audio_config = audio_config\n",
    "        \n",
    "        # Calculate CNN output size\n",
    "        self.mel_height = audio_config.N_MELS\n",
    "        self.mel_width = int(audio_config.SAMPLE_RATE * audio_config.DURATION / audio_config.HOP_LENGTH) + 1\n",
    "        \n",
    "        # CNN layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Calculate CNN output size\n",
    "        self.cnn_output_size = 128  # After adaptive pooling\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.cnn_output_size,\n",
    "            hidden_size=config.HIDDEN_SIZE,\n",
    "            num_layers=config.NUM_LAYERS,\n",
    "            batch_first=True,\n",
    "            dropout=config.DROPOUT if config.NUM_LAYERS > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(config.DROPOUT)\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(config.HIDDEN_SIZE, config.NUM_CLASSES)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, 1, mel_height, mel_width)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        x = x.view(batch_size, -1)  # (batch, cnn_output_size)\n",
    "        \n",
    "        # Add sequence dimension\n",
    "        x = x.unsqueeze(1)  # (batch, seq_len=1, features)\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Take the last output\n",
    "        x = lstm_out[:, -1, :]  # (batch, hidden_size)\n",
    "        \n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Final classification\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model and move to device\n",
    "model = WakewordModel().to(device)\n",
    "print(f\"Model created and moved to {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test model with dummy input\n",
    "dummy_input = torch.randn(1, 1, AudioConfig.N_MELS, 31).to(device)  # (batch, channels, height, width)\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "print(f\"Dummy output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Dataset Class with Background Noise Support\n",
    "\n",
    "class EnhancedWakewordDataset(Dataset):\n",
    "    \"\"\"Enhanced dataset with background noise mixing and balanced sampling\"\"\"\n",
    "    \n",
    "    def __init__(self, wakeword_files, hard_negative_files, random_negative_files, \n",
    "                 background_files, processor, augment=False, \n",
    "                 background_mix_prob=0.7, snr_range=(0, 20)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            wakeword_files: List of positive wakeword audio files\n",
    "            hard_negative_files: List of hard negative files (phonetically similar)\n",
    "            random_negative_files: List of random negative files\n",
    "            background_files: List of background noise files (66 hours)\n",
    "            processor: AudioProcessor instance\n",
    "            augment: Whether to apply augmentation\n",
    "            background_mix_prob: Probability of mixing with background noise\n",
    "            snr_range: SNR range in dB for mixing\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        self.augment = augment\n",
    "        self.background_mix_prob = background_mix_prob\n",
    "        self.snr_range = snr_range\n",
    "        \n",
    "        # Store file lists separately for balanced sampling\n",
    "        self.wakeword_files = wakeword_files\n",
    "        self.hard_negative_files = hard_negative_files\n",
    "        self.random_negative_files = random_negative_files\n",
    "        self.background_files = background_files\n",
    "        \n",
    "        # Pre-load background noise segments for efficient mixing\n",
    "        self.background_cache = self._cache_background_segments()\n",
    "        \n",
    "        # Calculate balanced dataset composition\n",
    "        self._create_balanced_dataset()\n",
    "        \n",
    "    def _cache_background_segments(self, max_cache_size=100):\n",
    "        \"\"\"Pre-load and cache background noise segments\"\"\"\n",
    "        print(\"🎵 Caching background noise segments...\")\n",
    "        cache = []\n",
    "        \n",
    "        for i, bg_file in enumerate(self.background_files[:max_cache_size]):\n",
    "            try:\n",
    "                audio = self.processor.load_audio(bg_file)\n",
    "                if audio is not None and len(audio) > 0:\n",
    "                    # Normalize background audio\n",
    "                    audio = audio / (np.max(np.abs(audio)) + 1e-8)\n",
    "                    cache.append(audio)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load background file {bg_file}: {e}\")\n",
    "            \n",
    "            if (i + 1) % 20 == 0:\n",
    "                print(f\"   Cached {i + 1}/{min(max_cache_size, len(self.background_files))} background files\")\n",
    "        \n",
    "        print(f\"✅ Cached {len(cache)} background noise segments\")\n",
    "        return cache\n",
    "    \n",
    "    def _create_balanced_dataset(self):\n",
    "        \"\"\"Create balanced dataset with proper ratios\"\"\"\n",
    "        # Target ratios: 1 positive : 4.5 hard_neg : 8.75 random_neg : 10 background\n",
    "        \n",
    "        n_wakeword = len(self.wakeword_files)\n",
    "        \n",
    "        # Calculate other categories based on ratios\n",
    "        n_hard_neg = min(len(self.hard_negative_files), int(n_wakeword * 4.5))\n",
    "        n_random_neg = min(len(self.random_negative_files), int(n_wakeword * 8.75))\n",
    "        n_background_pure = min(len(self.background_files), int(n_wakeword * 10))\n",
    "        \n",
    "        # Sample files for each category\n",
    "        import random\n",
    "        random.seed(42)  # For reproducibility\n",
    "        \n",
    "        sampled_hard_neg = random.sample(self.hard_negative_files, n_hard_neg) if n_hard_neg > 0 else []\n",
    "        sampled_random_neg = random.sample(self.random_negative_files, n_random_neg) if n_random_neg > 0 else []\n",
    "        sampled_background = random.sample(self.background_files, n_background_pure) if n_background_pure > 0 else []\n",
    "        \n",
    "        # Combine all files with their labels\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        self.categories = []  # Track category for analysis\n",
    "        \n",
    "        # Add wakeword samples (label=1)\n",
    "        for f in self.wakeword_files:\n",
    "            self.files.append(f)\n",
    "            self.labels.append(1)\n",
    "            self.categories.append('wakeword')\n",
    "        \n",
    "        # Add hard negatives (label=0)\n",
    "        for f in sampled_hard_neg:\n",
    "            self.files.append(f)\n",
    "            self.labels.append(0)\n",
    "            self.categories.append('hard_negative')\n",
    "        \n",
    "        # Add random negatives (label=0)\n",
    "        for f in sampled_random_neg:\n",
    "            self.files.append(f)\n",
    "            self.labels.append(0)\n",
    "            self.categories.append('random_negative')\n",
    "        \n",
    "        # Add pure background noise (label=0)\n",
    "        for f in sampled_background:\n",
    "            self.files.append(f)\n",
    "            self.labels.append(0)\n",
    "            self.categories.append('background')\n",
    "        \n",
    "        # Shuffle the dataset\n",
    "        indices = list(range(len(self.files)))\n",
    "        random.shuffle(indices)\n",
    "        self.files = [self.files[i] for i in indices]\n",
    "        self.labels = [self.labels[i] for i in indices]\n",
    "        self.categories = [self.categories[i] for i in indices]\n",
    "        \n",
    "        # Print dataset statistics\n",
    "        print(f\"\\n📊 Enhanced Dataset Statistics:\")\n",
    "        print(f\"   Wakeword samples: {n_wakeword}\")\n",
    "        print(f\"   Hard negatives: {n_hard_neg}\")\n",
    "        print(f\"   Random negatives: {n_random_neg}\")\n",
    "        print(f\"   Background noise: {n_background_pure}\")\n",
    "        print(f\"   Total samples: {len(self.files)}\")\n",
    "        print(f\"   Positive ratio: 1:{(len(self.files) - n_wakeword) / n_wakeword:.1f}\")\n",
    "    \n",
    "    def _mix_with_background(self, audio, target_snr_db=None):\n",
    "        \"\"\"Mix audio with random background noise at specified SNR\"\"\"\n",
    "        if len(self.background_cache) == 0:\n",
    "            return audio\n",
    "        \n",
    "        # Select random background\n",
    "        bg_audio = random.choice(self.background_cache)\n",
    "        \n",
    "        # Extract segment from background\n",
    "        target_len = len(audio)\n",
    "        if len(bg_audio) > target_len:\n",
    "            start_idx = random.randint(0, len(bg_audio) - target_len)\n",
    "            bg_segment = bg_audio[start_idx:start_idx + target_len]\n",
    "        else:\n",
    "            # Tile background if it's shorter\n",
    "            bg_segment = np.tile(bg_audio, (target_len // len(bg_audio) + 1))[:target_len]\n",
    "        \n",
    "        # Calculate SNR if not specified\n",
    "        if target_snr_db is None:\n",
    "            target_snr_db = random.uniform(self.snr_range[0], self.snr_range[1])\n",
    "        \n",
    "        # Calculate mixing weights based on SNR\n",
    "        signal_power = np.mean(audio ** 2)\n",
    "        noise_power = np.mean(bg_segment ** 2)\n",
    "        \n",
    "        if noise_power > 0:\n",
    "            current_snr = 10 * np.log10(signal_power / noise_power)\n",
    "            noise_scale = np.sqrt(signal_power / (10 ** (target_snr_db / 10)) / noise_power)\n",
    "            mixed = audio + noise_scale * bg_segment\n",
    "        else:\n",
    "            mixed = audio\n",
    "        \n",
    "        # Normalize to prevent clipping\n",
    "        max_val = np.max(np.abs(mixed))\n",
    "        if max_val > 0:\n",
    "            mixed = mixed / max_val * 0.95\n",
    "        \n",
    "        return mixed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.files[idx]\n",
    "        label = self.labels[idx]\n",
    "        category = self.categories[idx]\n",
    "        \n",
    "        # Load and process audio\n",
    "        audio = self.processor.load_audio(file_path)\n",
    "        \n",
    "        if audio is None:\n",
    "            # Return zeros if loading failed\n",
    "            mel_spec = np.zeros((self.processor.config.N_MELS, 31))\n",
    "        else:\n",
    "            # Normalize audio\n",
    "            audio = self.processor.normalize_audio(audio)\n",
    "            \n",
    "            # Pad/truncate to target length\n",
    "            target_length = int(self.processor.config.SAMPLE_RATE * self.processor.config.DURATION)\n",
    "            audio = self.processor.pad_or_truncate(audio, target_length)\n",
    "            \n",
    "            # Apply augmentation if training\n",
    "            if self.augment:\n",
    "                audio = self.processor.augment_audio(audio)\n",
    "            \n",
    "            # Mix with background noise (except for pure background samples)\n",
    "            if category != 'background' and random.random() < self.background_mix_prob:\n",
    "                audio = self._mix_with_background(audio)\n",
    "            \n",
    "            # Convert to mel-spectrogram\n",
    "            mel_spec = self.processor.audio_to_mel(audio)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        mel_tensor = torch.FloatTensor(mel_spec).unsqueeze(0)\n",
    "        label_tensor = torch.LongTensor([label])\n",
    "        \n",
    "        return mel_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WakewordTrainer:\n",
    "    \"\"\"Enhanced training class with learning rate scheduling\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, config=TrainingConfig):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss().to(device)\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='max', factor=0.5, patience=5\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "        # Early stopping\n",
    "        self.patience = 10\n",
    "        self.best_val_acc = 0.0\n",
    "        self.epochs_no_improve = 0\n",
    "        \n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"Train for one epoch with gradient clipping\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "            data, target = data.to(self.device), target.to(self.device).squeeze()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def validate(self, val_loader):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in tqdm(val_loader, desc=\"Validating\"):\n",
    "                data, target = data.to(self.device), target.to(self.device).squeeze()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(val_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs):\n",
    "        \"\"\"Complete training loop with early stopping\"\"\"\n",
    "        print(f\"Starting training for {epochs} epochs...\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        print(f\"Learning rate: {self.config.LEARNING_RATE}\")\n",
    "        print(f\"Batch size: {self.config.BATCH_SIZE}\")\n",
    "        \n",
    "        self.best_val_acc = 0.0\n",
    "        self.epochs_no_improve = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            # GPU Memory monitoring\n",
    "            if torch.cuda.is_available():\n",
    "                gpu_memory = torch.cuda.memory_allocated() / 1e6\n",
    "                gpu_reserved = torch.cuda.memory_reserved() / 1e6\n",
    "                print(f\"GPU Memory: {gpu_memory:.1f}MB allocated, {gpu_reserved:.1f}MB reserved\")\n",
    "            else:\n",
    "                print(\"GPU Memory: Not available (using CPU)\")\n",
    "            \n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate(val_loader)\n",
    "            \n",
    "            # Store metrics\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_acc)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                self.epochs_no_improve = 0\n",
    "                \n",
    "                # Save best model\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                    'train_acc': train_acc,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                }, 'best_wakeword_model.pth')\n",
    "                print(f\"🎉 New best model saved! Validation accuracy: {val_acc:.2f}%\")\n",
    "            else:\n",
    "                self.epochs_no_improve += 1\n",
    "                \n",
    "            # Early stopping\n",
    "            if self.epochs_no_improve >= self.patience:\n",
    "                print(f\"\\n⏹️  Early stopping triggered! No improvement for {self.patience} epochs.\")\n",
    "                print(f\"Best validation accuracy: {self.best_val_acc:.2f}%\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\n🎉 Training completed!\")\n",
    "        print(f\"Best validation accuracy: {self.best_val_acc:.2f}%\")\n",
    "        print(f\"Total epochs trained: {epoch + 1}\")\n",
    "        return self.best_val_acc\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(self.train_losses, label='Train Loss', linewidth=2)\n",
    "        ax1.plot(self.val_losses, label='Val Loss', linewidth=2)\n",
    "        ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax2.plot(self.train_accuracies, label='Train Accuracy', linewidth=2)\n",
    "        ax2.plot(self.val_accuracies, label='Val Accuracy', linewidth=2)\n",
    "        ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create trainer instance\n",
    "trainer = WakewordTrainer(model, device)\n",
    "print(\"Enhanced trainer created successfully!\")\n",
    "print(\"🔧 Added: Learning rate scheduling, gradient clipping, early stopping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Modified Data Loading Section\ndef prepare_enhanced_datasets(wakeword_dir='positive_dataset',\n                             negative_dir='negative_dataset',\n                             background_dir='background_noise',\n                             hard_negative_dir='hard_negatives'):\n    \"\"\"\n    Prepare datasets with proper categorization\n    \n    Directory structure expected:\n    - positive_dataset/: Your positive samples\n    - negative_dataset/: General negative samples\n    - hard_negatives/: Phonetically similar negatives (if separate)\n    - background_noise/: 66 hours of background recordings\n    \"\"\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the training cell to use enhanced dataset\n",
    "def create_enhanced_dataloaders(data_splits, processor, batch_size=16):\n",
    "    \"\"\"Create DataLoaders with the enhanced dataset\"\"\"\n",
    "    \n",
    "    # Unpack splits\n",
    "    wake_train, hard_train, rand_train, bg_train = data_splits['train']\n",
    "    wake_val, hard_val, rand_val, bg_val = data_splits['val']\n",
    "    wake_test, hard_test, rand_test, bg_test = data_splits['test']\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = EnhancedWakewordDataset(\n",
    "        wakeword_files=wake_train,\n",
    "        hard_negative_files=hard_train,\n",
    "        random_negative_files=rand_train,\n",
    "        background_files=bg_train,\n",
    "        processor=processor,\n",
    "        augment=True,\n",
    "        background_mix_prob=0.7,\n",
    "        snr_range=(0, 20)\n",
    "    )\n",
    "    \n",
    "    val_dataset = EnhancedWakewordDataset(\n",
    "        wakeword_files=wake_val,\n",
    "        hard_negative_files=hard_val,\n",
    "        random_negative_files=rand_val,\n",
    "        background_files=bg_val,\n",
    "        processor=processor,\n",
    "        augment=False,\n",
    "        background_mix_prob=0.5,  # Less mixing for validation\n",
    "        snr_range=(5, 15)  # More conservative SNR for validation\n",
    "    )\n",
    "    \n",
    "    test_dataset = EnhancedWakewordDataset(\n",
    "        wakeword_files=wake_test,\n",
    "        hard_negative_files=hard_test,\n",
    "        random_negative_files=rand_test,\n",
    "        background_files=bg_test,\n",
    "        processor=processor,\n",
    "        augment=False,\n",
    "        background_mix_prob=0.5,\n",
    "        snr_range=(5, 15)\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "# Usage in your notebook:\n",
    "if __name__ == \"__main__\":\n",
    "    # Prepare datasets\n",
    "    data_splits = prepare_enhanced_datasets()\n",
    "    \n",
    "    # Create processor (assuming it's already defined)\n",
    "    processor = AudioProcessor()\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader, val_loader, test_loader = create_enhanced_dataloaders(\n",
    "        data_splits, \n",
    "        processor, \n",
    "        batch_size=TrainingConfig.BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    print(\"\\n🚀 Ready for training with background noise integration!\")\n",
    "    print(f\"   Background noise mixing probability: 70% for training\")\n",
    "    print(f\"   SNR range: 0-20 dB for training, 5-15 dB for validation\")\n",
    "    print(f\"   Total training batches: {len(train_loader)}\")\n",
    "    print(f\"   Total validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "if os.path.exists('best_wakeword_model.pth'):\n",
    "    checkpoint = torch.load('best_wakeword_model.pth', map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"✅ Best model loaded (epoch {checkpoint['epoch'] + 1}, val_acc: {checkpoint['val_acc']:.2f}%)\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            data, target = data.to(device), target.to(device).squeeze()\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(target.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"\\n📊 Test Set Performance:\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall: {recall:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Wakeword'], \n",
    "                yticklabels=['Negative', 'Wakeword'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Negative', 'Wakeword']))\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No trained model found. Please run the training cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_wakeword(audio_file_path, model, processor, device, threshold=0.8):\n",
    "    \"\"\"Predict if audio contains wakeword\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Process audio file\n",
    "    mel_spec = processor.process_audio_file(audio_file_path, augment=False)\n",
    "    \n",
    "    if mel_spec is None:\n",
    "        print(f\"Error processing audio file: {audio_file_path}\")\n",
    "        return False, 0.0\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    mel_tensor = torch.FloatTensor(mel_spec).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(mel_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        wakeword_prob = probabilities[0][1].item()\n",
    "        \n",
    "    is_wakeword = wakeword_prob >= threshold\n",
    "    \n",
    "    return is_wakeword, wakeword_prob\n",
    "\n",
    "# Test prediction function\n",
    "if os.path.exists('best_wakeword_model.pth'):\n",
    "    print(\"\\n🎯 Wakeword Detection System Ready!\")\n",
    "    print(\"You can now use the predict_wakeword function for real-time detection.\")\n",
    "    print(\"\\nExample usage:\")\n",
    "    print(\"is_wakeword, confidence = predict_wakeword('./test_files/3.wav', model, processor, device)\")\n",
    "    print(f\"Result: {{'wakeword_detected': is_wakeword, 'confidence': confidence:.2f}}\")\n",
    "    \n",
    "    # Create a simple test function\n",
    "    def test_audio_file(file_path):\n",
    "        if os.path.exists(file_path):\n",
    "            is_wakeword, confidence = predict_wakeword(file_path, model, processor, device)\n",
    "            print(f\"\\n📁 Testing: {file_path}\")\n",
    "            print(f\"   Wakeword detected: {is_wakeword}\")\n",
    "            print(f\"   Confidence: {confidence:.2f}\")\n",
    "            print(f\"   Threshold: 0.80\")\n",
    "        else:\n",
    "            print(f\"\\n❌ File not found: {file_path}\")\n",
    "    \n",
    "    # Direct test with ./test_files/1.wav\n",
    "    test_audio_file(\"./test_files/3.wav\")\n",
    "else:\n",
    "    print(\"❌ Model not trained yet. Please run training cells first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete model for deployment\n",
    "if os.path.exists('best_wakeword_model.pth'):\n",
    "    # Create a complete deployment package\n",
    "    deployment_package = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'HIDDEN_SIZE': ModelConfig.HIDDEN_SIZE,\n",
    "            'NUM_LAYERS': ModelConfig.NUM_LAYERS,\n",
    "            'DROPOUT': ModelConfig.DROPOUT,\n",
    "            'NUM_CLASSES': ModelConfig.NUM_CLASSES\n",
    "        },\n",
    "        'audio_config': {\n",
    "            'SAMPLE_RATE': AudioConfig.SAMPLE_RATE,\n",
    "            'DURATION': AudioConfig.DURATION,\n",
    "            'N_MELS': AudioConfig.N_MELS,\n",
    "            'N_FFT': AudioConfig.N_FFT,\n",
    "            'HOP_LENGTH': AudioConfig.HOP_LENGTH,\n",
    "            'FMIN': AudioConfig.FMIN,\n",
    "            'FMAX': AudioConfig.FMAX\n",
    "        },\n",
    "        'training_info': {\n",
    "            'best_val_accuracy': checkpoint.get('val_acc', 0),\n",
    "            'epoch': checkpoint.get('epoch', 0) + 1,\n",
    "            'device': str(device)\n",
    "        },\n",
    "        'classes': ['negative', 'wakeword']\n",
    "    }\n",
    "    \n",
    "    # Save deployment package\n",
    "    torch.save(deployment_package, 'wakeword_deployment_model.pth')\n",
    "    print(\"✅ Deployment model saved as 'wakeword_deployment_model.pth'\")\n",
    "    \n",
    "    # Save model architecture for reference\n",
    "    with open('model_architecture.txt', 'w') as f:\n",
    "        f.write(\"Wakeword Detection Model Architecture\\n\")\n",
    "        f.write(\"================================\\n\\n\")\n",
    "        f.write(\"Model Type: CNN + LSTM\\n\")\n",
    "        f.write(f\"Input Shape: (1, {AudioConfig.N_MELS}, 31)\\n\")\n",
    "        f.write(f\"Hidden Size: {ModelConfig.HIDDEN_SIZE}\\n\")\n",
    "        f.write(f\"Number of Layers: {ModelConfig.NUM_LAYERS}\\n\")\n",
    "        f.write(f\"Dropout: {ModelConfig.DROPOUT}\\n\")\n",
    "        f.write(f\"Number of Classes: {ModelConfig.NUM_CLASSES}\\n\")\n",
    "        f.write(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\\n\")\n",
    "        f.write(f\"Device: {device}\\n\")\n",
    "    \n",
    "    print(\"✅ Model architecture saved as 'model_architecture.txt'\")\n",
    "    \n",
    "    print(\"\\n🎉 Model deployment package ready!\")\n",
    "    print(\"Files created:\")\n",
    "    print(\"   - wakeword_deployment_model.pth (complete model)\")\n",
    "    print(\"   - model_architecture.txt (model specs)\")\n",
    "    print(\"   - best_wakeword_model.pth (training checkpoint)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No trained model found to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f\"\\n🚀 Next Steps:\")\nprint(f\"   1. Add your wakeword recordings to positive_dataset/\")\nprint(f\"   2. Add negative samples to negative_dataset/\")\nprint(f\"   3. Add background noise to background_noise/\")\nprint(f\"   4. Run the training cells above\")\nprint(f\"   5. Use the trained model for wakeword detection\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}